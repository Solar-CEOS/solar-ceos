{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad40d410-0529-4e83-94a0-f4ea594c1e45",
   "metadata": {},
   "source": [
    "# Energy Envelope Test for Planetary Modulation\n",
    "\n",
    "Using the \"energy envelope model\" as a filter, we systematically examine which fundamental frequencies (signals in the residuals) have their energy truly modulated by the 11-year planetary cycle (fitted SSN). This method (energy envelope of high-frequency signals vs. low-frequency driving force) is physically more interpretable than the \"full frequency comb\" feature engineering of LGBM. We will no longer be limited to 20-35 days, but will traverse all relevant physical frequency bands (rotation, Rieger, annual, QBO), and quantify their respective \"modulation relationship\" with the 11-year cycle. The complete code for filtering modulated frequencies is provided below. It will: define a series of physical frequency bands of interest (in days). Loop through 4 models (M8+2, M0+3...). For each model, loop through each frequency band. Core step: extract the energy envelope of that frequency band, and (using a simple LGBM) fit $Y_{envelope} = f(X_{planetary\\_fit})$, calculating RÂ² on the test set. Finally, a table is summarized showing the average RÂ² for each frequency band across the four models, telling you which frequencies are \"worth\" being modulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a4832d-1230-4b22-a1e7-5afbf28cb181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM imported successfully.\n",
      "Scipy imported successfully (version: 1.16.3).\n",
      "Loading source data: ../../results/05_p_m_a_model/p_model_4/residual/Summary_Fit_Results.csv\n",
      "\n",
      "=== Start Screening for 'Energy Envelope Modulation' of Physical Bands ===\n",
      "\n",
      "==================================================\n",
      "Processing Model: M8+2 (Train End: 1996-08-01)\n",
      "==================================================\n",
      "  Testing band: Rotation_Wide (20-40 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 53904, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 29.679758\n",
      "  > Rotation_Wide Test R2: 0.3943\n",
      "  Testing band: Rotation_Core (25-30 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 53904, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 20.046650\n",
      "  > Rotation_Core Test R2: 0.2925\n",
      "  Testing band: Rieger (140-170 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 53904, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 8.188442\n",
      "  > Rieger Test R2: -0.2387\n",
      "  Testing band: Annual (350-380 days)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 53904, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 4.835737\n",
      "  > Annual Test R2: -0.6341\n",
      "  Testing band: QBO_1.5_3.0y (547.875-1095.75 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 53904, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 9.114447\n",
      "  > QBO_1.5_3.0y Test R2: 0.4471\n",
      "  Testing band: QBO_2.2y (730.5-876.6 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 53904, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 5.998310\n",
      "  > QBO_2.2y Test R2: -0.3194\n",
      "\n",
      "==================================================\n",
      "Processing Model: M8+3 (Train End: 1986-08-01)\n",
      "==================================================\n",
      "  Testing band: Rotation_Wide (20-40 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 50251, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 29.561354\n",
      "  > Rotation_Wide Test R2: 0.3229\n",
      "  Testing band: Rotation_Core (25-30 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 50251, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 19.796950\n",
      "  > Rotation_Core Test R2: 0.3428\n",
      "  Testing band: Rieger (140-170 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 50251, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 8.107376\n",
      "  > Rieger Test R2: -0.1263\n",
      "  Testing band: Annual (350-380 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 50251, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 5.011983\n",
      "  > Annual Test R2: -1.2428\n",
      "  Testing band: QBO_1.5_3.0y (547.875-1095.75 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 50251, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 8.969141\n",
      "  > QBO_1.5_3.0y Test R2: 0.2012\n",
      "  Testing band: QBO_2.2y (730.5-876.6 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 50251, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 5.819495\n",
      "  > QBO_2.2y Test R2: -0.1298\n",
      "\n",
      "==================================================\n",
      "Processing Model: M0+3 (Train End: 1986-09-01)\n",
      "==================================================\n",
      "  Testing band: Rotation_Wide (20-40 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 50282, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 29.549200\n",
      "  > Rotation_Wide Test R2: 0.3880\n",
      "  Testing band: Rotation_Core (25-30 days)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 50282, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 19.788554\n",
      "  > Rotation_Core Test R2: 0.3608\n",
      "  Testing band: Rieger (140-170 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 50282, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 8.104544\n",
      "  > Rieger Test R2: 0.0723\n",
      "  Testing band: Annual (350-380 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 50282, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 5.139585\n",
      "  > Annual Test R2: -0.9186\n",
      "  Testing band: QBO_1.5_3.0y (547.875-1095.75 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 50282, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 10.079142\n",
      "  > QBO_1.5_3.0y Test R2: 0.3098\n",
      "  Testing band: QBO_2.2y (730.5-876.6 days)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 50282, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 5.994016\n",
      "  > QBO_2.2y Test R2: 0.0398\n",
      "\n",
      "==================================================\n",
      "Processing Model: M0+2 (Train End: 1996-08-01)\n",
      "==================================================\n",
      "  Testing band: Rotation_Wide (20-40 days)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 53904, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 29.679665\n",
      "  > Rotation_Wide Test R2: 0.5210\n",
      "  Testing band: Rotation_Core (25-30 days)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 53904, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 20.046846\n",
      "  > Rotation_Core Test R2: 0.4125\n",
      "  Testing band: Rieger (140-170 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 53904, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 8.187757\n",
      "  > Rieger Test R2: 0.1514\n",
      "  Testing band: Annual (350-380 days)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 53904, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 4.843152\n",
      "  > Annual Test R2: -0.5138\n",
      "  Testing band: QBO_1.5_3.0y (547.875-1095.75 days)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 53904, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 9.793664\n",
      "  > QBO_1.5_3.0y Test R2: 0.2911\n",
      "  Testing band: QBO_2.2y (730.5-876.6 days)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 53904, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 6.076191\n",
      "  > QBO_2.2y Test R2: -0.0450\n",
      "\n",
      "\n",
      "============================================================\n",
      "      Energy Envelope Modulation Screening - Final Summary Report\n",
      "      (Testing Y_Envelope = f(X_Planetary_Fit) R2 score on Test Set)\n",
      "============================================================\n",
      "model            M0+2    M0+3    M8+2    M8+3  Average_R2\n",
      "band_name                                                \n",
      "Rotation_Wide  0.5210  0.3880  0.3943  0.3229      0.4065\n",
      "Rotation_Core  0.4125  0.3608  0.2925  0.3428      0.3522\n",
      "QBO_1.5_3.0y   0.2911  0.3098  0.4471  0.2012      0.3123\n",
      "Rieger         0.1514  0.0723 -0.2387 -0.1263     -0.0353\n",
      "QBO_2.2y      -0.0450  0.0398 -0.3194 -0.1298     -0.1136\n",
      "Annual        -0.5138 -0.9186 -0.6341 -1.2428     -0.8273\n",
      "\n",
      "============================================================\n",
      "Conclusion:\n",
      "âœ… Strong Evidence: Band 'Rotation_Wide' (Avg R2=0.4065) energy envelope\n",
      "   has a strong modulation relationship with the 11-year planetary cycle.\n",
      "\n",
      "ðŸ‘‰ Recommendation: In the LGBM full frequency comb model, focus on modulation features of the Rotation_Wide band.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.signal import butter, sosfiltfilt, hilbert\n",
    "import lightgbm as lgb  # We use a simple LGBM as a verification model\n",
    "\n",
    "# Ensure astronomy libraries are available (if LGBM code runs here too)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(\"LightGBM imported successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Error: lightgbm library not found.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    # Check scipy (signal processing library)\n",
    "    import scipy\n",
    "    print(f\"Scipy imported successfully (version: {scipy.__version__}).\")\n",
    "except ImportError:\n",
    "    print(\"Error: This script requires the scipy library.\")\n",
    "    print(\"Please install: pip install scipy\")\n",
    "    exit()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Core Function 1: Bandpass Filter ---\n",
    "def bandpass_filter(data, lowcut_days, highcut_days, fs=1.0, order=4):\n",
    "    \"\"\"\n",
    "    Apply zero-phase bandpass filter.\n",
    "    fs=1.0 means the sampling interval is 1 day.\n",
    "    \"\"\"\n",
    "    # Convert period (days) to frequency (1/day)\n",
    "    low_freq = 1.0 / highcut_days\n",
    "    high_freq = 1.0 / lowcut_days\n",
    "    \n",
    "    # Nyquist frequency\n",
    "    nyq = 0.5 * fs\n",
    "    \n",
    "    # Normalized frequency\n",
    "    low = low_freq / nyq\n",
    "    high = high_freq / nyq\n",
    "    \n",
    "    # Ensure frequencies are within valid range\n",
    "    low = max(low, 1e-9) \n",
    "    high = min(high, 0.9999)\n",
    "\n",
    "    if low >= high:\n",
    "        print(f\"Warning: Invalid frequency range [{low}, {high}]. Skipping filter.\")\n",
    "        return data # Return original data\n",
    "\n",
    "    # Create filter\n",
    "    sos = butter(order, [low, high], btype='band', output='sos')\n",
    "    \n",
    "    # Apply zero-phase filter\n",
    "    filtered_data = sosfiltfilt(sos, data)\n",
    "    return filtered_data\n",
    "\n",
    "# --- Core Function 2: Calculate Energy Envelope ---\n",
    "def get_energy_envelope(data, smoothing_window_days=365):\n",
    "    \"\"\"\n",
    "    Calculate the energy envelope of the signal (Hilbert transform) and smooth it.\n",
    "    \"\"\"\n",
    "    # 1. Calculate analytic signal\n",
    "    analytic_signal = hilbert(data)\n",
    "    \n",
    "    # 2. Calculate energy envelope (instantaneous amplitude)\n",
    "    envelope = np.abs(analytic_signal)\n",
    "    \n",
    "    # 3. (Critical) Smooth the envelope to observe its long-term trend\n",
    "    #    We use a centered rolling mean\n",
    "    envelope_series = pd.Series(envelope)\n",
    "    smoothed_envelope = envelope_series.rolling(\n",
    "        window=smoothing_window_days, \n",
    "        center=True, \n",
    "        min_periods=int(smoothing_window_days * 0.1) # Allow edges\n",
    "    ).mean()\n",
    "    \n",
    "    return smoothed_envelope\n",
    "\n",
    "# --- Core Function 3: Run Modulation Test ---\n",
    "def run_modulation_test(\n",
    "    data_full, model_name, train_end_date, test_end_date, \n",
    "    band_name, freq_band_days, envelope_smoothing_days=365\n",
    "):\n",
    "    \"\"\"\n",
    "    Test if the energy envelope of a frequency band is modulated by the 11-year cycle (Fit SSN).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Update column names to English format\n",
    "    target_residual_column = f'Residual_{model_name}'\n",
    "    planetary_fit_column = f'Fit_SSN_{model_name}'\n",
    "\n",
    "    print(f\"  Testing band: {band_name} ({freq_band_days[0]}-{freq_band_days[1]} days)\")\n",
    "\n",
    "    # 1. Prepare data\n",
    "    # Note: Changed 'æ—¥æœŸ' to 'Date'\n",
    "    data = data_full[['Date', target_residual_column, planetary_fit_column]].copy()\n",
    "    \n",
    "    # Filter clean data for analysis (both Fit SSN and Residual must exist)\n",
    "    data_clean = data.dropna(subset=[target_residual_column, planetary_fit_column])\n",
    "    if len(data_clean) < 5000: # Need enough data for filtering\n",
    "        print(f\"  Warning: Not enough clean data for {model_name} ({len(data_clean)} rows).\")\n",
    "        return None\n",
    "\n",
    "    # --- Signal Processing ---\n",
    "    \n",
    "    # 2. Y (Residual) -> Bandpass Filter\n",
    "    Y_filtered = bandpass_filter(\n",
    "        data_clean[target_residual_column].values, \n",
    "        freq_band_days[0], \n",
    "        freq_band_days[1]\n",
    "    )\n",
    "    \n",
    "    # 3. Y_filtered -> Energy Envelope\n",
    "    Y_envelope = get_energy_envelope(\n",
    "        Y_filtered, \n",
    "        smoothing_window_days=envelope_smoothing_days\n",
    "    )\n",
    "    \n",
    "    data_clean.loc[:, 'Y_Envelope'] = Y_envelope\n",
    "    \n",
    "    # 4. X (Driver) -> Also smoothed\n",
    "    #    We compare two slow variables: slow change of envelope vs slow change of 11-year cycle\n",
    "    data_clean.loc[:, 'X_Driver_Smooth'] = data_clean[planetary_fit_column].rolling(\n",
    "        window=envelope_smoothing_days, \n",
    "        center=True, \n",
    "        min_periods=int(envelope_smoothing_days * 0.1)\n",
    "    ).mean()\n",
    "\n",
    "    # --- Build Test Model ---\n",
    "    \n",
    "    # 5. Clean again (smoothing and filtering create new NaNs)\n",
    "    data_model = data_clean.dropna(subset=['Y_Envelope', 'X_Driver_Smooth'])\n",
    "    \n",
    "    # 6. Define Train/Test masks\n",
    "    # Note: Changed 'æ—¥æœŸ' to 'Date'\n",
    "    train_mask = (data_model['Date'] <= train_end_date)\n",
    "    test_mask = (data_model['Date'] > train_end_date) & (data_model['Date'] <= test_end_date)\n",
    "\n",
    "    X_train = data_model.loc[train_mask, ['X_Driver_Smooth']]\n",
    "    Y_train = data_model.loc[train_mask, 'Y_Envelope']\n",
    "    X_test = data_model.loc[test_mask, ['X_Driver_Smooth']]\n",
    "    Y_test = data_model.loc[test_mask, 'Y_Envelope']\n",
    "\n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(f\"  Error: Train or Test set is empty for band {band_name}.\")\n",
    "        return None\n",
    "\n",
    "    # 7. Fit a simple LGBM: Y_Envelope = f(X_Driver_Smooth)\n",
    "    #    We use it to capture potential non-linear relationships\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Use a very simple, anti-overfitting LGBM\n",
    "    lgbm_tester = lgb.LGBMRegressor(\n",
    "        n_estimators=50,\n",
    "        num_leaves=5,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    \n",
    "    lgbm_tester.fit(X_train_scaled, Y_train)\n",
    "    \n",
    "    # 8. Evaluate R^2 on the test set\n",
    "    Y_pred = lgbm_tester.predict(X_test_scaled)\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "    \n",
    "    print(f\"  > {band_name} Test R2: {r2:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'band_name': band_name,\n",
    "        'low_days': freq_band_days[0],\n",
    "        'high_days': freq_band_days[1],\n",
    "        'r2_score': r2\n",
    "    }\n",
    "\n",
    "\n",
    "# === Main Execution Script ===\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    # --- 1. Define bands to filter (Unit: Days) ---\n",
    "    #    We filter short-term and mid-term frequencies\n",
    "    BANDS_TO_TEST = {\n",
    "        # Solar Rotation (Wide)\n",
    "        'Rotation_Wide': [20, 40],\n",
    "        # Solar Rotation (Core)\n",
    "        'Rotation_Core': [25, 30],\n",
    "        # Rieger Period\n",
    "        'Rieger': [140, 170],\n",
    "        # Annual Period\n",
    "        'Annual': [350, 380],\n",
    "        # QBO (Quasi-Biennial Oscillation)\n",
    "        'QBO_1.5_3.0y': [1.5 * 365.25, 3.0 * 365.25],\n",
    "        # QBO (Narrow Band)\n",
    "        'QBO_2.2y': [2.0 * 365.25, 2.4 * 365.25]\n",
    "        # Note: The 22y Hale cycle is too long, not suitable for energy envelope testing (it is the driver scale itself)\n",
    "    }\n",
    "\n",
    "    # --- 2. Define model split points ---\n",
    "    model_splits = {\n",
    "        'M8+2': '1996-08-01', \n",
    "        'M8+3': '1986-08-01',\n",
    "        'M0+3': '1986-09-01', \n",
    "        'M0+2': '1996-08-01'\n",
    "    }\n",
    "    \n",
    "    test_end_date = '2019-11-30'\n",
    "    \n",
    "    # --- 3. Load Data ---\n",
    "    try:\n",
    "        data_source_file ='../../results/05_p_m_a_model/p_model_4/residual/Summary_Fit_Results.csv' \n",
    "        # Columns: ['Date', 'Raw_SSN', 'Smoothed_SSN', 'SIDC_SSN', 'Fit_SSN_M8+2', 'Residual_M8+2', ...]\n",
    "        print(f\"Loading source data: {data_source_file}\")\n",
    "        # Note: Changed 'æ—¥æœŸ' to 'Date'\n",
    "        full_data = pd.read_csv(data_source_file, parse_dates=['Date'])\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal Error: Cannot read {data_source_file}. Error: {e}\")\n",
    "        exit()\n",
    "\n",
    "    \n",
    "    # --- 4. Start Screening ---\n",
    "    all_results = []\n",
    "    \n",
    "    print(\"\\n=== Start Screening for 'Energy Envelope Modulation' of Physical Bands ===\")\n",
    "    \n",
    "    for model_name, split_date in model_splits.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing Model: {model_name} (Train End: {split_date})\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Check if required columns exist (Using English names now)\n",
    "        target_col = f'Residual_{model_name}'\n",
    "        driver_col = f'Fit_SSN_{model_name}'\n",
    "        if target_col not in full_data.columns or driver_col not in full_data.columns:\n",
    "            print(f\"Warning: Missing {target_col} or {driver_col}. Skipping this model.\")\n",
    "            continue\n",
    "\n",
    "        for band_name, freq_band_days in BANDS_TO_TEST.items():\n",
    "            result = run_modulation_test(\n",
    "                data_full=full_data,\n",
    "                model_name=model_name,\n",
    "                train_end_date=split_date,\n",
    "                test_end_date=test_end_date,\n",
    "                band_name=band_name,\n",
    "                freq_band_days=freq_band_days,\n",
    "                envelope_smoothing_days=365 # Use 1-year smoothing window\n",
    "            )\n",
    "            if result:\n",
    "                all_results.append(result)\n",
    "\n",
    "    # --- 5. Summary Report ---\n",
    "    if not all_results:\n",
    "        print(\"\\nAll analyses failed.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"      Energy Envelope Modulation Screening - Final Summary Report\")\n",
    "    print(\"      (Testing Y_Envelope = f(X_Planetary_Fit) R2 score on Test Set)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Calculate average R2 for each band across 4 models\n",
    "    summary = results_df.pivot_table(\n",
    "        index='band_name', \n",
    "        columns='model', \n",
    "        values='r2_score'\n",
    "    )\n",
    "    \n",
    "    # Calculate mean R2 and sort\n",
    "    summary['Average_R2'] = summary.mean(axis=1)\n",
    "    summary = summary.sort_values(by='Average_R2', ascending=False)\n",
    "    \n",
    "    print(summary.to_string(float_format=\"%.4f\"))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Conclusion:\")\n",
    "    top_band = summary.index[0]\n",
    "    top_r2 = summary.iloc[0]['Average_R2']\n",
    "    \n",
    "    if top_r2 > 0.05:\n",
    "        print(f\"âœ… Strong Evidence: Band '{top_band}' (Avg R2={top_r2:.4f}) energy envelope\")\n",
    "        print(\"   has a strong modulation relationship with the 11-year planetary cycle.\")\n",
    "        print(f\"\\nðŸ‘‰ Recommendation: In the LGBM full frequency comb model, focus on modulation features of the {top_band} band.\")\n",
    "    elif top_r2 > 0.01:\n",
    "        print(f\"âœ… Valid Evidence: Band '{top_band}' (Avg R2={top_r2:.4f}) shows a weak but stable modulation relationship.\")\n",
    "    else:\n",
    "        print(\"âŒ Insufficient Evidence: Correlation (R2) between energy envelopes of all bands and the 11-year cycle is very low.\")\n",
    "        print(\"   'Mod_' modulation features in LGBM might have limited effect or depend on other mechanisms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaeb146-7367-4ee2-828c-7f249a600247",
   "metadata": {},
   "source": [
    "# Energy envelope filtering, initial fitting results are unsatisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ffc7b8-bc18-4530-a970-d6966f037802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM imported successfully.\n",
      "=== Run LGBM (Physically Screened Optimized Version) ===\n",
      "Strategy: Basic full frequency comb + (QBO+Rotation) Modulation + Target Stabilization + Strong Regularization\n",
      "\n",
      "Loading source data: ../../results/05_p_m_a_model/p_model_4/residual/Summary_Fit_Results.csv\n",
      "\n",
      "==================================================\n",
      "Processing Model: M8+2 (Train End: 1996-08-01)\n",
      "==================================================\n",
      "\n",
      "=== LGBM Optimized (QBO+Rotation Modulation): M8+2 ===\n",
      "1. Generating B0 + Full basic frequency comb features (Full cycle)...\n",
      "Basic features: 30\n",
      "2. Generating modulation features (Full cycle)...\n",
      "Selected 18 features for modulation (based on energy envelope test).\n",
      "Total features: 48 (30 Basic + 18 Modulated)\n",
      "Applying QuantileTransformer to stabilize target variable Y (Residual)...\n",
      "Data: Train 51378, Test 8521\n",
      "Training LGBM (Optimized)...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12238\n",
      "[LightGBM] [Info] Number of data points in the train set: 51378, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 0.017457\n",
      "\n",
      "LGBM Optimized R2 (on transformed Y): -0.1283\n",
      "âŒ R2 is still negative.\n",
      "Best Params: OrderedDict({'colsample_bytree': 1.0, 'learning_rate': 0.0019409933305036306, 'n_estimators': 800, 'num_leaves': 19, 'reg_alpha': 0.1, 'reg_lambda': 1.4002996995850068, 'subsample': 0.7})\n",
      "\n",
      "Feature Importance (Top 20):\n",
      "Hale_22y_cos            3069\n",
      "Hale_22y_sin            2381\n",
      "Mod_QBO_2.7y_cos        1619\n",
      "Mod_QBO_2.7y_sin        1267\n",
      "Mod_QBO_2.2y_sin        1042\n",
      "Mod_QBO_2.2y_cos         935\n",
      "Mod_QBO_1.7y_cos         732\n",
      "Mod_QBO_1.7y_sin         494\n",
      "QBO_2.7y_cos             417\n",
      "QBO_2.7y_sin             352\n",
      "QBO_2.2y_sin             214\n",
      "QBO_1.7y_sin             207\n",
      "Annual_360d_cos          198\n",
      "Mod_Rotation_30d_cos     194\n",
      "Mod_Rotation_28d_cos     172\n",
      "Mod_Rotation_27d_sin     165\n",
      "QBO_1.7y_cos             156\n",
      "Mod_Rotation_30d_sin     132\n",
      "QBO_2.2y_cos             121\n",
      "Annual_370d_sin          113\n",
      "\n",
      "In Top 20, 10 are modulation features.\n",
      "\n",
      "Saving model and Scalers...\n",
      "Model saved: ../../results/05_p_m_a_model/m_model_4/LGBM_Models_Optimized\\M8+2_best_lgbm.joblib\n",
      "Generating full cycle (to 2050) predictions...\n",
      "\n",
      "==================================================\n",
      "Processing Model: M8+3 (Train End: 1986-08-01)\n",
      "==================================================\n",
      "\n",
      "=== LGBM Optimized (QBO+Rotation Modulation): M8+3 ===\n",
      "1. Generating B0 + Full basic frequency comb features (Full cycle)...\n",
      "Basic features: 30\n",
      "2. Generating modulation features (Full cycle)...\n",
      "Selected 18 features for modulation (based on energy envelope test).\n",
      "Total features: 48 (30 Basic + 18 Modulated)\n",
      "Applying QuantileTransformer to stabilize target variable Y (Residual)...\n",
      "Data: Train 47725, Test 12174\n",
      "Training LGBM (Optimized)...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12238\n",
      "[LightGBM] [Info] Number of data points in the train set: 47725, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 0.008678\n",
      "\n",
      "LGBM Optimized R2 (on transformed Y): 0.1058\n",
      "âœ… Success! Model R2 is positive.\n",
      "Best Params: OrderedDict({'colsample_bytree': 0.8242114669109599, 'learning_rate': 0.022750876875335783, 'n_estimators': 100, 'num_leaves': 5, 'reg_alpha': 1.7740438978248774, 'reg_lambda': 20.0, 'subsample': 0.9031222307619531})\n",
      "\n",
      "Feature Importance (Top 20):\n",
      "Hale_22y_cos            75\n",
      "Mod_QBO_2.7y_cos        68\n",
      "Mod_QBO_2.7y_sin        66\n",
      "Hale_22y_sin            60\n",
      "Mod_QBO_2.2y_cos        26\n",
      "Mod_QBO_1.7y_cos        22\n",
      "Mod_QBO_2.2y_sin        22\n",
      "Mod_Rotation_27d_sin    13\n",
      "Mod_Rotation_28d_cos     9\n",
      "Mod_Rotation_26d_sin     6\n",
      "Mod_Rotation_30d_cos     6\n",
      "Mod_Rotation_27d_cos     6\n",
      "Mod_Rotation_26d_cos     5\n",
      "Mod_QBO_1.7y_sin         4\n",
      "Mod_Rotation_29d_sin     3\n",
      "QBO_1.7y_sin             2\n",
      "Mod_Rotation_30d_sin     2\n",
      "Mod_Rotation_25d_sin     2\n",
      "Mod_Rotation_29d_cos     1\n",
      "Mod_Rotation_25d_cos     1\n",
      "\n",
      "In Top 20, 17 are modulation features.\n",
      "\n",
      "Saving model and Scalers...\n",
      "Model saved: ../../results/05_p_m_a_model/m_model_4/LGBM_Models_Optimized\\M8+3_best_lgbm.joblib\n",
      "Generating full cycle (to 2050) predictions...\n",
      "\n",
      "==================================================\n",
      "Processing Model: M0+3 (Train End: 1986-09-01)\n",
      "==================================================\n",
      "\n",
      "=== LGBM Optimized (QBO+Rotation Modulation): M0+3 ===\n",
      "1. Generating B0 + Full basic frequency comb features (Full cycle)...\n",
      "Basic features: 30\n",
      "2. Generating modulation features (Full cycle)...\n",
      "Selected 18 features for modulation (based on energy envelope test).\n",
      "Total features: 48 (30 Basic + 18 Modulated)\n",
      "Applying QuantileTransformer to stabilize target variable Y (Residual)...\n",
      "Data: Train 47756, Test 12143\n",
      "Training LGBM (Optimized)...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12239\n",
      "[LightGBM] [Info] Number of data points in the train set: 47756, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score -0.004984\n",
      "\n",
      "LGBM Optimized R2 (on transformed Y): 0.0506\n",
      "âœ… Success! Model R2 is positive.\n",
      "Best Params: OrderedDict({'colsample_bytree': 0.7, 'learning_rate': 0.001, 'n_estimators': 800, 'num_leaves': 28, 'reg_alpha': 20.0, 'reg_lambda': 0.5278332778609267, 'subsample': 1.0})\n",
      "\n",
      "Feature Importance (Top 20):\n",
      "Mod_QBO_2.7y_cos    2175\n",
      "Mod_QBO_2.7y_sin    2165\n",
      "Hale_22y_cos        2160\n",
      "Mod_QBO_2.2y_sin    2067\n",
      "Mod_QBO_2.2y_cos    1841\n",
      "Mod_QBO_1.7y_cos    1600\n",
      "Mod_QBO_1.7y_sin    1305\n",
      "Hale_22y_sin        1153\n",
      "QBO_2.7y_sin         995\n",
      "QBO_2.7y_cos         962\n",
      "QBO_2.2y_sin         636\n",
      "QBO_2.2y_cos         622\n",
      "QBO_1.7y_cos         586\n",
      "QBO_1.7y_sin         509\n",
      "Annual_360d_cos      452\n",
      "Annual_370d_cos      451\n",
      "Annual_365d_cos      391\n",
      "Annual_370d_sin      378\n",
      "Annual_360d_sin      374\n",
      "B0                   205\n",
      "\n",
      "In Top 20, 6 are modulation features.\n",
      "\n",
      "Saving model and Scalers...\n",
      "Model saved: ../../results/05_p_m_a_model/m_model_4/LGBM_Models_Optimized\\M0+3_best_lgbm.joblib\n",
      "Generating full cycle (to 2050) predictions...\n",
      "\n",
      "==================================================\n",
      "Processing Model: M0+2 (Train End: 1996-08-01)\n",
      "==================================================\n",
      "\n",
      "=== LGBM Optimized (QBO+Rotation Modulation): M0+2 ===\n",
      "1. Generating B0 + Full basic frequency comb features (Full cycle)...\n",
      "Basic features: 30\n",
      "2. Generating modulation features (Full cycle)...\n",
      "Selected 18 features for modulation (based on energy envelope test).\n",
      "Total features: 48 (30 Basic + 18 Modulated)\n",
      "Applying QuantileTransformer to stabilize target variable Y (Residual)...\n",
      "Data: Train 51378, Test 8521\n",
      "Training LGBM (Optimized)...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12238\n",
      "[LightGBM] [Info] Number of data points in the train set: 51378, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 0.012865\n",
      "\n",
      "LGBM Optimized R2 (on transformed Y): -0.0508\n",
      "âŒ R2 is still negative.\n",
      "Best Params: OrderedDict({'colsample_bytree': 0.9226074871851303, 'learning_rate': 0.002805668556494275, 'n_estimators': 800, 'num_leaves': 5, 'reg_alpha': 20.0, 'reg_lambda': 0.1, 'subsample': 0.885457770218165})\n",
      "\n",
      "Feature Importance (Top 20):\n",
      "Mod_QBO_2.7y_cos        654\n",
      "Hale_22y_cos            536\n",
      "Mod_QBO_2.2y_sin        404\n",
      "Mod_QBO_2.7y_sin        345\n",
      "Mod_QBO_2.2y_cos        298\n",
      "Hale_22y_sin            173\n",
      "Mod_QBO_1.7y_cos        150\n",
      "Mod_QBO_1.7y_sin        126\n",
      "Mod_Rotation_30d_sin     96\n",
      "Mod_Rotation_28d_cos     66\n",
      "Mod_Rotation_30d_cos     64\n",
      "QBO_2.7y_sin             53\n",
      "Mod_Rotation_28d_sin     51\n",
      "Mod_Rotation_27d_cos     49\n",
      "QBO_2.7y_cos             43\n",
      "QBO_1.7y_sin             26\n",
      "Mod_Rotation_27d_sin     15\n",
      "Mod_Rotation_26d_sin     13\n",
      "Annual_360d_cos          11\n",
      "Annual_365d_cos           6\n",
      "\n",
      "In Top 20, 13 are modulation features.\n",
      "\n",
      "Saving model and Scalers...\n",
      "Model saved: ../../results/05_p_m_a_model/m_model_4/LGBM_Models_Optimized\\M0+2_best_lgbm.joblib\n",
      "Generating full cycle (to 2050) predictions...\n",
      "\n",
      "============================================================\n",
      "Saving optimized quadratic fit and residuals for all models to: ../../results/05_p_m_a_model/m_model_4/LGBM_Optimized.csv\n",
      "Save successful. File contains 73780 rows.\n",
      "\n",
      "============================================================\n",
      "LGBM Physically Screened Optimized Version Summary\n",
      "============================================================\n",
      "model  r2_score  n_feat_total  n_feat_mod  top_20_mod  time(s)\n",
      " M8+2   -0.1283            48          18          10 157.5105\n",
      " M8+3    0.1058            48          18          17  98.8764\n",
      " M0+3    0.0506            48          18           6 120.0840\n",
      " M0+2   -0.0508            48          18          13 104.6840\n",
      "\n",
      "Average R2 (on transformed Y): -0.0057\n",
      "âŒ Note: Optimized R2 (-0.0057) is lower than baseline (approx 0.12).\n",
      "   This might mean there are still weak signals in the deleted features.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(\"LightGBM imported successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Error: lightgbm library not found.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    from astropy.time import Time\n",
    "    import astropy.units as u\n",
    "    from sunpy.coordinates import sun\n",
    "except ImportError:\n",
    "    print(\"Missing astronomy libraries.\")\n",
    "    exit()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_lgbm_optimized_comb_validation( # Renamed function\n",
    "    data_full, model_name, train_start_date, train_end_date, test_end_date, model_save_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Run LGBM (Optimized)\n",
    "    - Features: B0 + All basic cycles\n",
    "    - Modulation: Only modulate QBO and Rotation (based on energy envelope screening)\n",
    "    - Y Transform: QuantileTransformer\n",
    "    - Tuning: Strong regularization\n",
    "    - Functionality: Save model and full-cycle predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    data = data_full.copy() \n",
    "    # Update column names to English\n",
    "    target_residual_column = f'Residual_{model_name}'\n",
    "    planetary_fit_column = f'Fit_SSN_{model_name}'\n",
    "    \n",
    "    # Create directory for saving models\n",
    "    os.makedirs(model_save_dir, exist_ok=True) \n",
    "    \n",
    "    print(f\"\\n=== LGBM Optimized (QBO+Rotation Modulation): {model_name} ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if target_residual_column not in data.columns or planetary_fit_column not in data.columns:\n",
    "        print(f\"Error: Missing column {target_residual_column} or {planetary_fit_column}\")\n",
    "        return None, None\n",
    "\n",
    "    # --- 1. Feature Engineering: B0 + Full Frequency Comb (All dates) ---\n",
    "    print(\"1. Generating B0 + Full basic frequency comb features (Full cycle)...\")\n",
    "    times_astropy = Time(data['Date'].values)\n",
    "    \n",
    "    original_feature_names = []\n",
    "    \n",
    "    # B0 (Solar B0 angle)\n",
    "    b0_deg = sun.B0(times_astropy).deg\n",
    "    data['B0'] = b0_deg\n",
    "    data['B0_sq'] = b0_deg ** 2\n",
    "    original_feature_names.extend(['B0', 'B0_sq'])\n",
    "    \n",
    "    days_since_start = (data['Date'] - data['Date'].min()).dt.days\n",
    "    \n",
    "    # 1. Hale (22y)\n",
    "    hale_rad = 2 * np.pi * days_since_start / (22.0 * 365.25)\n",
    "    data['Hale_22y_sin'] = np.sin(hale_rad)\n",
    "    data['Hale_22y_cos'] = np.cos(hale_rad)\n",
    "    original_feature_names.extend(['Hale_22y_sin', 'Hale_22y_cos'])\n",
    "\n",
    "    # 2. QBO (1.5y - 3y)\n",
    "    qbo_periods = [1.7, 2.2, 2.7] \n",
    "    for p in qbo_periods:\n",
    "        rad = 2 * np.pi * days_since_start / (p * 365.25)\n",
    "        name = f'QBO_{p:.1f}y'\n",
    "        data[f'{name}_sin'] = np.sin(rad)\n",
    "        data[f'{name}_cos'] = np.cos(rad)\n",
    "        original_feature_names.extend([f'{name}_sin', f'{name}_cos'])\n",
    "\n",
    "    # 3. Annual (1y)\n",
    "    annual_periods = [360, 365.25, 370] \n",
    "    for p in annual_periods:\n",
    "        rad = 2 * np.pi * days_since_start / p\n",
    "        name = f'Annual_{p:.0f}d'\n",
    "        data[f'{name}_sin'] = np.sin(rad)\n",
    "        data[f'{name}_cos'] = np.cos(rad)\n",
    "        original_feature_names.extend([f'{name}_sin', f'{name}_cos'])\n",
    "\n",
    "    # 4. Rieger (154d)\n",
    "    rieger_rad = 2 * np.pi * days_since_start / 154\n",
    "    data['Rieger_154d_sin'] = np.sin(rieger_rad)\n",
    "    data['Rieger_154d_cos'] = np.cos(rieger_rad)\n",
    "    original_feature_names.extend(['Rieger_154d_sin', 'Rieger_154d_cos'])\n",
    "\n",
    "    # 5. Rotation (25d-30d)\n",
    "    rotation_periods = [25, 26, 27, 28, 29, 30] \n",
    "    for p in rotation_periods:\n",
    "        rad = 2 * np.pi * days_since_start / p\n",
    "        name = f'Rotation_{p:.0f}d'\n",
    "        data[f'{name}_sin'] = np.sin(rad)\n",
    "        data[f'{name}_cos'] = np.cos(rad)\n",
    "        original_feature_names.extend([f'{name}_sin', f'{name}_cos'])\n",
    "    \n",
    "    print(f\"Basic features: {len(original_feature_names)}\")\n",
    "\n",
    "    # --- 2. Modulation Feature Engineering (Refactored) ---\n",
    "    print(\"2. Generating modulation features (Full cycle)...\")\n",
    "    \n",
    "    train_mask_dates = (data['Date'] >= '1855-12-02') & (data['Date'] <= train_end_date)\n",
    "    train_mask_clean = train_mask_dates & data[target_residual_column].notna() & data[planetary_fit_column].notna()\n",
    "    \n",
    "    if train_mask_clean.sum() == 0:\n",
    "        print(f\"Error: No clean training data between {train_start_date} and {train_end_date}.\")\n",
    "        return None, None\n",
    "        \n",
    "    planetary_fit_values_train = data.loc[train_mask_clean, [planetary_fit_column]]\n",
    "    scaler_fit = StandardScaler()\n",
    "    scaler_fit.fit(planetary_fit_values_train)\n",
    "    \n",
    "    planetary_fit_values_full = data[[planetary_fit_column]].fillna(0) \n",
    "    data['Planetary_Scaled'] = scaler_fit.transform(planetary_fit_values_full).flatten()\n",
    "    \n",
    "    # --- Key Modification: Based on energy envelope screening results ---\n",
    "    #    Only create modulation features for Rotation and QBO\n",
    "    \n",
    "    # Define list of basic feature prefixes allowed to be modulated\n",
    "    modulation_allow_list = ['QBO_', 'Rotation_']\n",
    "    \n",
    "    features_to_modulate = []\n",
    "    for feat_name in original_feature_names:\n",
    "        # Check if feature name starts with 'QBO_' or 'Rotation_'\n",
    "        if any(feat_name.startswith(prefix) for prefix in modulation_allow_list):\n",
    "            features_to_modulate.append(feat_name)\n",
    "\n",
    "    print(f\"Selected {len(features_to_modulate)} features for modulation (based on energy envelope test).\")\n",
    "    \n",
    "    # Create modulation features (only for allowed list)\n",
    "    modulation_feature_names = []\n",
    "    for feature in features_to_modulate: # <--- Loop only through filtered subset\n",
    "        new_feature_name = f'Mod_{feature}'\n",
    "        data.loc[:, new_feature_name] = data['Planetary_Scaled'] * data[feature]\n",
    "        modulation_feature_names.append(new_feature_name)\n",
    "    \n",
    "    # --- End of Modification ---\n",
    "    \n",
    "    feature_names = original_feature_names + modulation_feature_names\n",
    "    print(f\"Total features: {len(feature_names)} (30 Basic + {len(modulation_feature_names)} Modulated)\")\n",
    "    \n",
    "    # --- 3. Data Split and Transformation (For Training) ---\n",
    "    data_clean = data.dropna(subset=[target_residual_column] + feature_names)\n",
    "    \n",
    "    X = data_clean[feature_names]\n",
    "    Y_raw = data_clean[target_residual_column]\n",
    "    groups = data_clean['Date'].dt.year\n",
    "    dates = data_clean['Date']\n",
    "    \n",
    "    train_mask = (dates >= train_start_date) & (dates <= train_end_date)\n",
    "    test_start_date_dt = pd.to_datetime(train_end_date) + pd.Timedelta(days=1)\n",
    "    test_mask = (dates >= test_start_date_dt) & (dates <= test_end_date)\n",
    "    \n",
    "    X_train_raw = X[train_mask]\n",
    "    Y_train_raw = Y_raw[train_mask]\n",
    "    X_test_raw = X[test_mask]\n",
    "    Y_test_raw = Y_raw[test_mask]\n",
    "    groups_train = groups[train_mask]\n",
    "    \n",
    "    if len(X_train_raw) == 0 or len(X_test_raw) == 0:\n",
    "        print(\"Error: Train or Test set is empty\")\n",
    "        return None, None\n",
    "\n",
    "    # a. Fit X Transformer\n",
    "    feature_scaler = StandardScaler()\n",
    "    X_train = feature_scaler.fit_transform(X_train_raw)\n",
    "    X_test = feature_scaler.transform(X_test_raw)\n",
    "\n",
    "    # b. Fit Y Transformer\n",
    "    print(\"Applying QuantileTransformer to stabilize target variable Y (Residual)...\")\n",
    "    target_transformer = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "    Y_train = target_transformer.fit_transform(Y_train_raw.values.reshape(-1, 1)).flatten()\n",
    "    Y_test = target_transformer.transform(Y_test_raw.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    print(f\"Data: Train {len(X_train)}, Test {len(X_test)}\")\n",
    "    \n",
    "    # --- 4. Bayesian Optimization - LGBM (Strong Regularization) ---\n",
    "    n_splits = max(2, min(5, groups_train.nunique()))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    search_spaces = {\n",
    "        'n_estimators': Integer(100, 800),\n",
    "        'num_leaves': Integer(5, 30),\n",
    "        'learning_rate': Real(1e-3, 0.1, 'log-uniform'),\n",
    "        'reg_alpha': Real(0.1, 20.0, 'log-uniform'), \n",
    "        'reg_lambda': Real(0.1, 20.0, 'log-uniform'),\n",
    "        'subsample': Real(0.7, 1.0, 'uniform'),\n",
    "        'colsample_bytree': Real(0.7, 1.0, 'uniform')\n",
    "    }\n",
    "    \n",
    "    lgb_model = lgb.LGBMRegressor(random_state=42, n_jobs=1, \n",
    "                                  objective='regression_l1')\n",
    "\n",
    "    opt_lgbm = BayesSearchCV(\n",
    "        lgb_model, \n",
    "        search_spaces, \n",
    "        n_iter=30, \n",
    "        cv=gkf, \n",
    "        n_jobs=-1,\n",
    "        random_state=42, \n",
    "        scoring='r2'\n",
    "    )\n",
    "    \n",
    "    print(\"Training LGBM (Optimized)...\")\n",
    "    opt_lgbm.fit(X_train, Y_train, groups=groups_train)\n",
    "    \n",
    "    \n",
    "    # --- 5. Result Analysis ---\n",
    "    best_model = opt_lgbm.best_estimator_\n",
    "    Y_pred_test = best_model.predict(X_test)\n",
    "    final_r2 = r2_score(Y_test, Y_pred_test) \n",
    "    \n",
    "    print(f\"\\nLGBM Optimized R2 (on transformed Y): {final_r2:.4f}\")\n",
    "    if final_r2 > 0:\n",
    "        print(\"âœ… Success! Model R2 is positive.\")\n",
    "    else:\n",
    "        print(\"âŒ R2 is still negative.\")\n",
    "        \n",
    "    print(f\"Best Params: {opt_lgbm.best_params_}\")\n",
    "    \n",
    "    importances = pd.Series(best_model.feature_importances_, index=feature_names)\n",
    "    print(\"\\nFeature Importance (Top 20):\")\n",
    "    top_features = importances.sort_values(ascending=False).head(20) \n",
    "    print(top_features.to_string())\n",
    "    \n",
    "    mod_features_in_top = [f for f in top_features.index if f.startswith('Mod_')]\n",
    "    print(f\"\\nIn Top 20, {len(mod_features_in_top)} are modulation features.\")\n",
    "    \n",
    "    # --- 6. New Functionality: Save Model ---\n",
    "    print(\"\\nSaving model and Scalers...\")\n",
    "    # Use the model_save_dir variable\n",
    "    model_path = os.path.join(model_save_dir, f\"{model_name}_best_lgbm.joblib\")\n",
    "    fscaler_path = os.path.join(model_save_dir, f\"{model_name}_feature_scaler.joblib\")\n",
    "    tscaler_path = os.path.join(model_save_dir, f\"{model_name}_target_transformer.joblib\")\n",
    "    \n",
    "    joblib.dump(best_model, model_path)\n",
    "    joblib.dump(feature_scaler, fscaler_path)\n",
    "    joblib.dump(target_transformer, tscaler_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "    # --- 7. New Functionality: Generate Full Cycle Predictions ---\n",
    "    print(\"Generating full cycle (to 2050) predictions...\")\n",
    "    X_full = data[feature_names].fillna(0) \n",
    "    \n",
    "    X_full_scaled = feature_scaler.transform(X_full)\n",
    "    Y_pred_full_transformed = best_model.predict(X_full_scaled)\n",
    "    \n",
    "    Y_pred_full_raw = target_transformer.inverse_transform(Y_pred_full_transformed.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    pred_col_name = f'LGBM_Pred_Opt_{model_name}' # Optimized column name\n",
    "    resid_col_name = f'LGBM_Resid_Opt_{model_name}' # Optimized column name\n",
    "    \n",
    "    data[pred_col_name] = Y_pred_full_raw\n",
    "    data[resid_col_name] = data[target_residual_column] - data[pred_col_name]\n",
    "    \n",
    "    stats_dict = {\n",
    "        'model': model_name,\n",
    "        'r2_score_transformed': final_r2,\n",
    "        'n_features': len(feature_names),\n",
    "        'n_mod_features': len(modulation_feature_names),\n",
    "        'top_20_mod_count': len(mod_features_in_top),\n",
    "        'best_params': opt_lgbm.best_params_,\n",
    "        'training_time': time.time() - start_time\n",
    "    }\n",
    "\n",
    "    return data[['Date', pred_col_name, resid_col_name]], stats_dict\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    model_splits = {\n",
    "        'M8+2': '1996-08-01', \n",
    "        'M8+3': '1986-08-01',\n",
    "        'M0+3': '1986-09-01', \n",
    "        'M0+2': '1996-08-01'\n",
    "    }\n",
    "    \n",
    "    # Define directory for joblib files\n",
    "    # You can change this path if needed\n",
    "    MODEL_SAVE_DIR = \"../../results/05_p_m_a_model/m_model_4/LGBM_Models_Optimized\" \n",
    "\n",
    "    print(\"=== Run LGBM (Physically Screened Optimized Version) ===\")\n",
    "    print(\"Strategy: Basic full frequency comb + (QBO+Rotation) Modulation + Target Stabilization + Strong Regularization\")\n",
    "    \n",
    "    try:\n",
    "        # Note: Updated to match Code 2's input path\n",
    "        data_source_file = \"../../results/05_p_m_a_model/p_model_4/residual/Summary_Fit_Results.csv\"\n",
    "        print(f\"\\nLoading source data: {data_source_file}\")\n",
    "        full_data_to_save = pd.read_csv(data_source_file, parse_dates=['Date'])\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal Error: Cannot read {data_source_file}. Error: {e}\")\n",
    "        exit()\n",
    "\n",
    "    max_date_in_file = full_data_to_save['Date'].max()\n",
    "    target_date = pd.to_datetime('2050-12-31')\n",
    "    \n",
    "    if max_date_in_file < target_date:\n",
    "        print(f\"Warning: Source data max date is {max_date_in_file}.\")\n",
    "        print(f\"Warning: Extending date range to {target_date}.\")\n",
    "        all_dates_df = pd.DataFrame({'Date': pd.date_range(\n",
    "            start=full_data_to_save['Date'].min(), \n",
    "            end=target_date, \n",
    "            freq='D'\n",
    "        )})\n",
    "        full_data_to_save = pd.merge(all_dates_df, full_data_to_save, on='Date', how='left')\n",
    "\n",
    "    results_stats_list = []\n",
    "    \n",
    "    for model_name, split_date in model_splits.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing Model: {model_name} (Train End: {split_date})\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        result_df, stats_dict = run_lgbm_optimized_comb_validation(\n",
    "            data_full=full_data_to_save.copy(), \n",
    "            model_name=model_name,\n",
    "            train_start_date='1855-12-02',\n",
    "            train_end_date=split_date,\n",
    "            test_end_date='2019-11-30',\n",
    "            model_save_dir=MODEL_SAVE_DIR\n",
    "        )\n",
    "        \n",
    "        if result_df is not None and stats_dict is not None:\n",
    "            results_stats_list.append(stats_dict)\n",
    "            full_data_to_save = pd.merge(full_data_to_save, result_df, on='Date', how='left')\n",
    "    \n",
    "    if results_stats_list:\n",
    "        # Output filename as provided in the prompt\n",
    "        output_filename = \"../../results/05_p_m_a_model/m_model_4/LGBM_Optimized.csv\" \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Saving optimized quadratic fit and residuals for all models to: {output_filename}\")\n",
    "        try:\n",
    "            full_data_to_save.to_csv(output_filename, index=False, float_format='%.6f')\n",
    "            print(f\"Save successful. File contains {len(full_data_to_save)} rows.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save CSV: {e}\")\n",
    "    \n",
    "        results_to_print = []\n",
    "        for r in results_stats_list:\n",
    "            results_to_print.append({\n",
    "                'model': r['model'],\n",
    "                'r2_score': r['r2_score_transformed'],\n",
    "                'n_feat_total': r['n_features'],\n",
    "                'n_feat_mod': r['n_mod_features'],\n",
    "                'top_20_mod': r['top_20_mod_count'],\n",
    "                'time(s)': r['training_time']\n",
    "            })\n",
    "            \n",
    "        results_df = pd.DataFrame(results_to_print)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LGBM Physically Screened Optimized Version Summary\")\n",
    "        print(\"=\"*60)\n",
    "        print(results_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "        \n",
    "        avg_r2 = results_df['r2_score'].mean()\n",
    "        print(f\"\\nAverage R2 (on transformed Y): {avg_r2:.4f}\")\n",
    "        \n",
    "        # Compare with previous baseline 0.1265 (or 0.1198)\n",
    "        if avg_r2 > 0.13:\n",
    "            print(f\"âœ…âœ… Huge Success: Optimized R2 ({avg_r2:.4f}) is higher than baseline (approx 0.12)!\")\n",
    "        elif avg_r2 > 0.11:\n",
    "            print(f\"âœ… Success: Optimized R2 ({avg_r2:.4f}) is comparable to baseline (approx 0.12).\")\n",
    "            print(\"   This proves the model robustness, and it has stronger physical meaning.\")\n",
    "        else:\n",
    "            print(f\"âŒ Note: Optimized R2 ({avg_r2:.4f}) is lower than baseline (approx 0.12).\")\n",
    "            print(\"   This might mean there are still weak signals in the deleted features.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nAll models failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608fbb0-9aed-4354-949c-1c3d9d4c119a",
   "metadata": {},
   "source": [
    "# Supplementing long-term information that may be missed by planetary fitting methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47796fe6-75a5-4e60-845e-ad68e065d073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM imported successfully.\n",
      "=== Run LGBM (Full Frequency Comb Version: B0 + 22y, QBO, 1y, 154d, 25-30d) ===\n",
      "Strategy: Physical Full Frequency Comb + 11-year Modulation + Target Stabilization + Strong Regularization\n",
      "New: Save model and full cycle secondary residuals\n",
      "\n",
      "Loading source data: ../../results/05_p_m_a_model/p_model_4/residual/Summary_Fit_Results.csv\n",
      "\n",
      "==================================================\n",
      "Processing Model: M8+2 (Train End: 1996-08-01)\n",
      "==================================================\n",
      "\n",
      "=== LGBM Final Full Frequency Comb Version: M8+2 ===\n",
      "1. Generating B0 Ephemeris Features + Full Frequency Comb Features (Full Cycle)...\n",
      "2. Generating modulation features (Full Cycle)...\n",
      "... Adding 'Planetary_Scaled_Diff' (Rate of Change) feature...\n",
      "Total features: 62 (Full Frequency Comb + Rate of Change)\n",
      "Applying QuantileTransformer to stabilize target variable Y (Residual)...\n",
      "Data: Train 51378, Test 8521\n",
      "Training LGBM (Full Frequency Comb Version)...\n",
      "\n",
      "LGBM Full Comb Version R2 (on transformed Y): 0.0159\n",
      "âœ… Success! Model R2 is positive.\n",
      "Best Params: OrderedDict({'colsample_bytree': 1.0, 'learning_rate': 0.0034624782266994576, 'n_estimators': 457, 'num_leaves': 14, 'reg_alpha': 20.0, 'reg_lambda': 0.19385885312249856, 'subsample': 0.9491687343141624})\n",
      "\n",
      "Feature Importance (Top 20):\n",
      "Mod_Hale_22y_sin        1492\n",
      "Hale_22y_cos            1088\n",
      "Hale_22y_sin             808\n",
      "Mod_Hale_22y_cos         763\n",
      "Mod_QBO_2.7y_cos         424\n",
      "Mod_QBO_2.2y_sin         206\n",
      "Mod_QBO_2.7y_sin         191\n",
      "QBO_2.7y_cos             123\n",
      "Mod_Annual_370d_cos      118\n",
      "Mod_Annual_365d_sin       80\n",
      "Mod_QBO_2.2y_cos          80\n",
      "Mod_B0_sq                 76\n",
      "QBO_2.7y_sin              54\n",
      "Mod_QBO_1.7y_cos          49\n",
      "QBO_2.2y_cos              46\n",
      "QBO_1.7y_sin              45\n",
      "Mod_Annual_360d_sin       42\n",
      "Mod_Rotation_27d_sin      40\n",
      "Mod_Rotation_28d_cos      30\n",
      "Mod_Annual_360d_cos       29\n",
      "\n",
      "In Top 20, 14 are modulation features.\n",
      "\n",
      "Saving model and Scalers...\n",
      "Model saved: ../../results/05_p_m_a_model/m_model_4/LGBM_Models1\\M8+2_best_lgbm.joblib\n",
      "Generating full cycle (to 2050) predictions...\n",
      "\n",
      "==================================================\n",
      "Processing Model: M8+3 (Train End: 1986-08-01)\n",
      "==================================================\n",
      "\n",
      "=== LGBM Final Full Frequency Comb Version: M8+3 ===\n",
      "1. Generating B0 Ephemeris Features + Full Frequency Comb Features (Full Cycle)...\n",
      "2. Generating modulation features (Full Cycle)...\n",
      "... Adding 'Planetary_Scaled_Diff' (Rate of Change) feature...\n",
      "Total features: 62 (Full Frequency Comb + Rate of Change)\n",
      "Applying QuantileTransformer to stabilize target variable Y (Residual)...\n",
      "Data: Train 47725, Test 12174\n",
      "Training LGBM (Full Frequency Comb Version)...\n",
      "\n",
      "LGBM Full Comb Version R2 (on transformed Y): 0.1205\n",
      "âœ… Success! Model R2 is positive.\n",
      "Best Params: OrderedDict({'colsample_bytree': 0.8342139851845605, 'learning_rate': 0.003923610713110215, 'n_estimators': 680, 'num_leaves': 5, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0})\n",
      "\n",
      "Feature Importance (Top 20):\n",
      "Mod_Hale_22y_sin        758\n",
      "Mod_Hale_22y_cos        488\n",
      "Hale_22y_cos            422\n",
      "Hale_22y_sin            368\n",
      "Mod_QBO_2.7y_cos        231\n",
      "Mod_QBO_2.7y_sin         79\n",
      "Mod_Annual_360d_cos      52\n",
      "Mod_Annual_370d_cos      43\n",
      "Mod_QBO_2.2y_cos         30\n",
      "Mod_B0_sq                29\n",
      "Mod_B0                   29\n",
      "Mod_Annual_365d_cos      26\n",
      "Mod_Annual_360d_sin      26\n",
      "Mod_Rotation_27d_sin     26\n",
      "Mod_Rotation_28d_cos     20\n",
      "Annual_370d_sin          18\n",
      "Annual_370d_cos          17\n",
      "Mod_QBO_2.2y_sin         15\n",
      "Mod_QBO_1.7y_cos         13\n",
      "Mod_Rieger_154d_cos       6\n",
      "\n",
      "In Top 20, 16 are modulation features.\n",
      "\n",
      "Saving model and Scalers...\n",
      "Model saved: ../../results/05_p_m_a_model/m_model_4/LGBM_Models1\\M8+3_best_lgbm.joblib\n",
      "Generating full cycle (to 2050) predictions...\n",
      "\n",
      "==================================================\n",
      "Processing Model: M0+3 (Train End: 1986-09-01)\n",
      "==================================================\n",
      "\n",
      "=== LGBM Final Full Frequency Comb Version: M0+3 ===\n",
      "1. Generating B0 Ephemeris Features + Full Frequency Comb Features (Full Cycle)...\n",
      "2. Generating modulation features (Full Cycle)...\n",
      "... Adding 'Planetary_Scaled_Diff' (Rate of Change) feature...\n",
      "Total features: 62 (Full Frequency Comb + Rate of Change)\n",
      "Applying QuantileTransformer to stabilize target variable Y (Residual)...\n",
      "Data: Train 47756, Test 12143\n",
      "Training LGBM (Full Frequency Comb Version)...\n",
      "\n",
      "LGBM Full Comb Version R2 (on transformed Y): 0.0881\n",
      "âœ… Success! Model R2 is positive.\n",
      "Best Params: OrderedDict({'colsample_bytree': 0.7, 'learning_rate': 0.005147865132149854, 'n_estimators': 412, 'num_leaves': 5, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0})\n",
      "\n",
      "Feature Importance (Top 20):\n",
      "Mod_Hale_22y_cos             464\n",
      "Mod_Hale_22y_sin             435\n",
      "Hale_22y_cos                 243\n",
      "Mod_QBO_2.7y_cos              96\n",
      "Planetary_Scaled_Diff         54\n",
      "Hale_22y_sin                  54\n",
      "Mod_B0_sq                     51\n",
      "QBO_2.7y_sin                  45\n",
      "Mod_QBO_2.7y_sin              42\n",
      "Mod_Planetary_Scaled_Diff     37\n",
      "Mod_QBO_1.7y_cos              29\n",
      "Mod_QBO_2.2y_sin              23\n",
      "Mod_QBO_2.2y_cos              22\n",
      "Mod_Annual_365d_cos           11\n",
      "Mod_QBO_1.7y_sin              11\n",
      "Mod_B0                         8\n",
      "Mod_Annual_360d_sin            7\n",
      "Mod_Annual_370d_cos            6\n",
      "QBO_2.7y_cos                   5\n",
      "Mod_Annual_365d_sin            3\n",
      "\n",
      "In Top 20, 15 are modulation features.\n",
      "\n",
      "Saving model and Scalers...\n",
      "Model saved: ../../results/05_p_m_a_model/m_model_4/LGBM_Models1\\M0+3_best_lgbm.joblib\n",
      "Generating full cycle (to 2050) predictions...\n",
      "\n",
      "==================================================\n",
      "Processing Model: M0+2 (Train End: 1996-08-01)\n",
      "==================================================\n",
      "\n",
      "=== LGBM Final Full Frequency Comb Version: M0+2 ===\n",
      "1. Generating B0 Ephemeris Features + Full Frequency Comb Features (Full Cycle)...\n",
      "2. Generating modulation features (Full Cycle)...\n",
      "... Adding 'Planetary_Scaled_Diff' (Rate of Change) feature...\n",
      "Total features: 62 (Full Frequency Comb + Rate of Change)\n",
      "Applying QuantileTransformer to stabilize target variable Y (Residual)...\n",
      "Data: Train 51378, Test 8521\n",
      "Training LGBM (Full Frequency Comb Version)...\n",
      "\n",
      "LGBM Full Comb Version R2 (on transformed Y): 0.0406\n",
      "âœ… Success! Model R2 is positive.\n",
      "Best Params: OrderedDict({'colsample_bytree': 0.7, 'learning_rate': 0.015328827815394233, 'n_estimators': 226, 'num_leaves': 5, 'reg_alpha': 0.1, 'reg_lambda': 20.0, 'subsample': 1.0})\n",
      "\n",
      "Feature Importance (Top 20):\n",
      "Mod_Hale_22y_sin             231\n",
      "Mod_Hale_22y_cos             150\n",
      "Planetary_Scaled_Diff        121\n",
      "Mod_Planetary_Scaled_Diff     80\n",
      "Hale_22y_cos                  80\n",
      "Mod_QBO_2.7y_cos              55\n",
      "Mod_B0_sq                     26\n",
      "Mod_QBO_2.7y_sin              25\n",
      "Mod_Annual_360d_sin           19\n",
      "Mod_QBO_2.2y_cos              19\n",
      "Mod_Annual_370d_cos           18\n",
      "Mod_QBO_1.7y_cos              16\n",
      "Mod_QBO_2.2y_sin              14\n",
      "Mod_Annual_370d_sin           12\n",
      "Mod_B0                         9\n",
      "Mod_Annual_365d_sin            8\n",
      "Mod_Annual_360d_cos            4\n",
      "QBO_1.7y_sin                   3\n",
      "Hale_22y_sin                   2\n",
      "QBO_1.7y_cos                   2\n",
      "\n",
      "In Top 20, 15 are modulation features.\n",
      "\n",
      "Saving model and Scalers...\n",
      "Model saved: ../../results/05_p_m_a_model/m_model_4/LGBM_Models1\\M0+2_best_lgbm.joblib\n",
      "Generating full cycle (to 2050) predictions...\n",
      "\n",
      "============================================================\n",
      "Saving secondary fit and residuals for all models to: ../../results/05_p_m_a_model/m_model_4/LGBM_results1.csv\n",
      "Save successful. File contains 73780 rows.\n",
      "\n",
      "============================================================\n",
      "LGBM Full Frequency Comb Version Summary\n",
      "============================================================\n",
      "model  r2_score  n_features  top_20_mod  time(s)\n",
      " M8+2    0.0159          62          14 642.3006\n",
      " M8+3    0.1205          62          16 447.5651\n",
      " M0+3    0.0881          62          15 446.5080\n",
      " M0+2    0.0406          62          15 456.9835\n",
      "\n",
      "Average R2 (on transformed Y): 0.0663\n",
      "âœ…âœ…âœ… Huge Success: All 4 models have positive R2!\n",
      "ðŸ’¡ Insight: Modulation features (Mod_) play a significant role in the models.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# --- Core: Non-linear Model Import ---\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(\"LightGBM imported successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Error: lightgbm library not found.\")\n",
    "    print(\"Please install: pip install lightgbm\")\n",
    "    exit()\n",
    "\n",
    "# Astronomy Library Import\n",
    "try:\n",
    "    from astropy.time import Time\n",
    "    import astropy.units as u\n",
    "    from sunpy.coordinates import sun\n",
    "except ImportError:\n",
    "    print(\"Missing astronomy libraries.\")\n",
    "    exit()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_lgbm_full_comb_validation(\n",
    "    data_full, model_name, train_start_date, train_end_date, \n",
    "    test_end_date, model_save_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Run LGBM (Final Full Frequency Comb Version)\n",
    "    - Features: B0 + All famous cycles (22y, QBO, 1y, 154d, 25-30d)\n",
    "    - Modulation: 11-year planetary fit\n",
    "    - Y Transform: QuantileTransformer\n",
    "    - Tuning: Strong regularization\n",
    "    - New: Save model and full-cycle predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the passed complete data\n",
    "    data = data_full.copy() \n",
    "    \n",
    "    # Update column names to English\n",
    "    target_residual_column = f'Residual_{model_name}'\n",
    "    planetary_fit_column = f'Fit_SSN_{model_name}'\n",
    "    \n",
    "    # Ensure model save directory exists\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "   \n",
    "    print(f\"\\n=== LGBM Final Full Frequency Comb Version: {model_name} ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Check if necessary columns exist\n",
    "    if target_residual_column not in data.columns or planetary_fit_column not in data.columns:\n",
    "        print(f\"Error: Missing column {target_residual_column} or {planetary_fit_column}\")\n",
    "        return None, None\n",
    "\n",
    "    # --- 1. Feature Engineering: B0 + Full Frequency Comb (All dates) ---\n",
    "    print(\"1. Generating B0 Ephemeris Features + Full Frequency Comb Features (Full Cycle)...\")\n",
    "    times_astropy = Time(data['Date'].values)\n",
    "    \n",
    "    original_feature_names = []\n",
    "    \n",
    "    # B0 (Heliographic latitude) - Real annual amplitude\n",
    "    b0_deg = sun.B0(times_astropy).deg\n",
    "    data['B0'] = b0_deg\n",
    "    data['B0_sq'] = b0_deg ** 2\n",
    "    original_feature_names.extend(['B0', 'B0_sq'])\n",
    "    \n",
    "    # Time-based fixed frequency features\n",
    "    days_since_start = (data['Date'] - data['Date'].min()).dt.days\n",
    "    \n",
    "    # === Define our Frequency Comb ===\n",
    "    \n",
    "    # 1. Hale (22y)\n",
    "    hale_rad = 2 * np.pi * days_since_start / (22.0 * 365.25)\n",
    "    data['Hale_22y_sin'] = np.sin(hale_rad)\n",
    "    data['Hale_22y_cos'] = np.cos(hale_rad)\n",
    "    original_feature_names.extend(['Hale_22y_sin', 'Hale_22y_cos'])\n",
    "\n",
    "    # 2. QBO (1.5y - 3y)\n",
    "    qbo_periods = [1.7, 2.2, 2.7] # Years\n",
    "    for p in qbo_periods:\n",
    "        rad = 2 * np.pi * days_since_start / (p * 365.25)\n",
    "        name = f'QBO_{p:.1f}y'\n",
    "        data[f'{name}_sin'] = np.sin(rad)\n",
    "        data[f'{name}_cos'] = np.cos(rad)\n",
    "        original_feature_names.extend([f'{name}_sin', f'{name}_cos'])\n",
    "\n",
    "    # 3. Annual (1y)\n",
    "    annual_periods = [360, 365.25, 370] # Days\n",
    "    for p in annual_periods:\n",
    "        rad = 2 * np.pi * days_since_start / p\n",
    "        name = f'Annual_{p:.0f}d'\n",
    "        data[f'{name}_sin'] = np.sin(rad)\n",
    "        data[f'{name}_cos'] = np.cos(rad)\n",
    "        original_feature_names.extend([f'{name}_sin', f'{name}_cos'])\n",
    "\n",
    "    # 4. Rieger (154d)\n",
    "    rieger_rad = 2 * np.pi * days_since_start / 154\n",
    "    data['Rieger_154d_sin'] = np.sin(rieger_rad)\n",
    "    data['Rieger_154d_cos'] = np.cos(rieger_rad)\n",
    "    original_feature_names.extend(['Rieger_154d_sin', 'Rieger_154d_cos'])\n",
    "\n",
    "    # 5. Rotation (25d-30d)\n",
    "    rotation_periods = [25, 26, 27, 28, 29, 30] # Days\n",
    "    for p in rotation_periods:\n",
    "        rad = 2 * np.pi * days_since_start / p\n",
    "        name = f'Rotation_{p:.0f}d'\n",
    "        data[f'{name}_sin'] = np.sin(rad)\n",
    "        data[f'{name}_cos'] = np.cos(rad)\n",
    "        original_feature_names.extend([f'{name}_sin', f'{name}_cos'])\n",
    "    \n",
    "    \n",
    "    # --- 2. Modulation Feature Engineering (Refactored) ---\n",
    "    # Goal: Fit Scaler on training set, Transform on full cycle\n",
    "    print(\"2. Generating modulation features (Full Cycle)...\")\n",
    "    \n",
    "    # a. Create training mask for fitting Scaler\n",
    "    # Training set mask (only for fitting Scaler and Model)\n",
    "    train_mask_dates = (data['Date'] >= train_start_date) & (data['Date'] <= train_end_date)\n",
    "    # Must be clean, non-NaN data\n",
    "    train_mask_clean = train_mask_dates & data[target_residual_column].notna() & data[planetary_fit_column].notna()\n",
    "    \n",
    "    if train_mask_clean.sum() == 0:\n",
    "        print(f\"Error: No clean training data between {train_start_date} and {train_end_date}.\")\n",
    "        return None, None\n",
    "        \n",
    "    # b. Fit Modulator (11-year cycle signal) Scaler (Only on training data)\n",
    "    planetary_fit_values_train = data.loc[train_mask_clean, [planetary_fit_column]]\n",
    "    scaler_fit = StandardScaler()\n",
    "    scaler_fit.fit(planetary_fit_values_train)\n",
    "    \n",
    "    # c. Transform (Full Cycle)\n",
    "    # Warning: If planetary_fit_column is NaN in the future, this will create NaNs.\n",
    "    # We fill NaNs with 0 (assuming future fit is 0 if missing, not ideal but prevents crash)\n",
    "    planetary_fit_values_full = data[[planetary_fit_column]].fillna(0) \n",
    "    data['Planetary_Scaled'] = scaler_fit.transform(planetary_fit_values_full).flatten()\n",
    "\n",
    "    # --- [ *** New Code *** ] ---\n",
    "    # 2.c.1: Create rate of change feature (1st order difference)\n",
    "    # This represents the \"Rising Phase\" (positive) or \"Declining Phase\" (negative) of the 11-year cycle\n",
    "    print(\"... Adding 'Planetary_Scaled_Diff' (Rate of Change) feature...\")\n",
    "    data['Planetary_Scaled_Diff'] = data['Planetary_Scaled'].diff(1).fillna(0)\n",
    "    \n",
    "    # 2.c.2: Add it to the *Basic* feature list\n",
    "    # Key: This way it will automatically get its own \"Mod_\" version in the next step\n",
    "    original_feature_names.append('Planetary_Scaled_Diff')\n",
    "    # --- [ *** End of New Code *** ] ---\n",
    "    \n",
    "    # d. Create Modulation Features (Full Cycle)\n",
    "    modulation_feature_names = []\n",
    "    for feature in original_feature_names:\n",
    "        new_feature_name = f'Mod_{feature}'\n",
    "        data.loc[:, new_feature_name] = data['Planetary_Scaled'] * data[feature]\n",
    "        modulation_feature_names.append(new_feature_name)\n",
    "    \n",
    "    feature_names = original_feature_names + modulation_feature_names\n",
    "    print(f\"Total features: {len(feature_names)} (Full Frequency Comb + Rate of Change)\")\n",
    "    \n",
    "    # --- 3. Data Split and Transformation (For Training) ---\n",
    "    \n",
    "    # Only use rows with valid residuals for training and testing\n",
    "    data_clean = data.dropna(subset=[target_residual_column] + feature_names)\n",
    "    \n",
    "    X = data_clean[feature_names]\n",
    "    Y_raw = data_clean[target_residual_column] # Y Raw values\n",
    "    groups = data_clean['Date'].dt.year\n",
    "    dates = data_clean['Date']\n",
    "    \n",
    "    train_mask = (dates >= train_start_date) & (dates <= train_end_date)\n",
    "    test_start_date_dt = pd.to_datetime(train_end_date) + pd.Timedelta(days=1)\n",
    "    test_mask = (dates >= test_start_date_dt) & (dates <= test_end_date)\n",
    "    \n",
    "    X_train_raw = X[train_mask]\n",
    "    Y_train_raw = Y_raw[train_mask]\n",
    "    X_test_raw = X[test_mask]\n",
    "    Y_test_raw = Y_raw[test_mask]\n",
    "    groups_train = groups[train_mask]\n",
    "    \n",
    "    if len(X_train_raw) == 0 or len(X_test_raw) == 0:\n",
    "        print(\"Error: Train or Test set is empty\")\n",
    "        return None, None\n",
    "\n",
    "    # a. Fit X Transformer (StandardScaler)\n",
    "    feature_scaler = StandardScaler()\n",
    "    X_train = feature_scaler.fit_transform(X_train_raw)\n",
    "    X_test = feature_scaler.transform(X_test_raw)\n",
    "\n",
    "    # b. Fit Y Transformer (QuantileTransformer) - Key Fix\n",
    "    print(\"Applying QuantileTransformer to stabilize target variable Y (Residual)...\")\n",
    "    target_transformer = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "    Y_train = target_transformer.fit_transform(Y_train_raw.values.reshape(-1, 1)).flatten()\n",
    "    Y_test = target_transformer.transform(Y_test_raw.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    print(f\"Data: Train {len(X_train)}, Test {len(X_test)}\")\n",
    "    \n",
    "    # --- 4. Bayesian Optimization - LGBM (Strong Regularization) ---\n",
    "    n_splits = max(2, min(5, groups_train.nunique()))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    search_spaces = {\n",
    "        'n_estimators': Integer(100, 800),\n",
    "        'num_leaves': Integer(5, 30),\n",
    "        'learning_rate': Real(1e-3, 0.1, 'log-uniform'),\n",
    "        'reg_alpha': Real(0.1, 20.0, 'log-uniform'), \n",
    "        'reg_lambda': Real(0.1, 20.0, 'log-uniform'),\n",
    "        'subsample': Real(0.7, 1.0, 'uniform'),\n",
    "        'colsample_bytree': Real(0.7, 1.0, 'uniform')\n",
    "    }\n",
    "    \n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        random_state=42, \n",
    "        n_jobs=1, \n",
    "        objective='regression_l1',\n",
    "        force_row_wise=True, # Force row-wise to eliminate auto-detection warnings\n",
    "        verbose=-1           # Suppress all LightGBM internal output (recommended)\n",
    "    )\n",
    "\n",
    "    opt_lgbm = BayesSearchCV(\n",
    "        lgb_model, \n",
    "        search_spaces, \n",
    "        n_iter=100, # <-- Increased from 30 to 50 (or 100 as per code)\n",
    "        cv=gkf, \n",
    "        n_jobs=-2,\n",
    "        random_state=42, \n",
    "        scoring='r2'\n",
    "    )\n",
    "    \n",
    "    print(\"Training LGBM (Full Frequency Comb Version)...\")\n",
    "    opt_lgbm.fit(X_train, Y_train, groups=groups_train)\n",
    "    \n",
    "    # --- 5. Result Analysis ---\n",
    "    best_model = opt_lgbm.best_estimator_\n",
    "    Y_pred_test = best_model.predict(X_test)\n",
    "    final_r2 = r2_score(Y_test, Y_pred_test) \n",
    "    \n",
    "    print(f\"\\nLGBM Full Comb Version R2 (on transformed Y): {final_r2:.4f}\")\n",
    "    if final_r2 > 0:\n",
    "        print(\"âœ… Success! Model R2 is positive.\")\n",
    "    else:\n",
    "        print(\"âŒ R2 is still negative.\")\n",
    "        \n",
    "    print(f\"Best Params: {opt_lgbm.best_params_}\")\n",
    "    \n",
    "    # Feature Importance\n",
    "    importances = pd.Series(best_model.feature_importances_, index=feature_names)\n",
    "    print(\"\\nFeature Importance (Top 20):\")\n",
    "    top_features = importances.sort_values(ascending=False).head(20) \n",
    "    print(top_features.to_string())\n",
    "    \n",
    "    mod_features_in_top = [f for f in top_features.index if f.startswith('Mod_')]\n",
    "    print(f\"\\nIn Top 20, {len(mod_features_in_top)} are modulation features.\")\n",
    "    \n",
    "    # --- 6. New Functionality: Save Model ---\n",
    "    print(\"\\nSaving model and Scalers...\")\n",
    "    \n",
    "    # Updated to use model_save_dir\n",
    "    model_path = os.path.join(model_save_dir, f\"{model_name}_best_lgbm.joblib\")\n",
    "    fscaler_path = os.path.join(model_save_dir, f\"{model_name}_feature_scaler.joblib\")\n",
    "    tscaler_path = os.path.join(model_save_dir, f\"{model_name}_target_transformer.joblib\")\n",
    "    \n",
    "    joblib.dump(best_model, model_path)\n",
    "    joblib.dump(feature_scaler, fscaler_path)\n",
    "    joblib.dump(target_transformer, tscaler_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "    # --- 7. New Functionality: Generate Full Cycle Predictions ---\n",
    "    print(\"Generating full cycle (to 2050) predictions...\")\n",
    "    \n",
    "    # Prepare X features for full cycle\n",
    "    # Must fill NaNs in features (e.g., if Planetary_Scaled is NaN at the end)\n",
    "    X_full = data[feature_names].fillna(0) \n",
    "    \n",
    "    X_full_scaled = feature_scaler.transform(X_full)\n",
    "    Y_pred_full_transformed = best_model.predict(X_full_scaled)\n",
    "    \n",
    "    # Inverse transform back to original scale\n",
    "    Y_pred_full_raw = target_transformer.inverse_transform(Y_pred_full_transformed.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create prediction column and secondary residual column\n",
    "    pred_col_name = f'LGBM_Pred_{model_name}'\n",
    "    resid_col_name = f'LGBM_Resid_{model_name}'\n",
    "    \n",
    "    data[pred_col_name] = Y_pred_full_raw\n",
    "    # Original Residual - LGBM Predicted Residual = Secondary Residual\n",
    "    data[resid_col_name] = data[target_residual_column] - data[pred_col_name]\n",
    "    \n",
    "    # Prepare stats dictionary\n",
    "    stats_dict = {\n",
    "        'model': model_name,\n",
    "        'r2_score_transformed': final_r2,\n",
    "        'n_features': len(feature_names),\n",
    "        'top_20_mod_count': len(mod_features_in_top),\n",
    "        'best_params': opt_lgbm.best_params_,\n",
    "        'training_time': time.time() - start_time\n",
    "    }\n",
    "\n",
    "    # Return DataFrame with new columns (only necessary columns) and stats results\n",
    "    return data[['Date', pred_col_name, resid_col_name]], stats_dict\n",
    "\n",
    "\n",
    "# Main Execution - Run LGBM Full Frequency Comb Version\n",
    "if __name__ == \"__main__\":\n",
    "    model_splits = {\n",
    "        'M8+2': '1996-08-01', \n",
    "        'M8+3': '1986-08-01',\n",
    "        'M0+3': '1986-09-01', \n",
    "        'M0+2': '1996-08-01'\n",
    "    }\n",
    "    \n",
    "    print(\"=== Run LGBM (Full Frequency Comb Version: B0 + 22y, QBO, 1y, 154d, 25-30d) ===\")\n",
    "    print(\"Strategy: Physical Full Frequency Comb + 11-year Modulation + Target Stabilization + Strong Regularization\")\n",
    "    print(\"New: Save model and full cycle secondary residuals\")\n",
    "\n",
    "    MODEL_SAVE_DIR = \"../../results/05_p_m_a_model/m_model_4/LGBM_Models1\" \n",
    "    \n",
    "    # --- New: Load and Prepare Data Outside Loop ---\n",
    "    try:\n",
    "        data_source_file = \"../../results/05_p_m_a_model/p_model_4/residual/Summary_Fit_Results.csv\"\n",
    "        print(f\"\\nLoading source data: {data_source_file}\")\n",
    "        full_data_to_save = pd.read_csv(data_source_file, parse_dates=['Date'])\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal Error: Cannot read {data_source_file}. Error: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Check and extend dates to 2050\n",
    "    max_date_in_file = full_data_to_save['Date'].max()\n",
    "    target_date = pd.to_datetime('2050-12-31')\n",
    "    \n",
    "    if max_date_in_file < target_date:\n",
    "        print(f\"Warning: Source data max date is {max_date_in_file}.\")\n",
    "        print(f\"Warning: Extending date range to {target_date}.\")\n",
    "        print(\"Warning: Please ensure 'Fit_SSN' (Planetary Fit) column in source file is extrapolated to 2050, otherwise predictions will be inaccurate!\")\n",
    "        \n",
    "        all_dates_df = pd.DataFrame({'Date': pd.date_range(\n",
    "            start=full_data_to_save['Date'].min(), \n",
    "            end=target_date, \n",
    "            freq='D'\n",
    "        )})\n",
    "        full_data_to_save = pd.merge(all_dates_df, full_data_to_save, on='Date', how='left')\n",
    "\n",
    "    # To save stats results\n",
    "    results_stats_list = []\n",
    "    \n",
    "    # Loop through each model\n",
    "    for model_name, split_date in model_splits.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing Model: {model_name} (Train End: {split_date})\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Pass full data\n",
    "        result_df, stats_dict = run_lgbm_full_comb_validation(\n",
    "            data_full=full_data_to_save.copy(), # Pass a copy of data\n",
    "            model_name=model_name,\n",
    "            train_start_date='1855-12-02',\n",
    "            train_end_date=split_date,\n",
    "            test_end_date='2019-11-30',\n",
    "            model_save_dir=MODEL_SAVE_DIR # Pass the directory explicitly\n",
    "        )\n",
    "        \n",
    "        if result_df is not None and stats_dict is not None:\n",
    "            results_stats_list.append(stats_dict)\n",
    "            # Merge this model's results back to main DataFrame\n",
    "            full_data_to_save = pd.merge(full_data_to_save, result_df, on='Date', how='left')\n",
    "    \n",
    "    # --- New: Save CSV containing all results ---\n",
    "    if results_stats_list:\n",
    "        output_filename = \"../../results/05_p_m_a_model/m_model_4/LGBM_results1.csv\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Saving secondary fit and residuals for all models to: {output_filename}\")\n",
    "        try:\n",
    "            full_data_to_save.to_csv(output_filename, index=False, float_format='%.6f')\n",
    "            print(f\"Save successful. File contains {len(full_data_to_save)} rows.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save CSV: {e}\")\n",
    "        \n",
    "        # --- Print Summary Report (Using stats data) ---\n",
    "        results_to_print = []\n",
    "        for r in results_stats_list:\n",
    "            results_to_print.append({\n",
    "                'model': r['model'],\n",
    "                'r2_score': r['r2_score_transformed'],\n",
    "                'n_features': r['n_features'],\n",
    "                'top_20_mod': r['top_20_mod_count'],\n",
    "                'time(s)': r['training_time']\n",
    "            })\n",
    "            \n",
    "        results_df = pd.DataFrame(results_to_print)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LGBM Full Frequency Comb Version Summary\")\n",
    "        print(\"=\"*60)\n",
    "        print(results_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "        \n",
    "        avg_r2 = results_df['r2_score'].mean()\n",
    "        print(f\"\\nAverage R2 (on transformed Y): {avg_r2:.4f}\")\n",
    "        \n",
    "        positive_r2 = results_df[results_df['r2_score'] > 0]\n",
    "        if len(positive_r2) == 4:\n",
    "            print(\"âœ…âœ…âœ… Huge Success: All 4 models have positive R2!\")\n",
    "        elif len(positive_r2) > 0:\n",
    "            print(f\"âœ… Success: {len(positive_r2)} models obtained positive R2.\")\n",
    "        else:\n",
    "            print(\"âŒ Failure: All models still have negative R2.\")\n",
    "        \n",
    "        if results_df['top_20_mod'].mean() >= 10: \n",
    "             print(\"ðŸ’¡ Insight: Modulation features (Mod_) play a significant role in the models.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nAll models failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86716369-40f0-4ee3-9e1f-bb436f984abb",
   "metadata": {},
   "source": [
    "# Based on the 11 rate of change parameters, add \"acceleration.\" This is the optimal approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c96222-8020-4e3a-9743-33721bbe6894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM imported successfully.\n",
      "=== Run LGBM (Full Frequency Comb Version: B0 + 22y, QBO, 1y, 154d, 25-30d) ===\n",
      "Strategy: Physical Full Frequency Comb + 11-year Modulation + Target Stabilization + Strong Regularization\n",
      "New: Save model and full cycle secondary residuals\n",
      "\n",
      "Loading source data: ../../results/05_p_m_a_model/p_model_4/residual/Summary_Fit_Results.csv\n",
      "\n",
      "==================================================\n",
      "Processing Model: M8+2 (Train End: 1996-08-01)\n",
      "==================================================\n",
      "\n",
      "=== LGBM Final Full Frequency Comb Version: M8+2 ===\n",
      "1. Generating B0 Ephemeris Features + Full Frequency Comb Features (Full Cycle)...\n",
      "2. Generating modulation features (Full Cycle)...\n",
      "... Adding 'Planetary_Scaled_Diff' (Velocity) feature...\n",
      "... Adding 'Planetary_Scaled_Accel' (Acceleration) feature...\n",
      "Number of Basic Features (including Diff and Accel): 32\n",
      "Total Features: 64 (Full Frequency Comb + Velocity + Acceleration)\n",
      "Applying QuantileTransformer to stabilize target variable Y (Residual)...\n",
      "Data: Train 51378, Test 8521\n",
      "Training LGBM (Full Frequency Comb Version)...\n",
      "\n",
      "LGBM Best CV R2 (M_CV_R2): 0.0626\n",
      "LGBM Test Set R2 (M_T_R2): 0.0071\n",
      "âœ… Success! Model R2 is positive.\n",
      "Best Params: OrderedDict({'colsample_bytree': 1.0, 'learning_rate': 0.0018298991678292248, 'n_estimators': 691, 'num_leaves': 14, 'reg_alpha': 17.399601668555214, 'reg_lambda': 0.1, 'subsample': 1.0})\n",
      "\n",
      "Feature Importance (Top 20):\n",
      "Mod_Hale_22y_sin        2424\n",
      "Hale_22y_cos            1687\n",
      "Mod_Hale_22y_cos        1153\n",
      "Hale_22y_sin            1121\n",
      "Mod_QBO_2.7y_cos         696\n",
      "Mod_QBO_2.2y_sin         278\n",
      "Mod_QBO_2.7y_sin         196\n",
      "QBO_2.7y_cos             188\n",
      "Mod_Annual_365d_sin      145\n",
      "Mod_Annual_370d_cos      144\n",
      "Mod_B0_sq                131\n",
      "Mod_QBO_2.2y_cos         100\n",
      "QBO_2.2y_cos              87\n",
      "QBO_2.7y_sin              73\n",
      "Mod_Annual_360d_sin       72\n",
      "QBO_1.7y_sin              63\n",
      "Mod_Rotation_28d_cos      60\n",
      "Mod_Rotation_27d_sin      53\n",
      "Mod_QBO_1.7y_cos          47\n",
      "QBO_2.2y_sin              44\n",
      "\n",
      "In Top 20, 13 are modulation features.\n",
      "\n",
      "Saving model and Scalers...\n",
      "Model saved: ../../results/05_p_m_a_model/m_model_4/LGBM_Models2/M8+2_best_lgbm.joblib\n",
      "Generating full cycle (to 2050) predictions...\n",
      "\n",
      "==================================================\n",
      "Processing Model: M8+3 (Train End: 1986-08-01)\n",
      "==================================================\n",
      "\n",
      "=== LGBM Final Full Frequency Comb Version: M8+3 ===\n",
      "1. Generating B0 Ephemeris Features + Full Frequency Comb Features (Full Cycle)...\n",
      "2. Generating modulation features (Full Cycle)...\n",
      "... Adding 'Planetary_Scaled_Diff' (Velocity) feature...\n",
      "... Adding 'Planetary_Scaled_Accel' (Acceleration) feature...\n",
      "Number of Basic Features (including Diff and Accel): 32\n",
      "Total Features: 64 (Full Frequency Comb + Velocity + Acceleration)\n",
      "Applying QuantileTransformer to stabilize target variable Y (Residual)...\n",
      "Data: Train 47725, Test 12174\n",
      "Training LGBM (Full Frequency Comb Version)...\n",
      "\n",
      "LGBM Best CV R2 (M_CV_R2): 0.0304\n",
      "LGBM Test Set R2 (M_T_R2): 0.1182\n",
      "âœ… Success! Model R2 is positive.\n",
      "Best Params: OrderedDict({'colsample_bytree': 0.7, 'learning_rate': 0.014343366282931275, 'n_estimators': 156, 'num_leaves': 5, 'reg_alpha': 20.0, 'reg_lambda': 0.7777599096778142, 'subsample': 0.7438966686056694})\n",
      "\n",
      "Feature Importance (Top 20):\n",
      "Mod_Hale_22y_sin        173\n",
      "Mod_Hale_22y_cos        112\n",
      "Hale_22y_cos             96\n",
      "Hale_22y_sin             94\n",
      "Mod_QBO_2.7y_cos         55\n",
      "Mod_QBO_2.2y_cos         18\n",
      "Mod_QBO_2.7y_sin         16\n",
      "Mod_B0_sq                12\n",
      "Mod_Annual_360d_cos       8\n",
      "Mod_QBO_2.2y_sin          7\n",
      "Mod_B0                    6\n",
      "Mod_QBO_1.7y_cos          5\n",
      "Mod_Rotation_28d_cos      5\n",
      "Mod_Annual_370d_sin       3\n",
      "Mod_Annual_370d_cos       3\n",
      "Mod_Rieger_154d_sin       3\n",
      "Mod_Annual_360d_sin       2\n",
      "Mod_Annual_365d_cos       2\n",
      "Annual_370d_cos           1\n",
      "Mod_Rotation_27d_sin      1\n",
      "\n",
      "In Top 20, 17 are modulation features.\n",
      "\n",
      "Saving model and Scalers...\n",
      "Model saved: ../../results/05_p_m_a_model/m_model_4/LGBM_Models2/M8+3_best_lgbm.joblib\n",
      "Generating full cycle (to 2050) predictions...\n",
      "\n",
      "==================================================\n",
      "Processing Model: M0+3 (Train End: 1986-09-01)\n",
      "==================================================\n",
      "\n",
      "=== LGBM Final Full Frequency Comb Version: M0+3 ===\n",
      "1. Generating B0 Ephemeris Features + Full Frequency Comb Features (Full Cycle)...\n",
      "2. Generating modulation features (Full Cycle)...\n",
      "... Adding 'Planetary_Scaled_Diff' (Velocity) feature...\n",
      "... Adding 'Planetary_Scaled_Accel' (Acceleration) feature...\n",
      "Number of Basic Features (including Diff and Accel): 32\n",
      "Total Features: 64 (Full Frequency Comb + Velocity + Acceleration)\n",
      "Applying QuantileTransformer to stabilize target variable Y (Residual)...\n",
      "Data: Train 47756, Test 12143\n",
      "Training LGBM (Full Frequency Comb Version)...\n",
      "\n",
      "LGBM Best CV R2 (M_CV_R2): 0.0371\n",
      "LGBM Test Set R2 (M_T_R2): 0.0979\n",
      "âœ… Success! Model R2 is positive.\n",
      "Best Params: OrderedDict({'colsample_bytree': 0.7, 'learning_rate': 0.005759386706531362, 'n_estimators': 381, 'num_leaves': 5, 'reg_alpha': 9.701907653430114, 'reg_lambda': 0.1, 'subsample': 1.0})\n",
      "\n",
      "Feature Importance (Top 20):\n",
      "Mod_Hale_22y_cos             377\n",
      "Mod_Hale_22y_sin             360\n",
      "Hale_22y_cos                 197\n",
      "Planetary_Scaled_Accel       101\n",
      "Mod_QBO_2.7y_cos              82\n",
      "Planetary_Scaled_Diff         59\n",
      "Mod_B0_sq                     52\n",
      "Mod_QBO_1.7y_cos              51\n",
      "QBO_2.7y_sin                  42\n",
      "Hale_22y_sin                  39\n",
      "Mod_QBO_2.2y_sin              39\n",
      "Mod_QBO_2.7y_sin              37\n",
      "Mod_Planetary_Scaled_Diff     35\n",
      "Mod_Annual_365d_cos           12\n",
      "Mod_QBO_1.7y_sin               7\n",
      "Mod_Annual_370d_cos            7\n",
      "Mod_QBO_2.2y_cos               7\n",
      "Mod_B0                         5\n",
      "Mod_Annual_360d_sin            5\n",
      "Mod_Annual_365d_sin            5\n",
      "\n",
      "In Top 20, 15 are modulation features.\n",
      "\n",
      "Saving model and Scalers...\n",
      "Model saved: ../../results/05_p_m_a_model/m_model_4/LGBM_Models2/M0+3_best_lgbm.joblib\n",
      "Generating full cycle (to 2050) predictions...\n",
      "\n",
      "==================================================\n",
      "Processing Model: M0+2 (Train End: 1996-08-01)\n",
      "==================================================\n",
      "\n",
      "=== LGBM Final Full Frequency Comb Version: M0+2 ===\n",
      "1. Generating B0 Ephemeris Features + Full Frequency Comb Features (Full Cycle)...\n",
      "2. Generating modulation features (Full Cycle)...\n",
      "... Adding 'Planetary_Scaled_Diff' (Velocity) feature...\n",
      "... Adding 'Planetary_Scaled_Accel' (Acceleration) feature...\n",
      "Number of Basic Features (including Diff and Accel): 32\n",
      "Total Features: 64 (Full Frequency Comb + Velocity + Acceleration)\n",
      "Applying QuantileTransformer to stabilize target variable Y (Residual)...\n",
      "Data: Train 51378, Test 8521\n",
      "Training LGBM (Full Frequency Comb Version)...\n",
      "\n",
      "LGBM Best CV R2 (M_CV_R2): 0.0559\n",
      "LGBM Test Set R2 (M_T_R2): 0.0088\n",
      "âœ… Success! Model R2 is positive.\n",
      "Best Params: OrderedDict({'colsample_bytree': 0.9928583505072504, 'learning_rate': 0.01125175464409754, 'n_estimators': 161, 'num_leaves': 6, 'reg_alpha': 9.767067518713215, 'reg_lambda': 1.033687944001253, 'subsample': 0.8177510233782982})\n",
      "\n",
      "Feature Importance (Top 20):\n",
      "Mod_Hale_22y_sin              312\n",
      "Mod_Hale_22y_cos              136\n",
      "Planetary_Scaled_Diff         123\n",
      "Hale_22y_cos                   81\n",
      "Mod_Planetary_Scaled_Diff      58\n",
      "Mod_QBO_2.7y_cos               44\n",
      "Mod_B0_sq                      15\n",
      "Mod_QBO_2.2y_cos                7\n",
      "Mod_QBO_2.7y_sin                6\n",
      "Mod_QBO_2.2y_sin                5\n",
      "Mod_Annual_360d_sin             5\n",
      "Mod_Annual_365d_sin             5\n",
      "QBO_1.7y_sin                    2\n",
      "Mod_Annual_370d_cos             2\n",
      "Mod_B0                          1\n",
      "Mod_Annual_360d_cos             1\n",
      "Mod_QBO_1.7y_cos                1\n",
      "Mod_Planetary_Scaled_Accel      1\n",
      "Annual_370d_cos                 0\n",
      "B0_sq                           0\n",
      "\n",
      "In Top 20, 15 are modulation features.\n",
      "\n",
      "Saving model and Scalers...\n",
      "Model saved: ../../results/05_p_m_a_model/m_model_4/LGBM_Models2/M0+2_best_lgbm.joblib\n",
      "Generating full cycle (to 2050) predictions...\n",
      "\n",
      "============================================================\n",
      "Saving secondary fit and residuals for all models to: ../../results/05_p_m_a_model/m_model_4/LGBM_results2.csv\n",
      "Save successful. File contains 73780 rows.\n",
      "\n",
      "============================================================\n",
      "LGBM Full Frequency Comb Version Summary\n",
      "============================================================\n",
      "model  M_CV_R2  M_T_R2  n_features  top_20_mod  time(s)\n",
      " M8+2   0.0626  0.0071          64          13 335.2403\n",
      " M8+3   0.0304  0.1182          64          17 233.3128\n",
      " M0+3   0.0371  0.0979          64          15 236.8445\n",
      " M0+2   0.0559  0.0088          64          15 245.6706\n",
      "\n",
      "Average Test R2 (M_T_R2): 0.0580\n",
      "âœ…âœ…âœ… Huge Success: All models have positive Test R2!\n",
      "ðŸ’¡ Insight: Modulation features (Mod_) play a significant role in the models.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# --- Core: Non-linear Model Import ---\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(\"LightGBM imported successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Error: lightgbm library not found.\")\n",
    "    print(\"Please install: pip install lightgbm\")\n",
    "    exit()\n",
    "\n",
    "# Astronomy Library Import\n",
    "try:\n",
    "    from astropy.time import Time\n",
    "    import astropy.units as u\n",
    "    from sunpy.coordinates import sun\n",
    "except ImportError:\n",
    "    print(\"Missing astronomy libraries.\")\n",
    "    exit()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_lgbm_full_comb_validation(\n",
    "    data_full, model_name, train_start_date, train_end_date, \n",
    "    test_end_date, model_save_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Run LGBM (Final Full Frequency Comb Version)\n",
    "    - Features: B0 + All famous cycles (22y, QBO, 1y, 154d, 25-30d)\n",
    "    - Modulation: 11-year planetary fit\n",
    "    - Y Transform: QuantileTransformer\n",
    "    - Tuning: Strong regularization\n",
    "    - New: Save model and full-cycle predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the passed complete data\n",
    "    data = data_full.copy() \n",
    "    \n",
    "    # Update column names to English\n",
    "    target_residual_column = f'Residual_{model_name}'\n",
    "    planetary_fit_column = f'Fit_SSN_{model_name}'\n",
    "    \n",
    "    # [cite_start]Ensure model save directory exists [cite: 87]\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n=== LGBM Final Full Frequency Comb Version: {model_name} ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Check if necessary columns exist\n",
    "    if target_residual_column not in data.columns or planetary_fit_column not in data.columns:\n",
    "        print(f\"Error: Missing column {target_residual_column} or {planetary_fit_column}\")\n",
    "        return None, None\n",
    "\n",
    "    # --- 1. Feature Engineering: B0 + Full Frequency Comb (All dates) ---\n",
    "    print(\"1. Generating B0 Ephemeris Features + Full Frequency Comb Features (Full Cycle)...\")\n",
    "    times_astropy = Time(data['Date'].values)\n",
    "    \n",
    "    original_feature_names = []\n",
    "    \n",
    "    # B0 (Heliographic latitude) - Real annual amplitude\n",
    "    b0_deg = sun.B0(times_astropy).deg\n",
    "    data['B0'] = b0_deg\n",
    "    data['B0_sq'] = b0_deg ** 2\n",
    "    original_feature_names.extend(['B0', 'B0_sq'])\n",
    "    \n",
    "    # Time-based fixed frequency features\n",
    "    days_since_start = (data['Date'] - data['Date'].min()).dt.days\n",
    "    \n",
    "    # === Define our Frequency Comb ===\n",
    "    \n",
    "    # 1. Hale (22y)\n",
    "    hale_rad = 2 * np.pi * days_since_start / (22.0 * 365.25)\n",
    "    data['Hale_22y_sin'] = np.sin(hale_rad)\n",
    "    data['Hale_22y_cos'] = np.cos(hale_rad)\n",
    "    original_feature_names.extend(['Hale_22y_sin', 'Hale_22y_cos'])\n",
    "\n",
    "    # 2. QBO (1.5y - 3y)\n",
    "    qbo_periods = [1.7, 2.2, 2.7] # Years\n",
    "    for p in qbo_periods:\n",
    "        rad = 2 * np.pi * days_since_start / (p * 365.25)\n",
    "        name = f'QBO_{p:.1f}y'\n",
    "        data[f'{name}_sin'] = np.sin(rad)\n",
    "        data[f'{name}_cos'] = np.cos(rad)\n",
    "        original_feature_names.extend([f'{name}_sin', f'{name}_cos'])\n",
    "\n",
    "    # 3. Annual (1y)\n",
    "    annual_periods = [360, 365.25, 370] # Days\n",
    "    for p in annual_periods:\n",
    "        rad = 2 * np.pi * days_since_start / p\n",
    "        name = f'Annual_{p:.0f}d'\n",
    "        data[f'{name}_sin'] = np.sin(rad)\n",
    "        data[f'{name}_cos'] = np.cos(rad)\n",
    "        original_feature_names.extend([f'{name}_sin', f'{name}_cos'])\n",
    "\n",
    "    # 4. Rieger (154d)\n",
    "    rieger_rad = 2 * np.pi * days_since_start / 154\n",
    "    data['Rieger_154d_sin'] = np.sin(rieger_rad)\n",
    "    data['Rieger_154d_cos'] = np.cos(rieger_rad)\n",
    "    original_feature_names.extend(['Rieger_154d_sin', 'Rieger_154d_cos'])\n",
    "\n",
    "    # 5. Rotation (25d-30d)\n",
    "    rotation_periods = [25, 26, 27, 28, 29, 30] # Days\n",
    "    for p in rotation_periods:\n",
    "        rad = 2 * np.pi * days_since_start / p\n",
    "        name = f'Rotation_{p:.0f}d'\n",
    "        data[f'{name}_sin'] = np.sin(rad)\n",
    "        data[f'{name}_cos'] = np.cos(rad)\n",
    "        original_feature_names.extend([f'{name}_sin', f'{name}_cos'])\n",
    "    \n",
    "    \n",
    "    # --- 2. Modulation Feature Engineering (Refactored) ---\n",
    "    # Goal: Fit Scaler on training set, Transform on full cycle\n",
    "    print(\"2. Generating modulation features (Full Cycle)...\")\n",
    "    \n",
    "    # a. Create training mask for fitting Scaler\n",
    "    # Training set mask (only for fitting Scaler and Model)\n",
    "    train_mask_dates = (data['Date'] >= train_start_date) & (data['Date'] <= train_end_date)\n",
    "    # Must be clean, non-NaN data\n",
    "    train_mask_clean = train_mask_dates & data[target_residual_column].notna() & data[planetary_fit_column].notna()\n",
    "    \n",
    "    if train_mask_clean.sum() == 0:\n",
    "        print(f\"Error: No clean training data between {train_start_date} and {train_end_date}.\")\n",
    "        return None, None\n",
    "        \n",
    "    # b. Fit Modulator (11-year cycle signal) Scaler (Only on training data)\n",
    "    planetary_fit_values_train = data.loc[train_mask_clean, [planetary_fit_column]]\n",
    "    scaler_fit = StandardScaler()\n",
    "    scaler_fit.fit(planetary_fit_values_train)\n",
    "    \n",
    "    # c. Transform (Full Cycle)\n",
    "    # Warning: If planetary_fit_column is NaN in the future, this will create NaNs.\n",
    "    # We fill NaNs with 0 (assuming future fit is 0 if missing, not ideal but prevents crash)\n",
    "    planetary_fit_values_full = data[[planetary_fit_column]].fillna(0) \n",
    "    data['Planetary_Scaled'] = scaler_fit.transform(planetary_fit_values_full).flatten()\n",
    "\n",
    "    # --- [ *** Modified Here *** ] ---\n",
    "    \n",
    "    # 2.c.1: Create Rate of Change feature (Velocity)\n",
    "    print(\"... Adding 'Planetary_Scaled_Diff' (Velocity) feature...\")\n",
    "    data['Planetary_Scaled_Diff'] = data['Planetary_Scaled'].diff(1).fillna(0)\n",
    "    \n",
    "    # 2.c.2: Create Acceleration feature (Rate of change of Velocity)\n",
    "    print(\"... Adding 'Planetary_Scaled_Accel' (Acceleration) feature...\")\n",
    "    data['Planetary_Scaled_Accel'] = data['Planetary_Scaled_Diff'].diff(1).fillna(0)\n",
    "\n",
    "    # 2.c.3: Add both to the *Basic* feature list\n",
    "    # Key: This way they will automatically get their own \"Mod_\" versions in the next step\n",
    "    original_feature_names.append('Planetary_Scaled_Diff')\n",
    "    original_feature_names.append('Planetary_Scaled_Accel')\n",
    "    # --- [ *** End of Modification *** ] ---\n",
    "    \n",
    "    # d. Create Modulation Features (Full Cycle)\n",
    "    modulation_feature_names = []\n",
    "    for feature in original_feature_names:\n",
    "        new_feature_name = f'Mod_{feature}'\n",
    "        data.loc[:, new_feature_name] = data['Planetary_Scaled'] * data[feature]\n",
    "        modulation_feature_names.append(new_feature_name)\n",
    "    \n",
    "    feature_names = original_feature_names + modulation_feature_names\n",
    "\n",
    "    # Print statement now shows 32 basic features (30 original + 1 Diff + 1 Accel)\n",
    "    print(f\"Number of Basic Features (including Diff and Accel): {len(original_feature_names)}\") \n",
    "    # Total features should now be 64 (32 + 32)\n",
    "    print(f\"Total Features: {len(feature_names)} (Full Frequency Comb + Velocity + Acceleration)\")\n",
    "    \n",
    "    # --- 3. Data Split and Transformation (For Training) ---\n",
    "    \n",
    "    # Only use rows with valid residuals for training and testing\n",
    "    data_clean = data.dropna(subset=[target_residual_column] + feature_names)\n",
    "    \n",
    "    X = data_clean[feature_names]\n",
    "    Y_raw = data_clean[target_residual_column] # Y Raw values\n",
    "    groups = data_clean['Date'].dt.year\n",
    "    dates = data_clean['Date']\n",
    "    \n",
    "    train_mask = (dates >= train_start_date) & (dates <= train_end_date)\n",
    "    test_start_date_dt = pd.to_datetime(train_end_date) + pd.Timedelta(days=1)\n",
    "    test_mask = (dates >= test_start_date_dt) & (dates <= test_end_date)\n",
    "    \n",
    "    X_train_raw = X[train_mask]\n",
    "    Y_train_raw = Y_raw[train_mask]\n",
    "    X_test_raw = X[test_mask]\n",
    "    Y_test_raw = Y_raw[test_mask]\n",
    "    groups_train = groups[train_mask]\n",
    "    \n",
    "    if len(X_train_raw) == 0 or len(X_test_raw) == 0:\n",
    "        print(\"Error: Train or Test set is empty\")\n",
    "        return None, None\n",
    "\n",
    "    # a. Fit X Transformer (StandardScaler)\n",
    "    feature_scaler = StandardScaler()\n",
    "    X_train = feature_scaler.fit_transform(X_train_raw)\n",
    "    X_test = feature_scaler.transform(X_test_raw)\n",
    "\n",
    "    # b. Fit Y Transformer (QuantileTransformer) - Key Fix\n",
    "    print(\"Applying QuantileTransformer to stabilize target variable Y (Residual)...\")\n",
    "    target_transformer = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "    Y_train = target_transformer.fit_transform(Y_train_raw.values.reshape(-1, 1)).flatten()\n",
    "    Y_test = target_transformer.transform(Y_test_raw.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    print(f\"Data: Train {len(X_train)}, Test {len(X_test)}\")\n",
    "    \n",
    "    # --- 4. Bayesian Optimization - LGBM (Strong Regularization) ---\n",
    "    n_splits = max(2, min(5, groups_train.nunique()))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    search_spaces = {\n",
    "        'n_estimators': Integer(100, 800),\n",
    "        'num_leaves': Integer(5, 30),\n",
    "        'learning_rate': Real(1e-3, 0.1, 'log-uniform'),\n",
    "        'reg_alpha': Real(0.1, 20.0, 'log-uniform'), \n",
    "        'reg_lambda': Real(0.1, 20.0, 'log-uniform'),\n",
    "        'subsample': Real(0.7, 1.0, 'uniform'),\n",
    "        'colsample_bytree': Real(0.7, 1.0, 'uniform')\n",
    "    }\n",
    "    \n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        random_state=42, \n",
    "        n_jobs=1, \n",
    "        objective='regression_l1',\n",
    "        force_row_wise=True, # Force row-wise to eliminate auto-detection warnings\n",
    "        verbose=-1           # Suppress all LightGBM internal output (recommended)\n",
    "    )\n",
    "\n",
    "    opt_lgbm = BayesSearchCV(\n",
    "        lgb_model, \n",
    "        search_spaces, \n",
    "        n_iter=100, \n",
    "        cv=gkf, \n",
    "        n_jobs=-1,\n",
    "        random_state=42, \n",
    "        scoring='r2'\n",
    "    )\n",
    "    \n",
    "    print(\"Training LGBM (Full Frequency Comb Version)...\")\n",
    "    opt_lgbm.fit(X_train, Y_train, groups=groups_train)\n",
    "    \n",
    "    # --- 5. Result Analysis ---\n",
    "    best_model = opt_lgbm.best_estimator_\n",
    "    \n",
    "    # Predict on test set\n",
    "    Y_pred_test = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate M_T_R2 (on transformed Y)\n",
    "    final_r2_test = r2_score(Y_test, Y_pred_test) \n",
    "    \n",
    "    # Get M_CV_R2 (Best CV Score on transformed Y)\n",
    "    best_cv_score = opt_lgbm.best_score_\n",
    "    \n",
    "    print(f\"\\nLGBM Best CV R2 (M_CV_R2): {best_cv_score:.4f}\")\n",
    "    print(f\"LGBM Test Set R2 (M_T_R2): {final_r2_test:.4f}\")\n",
    "    \n",
    "    if final_r2_test > 0: # Check using the corrected variable name\n",
    "        print(\"âœ… Success! Model R2 is positive.\")\n",
    "    else:\n",
    "        print(\"âŒ R2 is still negative.\")\n",
    "        \n",
    "    print(f\"Best Params: {opt_lgbm.best_params_}\")\n",
    "    \n",
    "    # Feature Importance\n",
    "    importances = pd.Series(best_model.feature_importances_, index=feature_names)\n",
    "    print(\"\\nFeature Importance (Top 20):\")\n",
    "    top_features = importances.sort_values(ascending=False).head(20)    \n",
    "    print(top_features.to_string())\n",
    "    \n",
    "    mod_features_in_top = [f for f in top_features.index if f.startswith('Mod_')]\n",
    "    print(f\"\\nIn Top 20, {len(mod_features_in_top)} are modulation features.\")\n",
    "    \n",
    "    # --- 6. New Functionality: Save Model ---\n",
    "    print(\"\\nSaving model and Scalers...\")\n",
    "    # Updated to use model_save_dir\n",
    "    model_path = os.path.join(model_save_dir, f\"{model_name}_best_lgbm.joblib\")\n",
    "    fscaler_path = os.path.join(model_save_dir, f\"{model_name}_feature_scaler.joblib\")\n",
    "    tscaler_path = os.path.join(model_save_dir, f\"{model_name}_target_transformer.joblib\")\n",
    "    \n",
    "    joblib.dump(best_model, model_path)\n",
    "    joblib.dump(feature_scaler, fscaler_path)\n",
    "    joblib.dump(target_transformer, tscaler_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "    # --- 7. New Functionality: Generate Full Cycle Predictions ---\n",
    "    print(\"Generating full cycle (to 2050) predictions...\")\n",
    "    \n",
    "    # Prepare X features for full cycle\n",
    "    # Must fill NaNs in features (e.g., if Planetary_Scaled is NaN at the end)\n",
    "    X_full = data[feature_names].fillna(0)    \n",
    "    \n",
    "    X_full_scaled = feature_scaler.transform(X_full)\n",
    "    Y_pred_full_transformed = best_model.predict(X_full_scaled)\n",
    "    \n",
    "    # Inverse transform back to original scale\n",
    "    Y_pred_full_raw = target_transformer.inverse_transform(Y_pred_full_transformed.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Create prediction column and secondary residual column\n",
    "    pred_col_name = f'LGBM_Pred_{model_name}'\n",
    "    resid_col_name = f'LGBM_Resid_{model_name}'\n",
    "    \n",
    "    data[pred_col_name] = Y_pred_full_raw\n",
    "    # Original Residual - LGBM Predicted Residual = Secondary Residual\n",
    "    data[resid_col_name] = data[target_residual_column] - data[pred_col_name]\n",
    "    \n",
    "    # Prepare stats dictionary\n",
    "    stats_dict = {\n",
    "        'model': model_name,\n",
    "        # Added M_CV_R2 and M_T_R2\n",
    "        'M_T_R2': final_r2_test,\n",
    "        'M_CV_R2': best_cv_score,\n",
    "        'n_features': len(feature_names),\n",
    "        'top_20_mod_count': len(mod_features_in_top),\n",
    "        'best_params': opt_lgbm.best_params_,\n",
    "        'training_time': time.time() - start_time\n",
    "    }\n",
    "\n",
    "    # Return DataFrame with new columns (only necessary columns) and stats results\n",
    "    return data[['Date', pred_col_name, resid_col_name]], stats_dict\n",
    "\n",
    "\n",
    "# Main Execution - Run LGBM Full Frequency Comb Version\n",
    "if __name__ == \"__main__\":\n",
    "    model_splits = {\n",
    "        'M8+2': '1996-08-01', \n",
    "        'M8+3': '1986-08-01',\n",
    "        'M0+3': '1986-09-01', \n",
    "        'M0+2': '1996-08-01'\n",
    "    }\n",
    "    \n",
    "    print(\"=== Run LGBM (Full Frequency Comb Version: B0 + 22y, QBO, 1y, 154d, 25-30d) ===\")\n",
    "    print(\"Strategy: Physical Full Frequency Comb + 11-year Modulation + Target Stabilization + Strong Regularization\")\n",
    "    print(\"New: Save model and full cycle secondary residuals\")\n",
    "\n",
    "    # Directory for saving models\n",
    "    MODEL_SAVE_DIR = \"../../results/05_p_m_a_model/m_model_4/LGBM_Models2\" \n",
    "    \n",
    "    # --- New: Load and Prepare Data Outside Loop ---\n",
    "    try:\n",
    "        data_source_file = '../../results/05_p_m_a_model/p_model_4/residual/Summary_Fit_Results.csv'\n",
    "        print(f\"\\nLoading source data: {data_source_file}\")\n",
    "        full_data_to_save = pd.read_csv(data_source_file, parse_dates=['Date'])\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal Error: Cannot read {data_source_file}. Error: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Check and extend dates to 2050\n",
    "    max_date_in_file = full_data_to_save['Date'].max()\n",
    "    target_date = pd.to_datetime('2050-12-31')\n",
    "    \n",
    "    if max_date_in_file < target_date:\n",
    "        print(f\"Warning: Source data max date is {max_date_in_file}.\")\n",
    "        print(f\"Warning: Extending date range to {target_date}.\")\n",
    "        print(\"Warning: Please ensure 'Fit_SSN' (Planetary Fit) column in source file is extrapolated to 2050, otherwise predictions will be inaccurate!\")\n",
    "        \n",
    "        all_dates_df = pd.DataFrame({'Date': pd.date_range(\n",
    "            start=full_data_to_save['Date'].min(), \n",
    "            end=target_date, \n",
    "            freq='D'\n",
    "        )})\n",
    "        full_data_to_save = pd.merge(all_dates_df, full_data_to_save, on='Date', how='left')\n",
    "\n",
    "    # To save stats results\n",
    "    results_stats_list = []\n",
    "    \n",
    "    # Loop through each model\n",
    "    for model_name, split_date in model_splits.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing Model: {model_name} (Train End: {split_date})\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Pass full data\n",
    "        result_df, stats_dict = run_lgbm_full_comb_validation(\n",
    "            data_full=full_data_to_save.copy(), # Pass a copy of data\n",
    "            model_name=model_name,\n",
    "            train_start_date='1855-12-02',\n",
    "            train_end_date=split_date,\n",
    "            test_end_date='2019-11-30',\n",
    "            model_save_dir=MODEL_SAVE_DIR # Pass the directory explicitly\n",
    "        )\n",
    "        \n",
    "        if result_df is not None and stats_dict is not None:\n",
    "            results_stats_list.append(stats_dict)\n",
    "            # Merge this model's results back to main DataFrame\n",
    "            full_data_to_save = pd.merge(full_data_to_save, result_df, on='Date', how='left')\n",
    "    \n",
    "    # --- New: Save CSV containing all results ---\n",
    "    if results_stats_list:\n",
    "        output_filename =  \"../../results/05_p_m_a_model/m_model_4/LGBM_results2.csv\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Saving secondary fit and residuals for all models to: {output_filename}\")\n",
    "        try:\n",
    "            full_data_to_save.to_csv(output_filename, index=False, float_format='%.6f')\n",
    "            print(f\"Save successful. File contains {len(full_data_to_save)} rows.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save CSV: {e}\")\n",
    "        \n",
    "        # --- Print Summary Report (Using stats data) ---\n",
    "        results_to_print = []\n",
    "        for r in results_stats_list:\n",
    "            results_to_print.append({\n",
    "                'model': r['model'],\n",
    "                # Correction: Use new key name M_T_R2\n",
    "                'M_T_R2': r['M_T_R2'], \n",
    "                # Correction: Add M_CV_R2 for easy viewing\n",
    "                'M_CV_R2': r['M_CV_R2'], \n",
    "                'n_features': r['n_features'],\n",
    "                'top_20_mod': r['top_20_mod_count'],\n",
    "                'time(s)': r['training_time']\n",
    "            })\n",
    "            \n",
    "        results_df = pd.DataFrame(results_to_print)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LGBM Full Frequency Comb Version Summary\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "        # Print key columns\n",
    "        print(results_df[['model', 'M_CV_R2', 'M_T_R2', 'n_features', 'top_20_mod', 'time(s)']].to_string(index=False, float_format=\"%.4f\"))\n",
    "        \n",
    "        # Calculate mean using M_T_R2\n",
    "        avg_r2 = results_df['M_T_R2'].mean()\n",
    "        print(f\"\\nAverage Test R2 (M_T_R2): {avg_r2:.4f}\")\n",
    "        \n",
    "        # Check positive/negative using M_T_R2\n",
    "        positive_r2 = results_df[results_df['M_T_R2'] > 0]\n",
    "        if len(positive_r2) == len(results_df):\n",
    "            print(\"âœ…âœ…âœ… Huge Success: All models have positive Test R2!\")\n",
    "        elif len(positive_r2) > 0:\n",
    "            print(f\"âœ… Success: {len(positive_r2)} models obtained positive Test R2.\")\n",
    "        else:\n",
    "            print(\"âŒ Failure: All models still have negative Test R2.\")\n",
    "        \n",
    "        if results_df['top_20_mod'].mean() >= 10: \n",
    "             print(\"ðŸ’¡ Insight: Modulation features (Mod_) play a significant role in the models.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nAll models failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1ac3f-ad5e-45a2-b9eb-61fb2dd8b0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py313ssn)",
   "language": "python",
   "name": "ssn13_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
