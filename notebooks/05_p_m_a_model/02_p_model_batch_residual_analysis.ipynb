{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8687b80c-e962-409e-ba1f-26b97940a48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 17:02:05,024 - INFO - Starting batch analysis of 4 models...\n",
      "2025-12-29 17:02:05,025 - INFO - Loading sunspot data: ../../data/ready\\ssn_daily_1849_2025.csv\n",
      "2025-12-29 17:02:05,057 - INFO - Loading planet position and velocity data...\n",
      "2025-12-29 17:02:09,601 - INFO - Data merge complete: Position=73780, Velocity=73780, Combined=73780\n",
      "2025-12-29 17:02:09,653 - INFO - Calculating SSN smoothed trend line...\n",
      "2025-12-29 17:02:09,730 - INFO - SSN smoothed trend line calculation complete.\n",
      "2025-12-29 17:02:09,731 - INFO - Loading SIDC monthly smoothed data: ../../data/ready\\ssn_smoothed_monthly_1749_2025.csv\n",
      "2025-12-29 17:02:09,771 - INFO - SIDC monthly smoothed data loaded and interpolated to daily.\n",
      "2025-12-29 17:02:09,772 - INFO - --- Processing Model: M8+2 (12stars_Ridge_CV-R2_0.6880_OOT-SMOOTH-R2_0.4587_OOT-RAW-R2_0.3326_Params_alpha_0.1000.joblib) ---\n",
      "2025-12-29 17:02:09,773 - INFO - Loading model file: ../../results/05_p_m_a_model/p_model_4\\12stars_Ridge_CV-R2_0.6880_OOT-SMOOTH-R2_0.4587_OOT-RAW-R2_0.3326_Params_alpha_0.1000.joblib\n",
      "2025-12-29 17:02:10,295 - INFO - M8+2 - Feature count: 72, Stars: 12, Mode: 6D\n",
      "2025-12-29 17:02:10,295 - INFO - M8+2 - CV R²: 0.6880, OOT R²: 0.4587\n",
      "2025-12-29 17:02:10,296 - INFO - M8+2 using all 72 features for prediction.\n",
      "2025-12-29 17:02:10,309 - INFO - Performing full prediction...\n",
      "2025-12-29 17:02:10,325 - INFO - Calculating residuals and storing results for M8+2...\n",
      "2025-12-29 17:02:10,335 - INFO - Performing spectral analysis on Residuals for M8+2...\n",
      "2025-12-29 17:02:10,339 - INFO - M8+2 Residual spectral analysis complete: 10 valid periods found.\n",
      "2025-12-29 17:02:10,339 - INFO - Calculating spectrum for Fitted Values of M8+2...\n",
      "2025-12-29 17:02:10,346 - INFO - --- Processing Model: M8+3 (15stars_Ridge_CV-R2_0.7291_OOT-SMOOTH-R2_0.4643_OOT-RAW-R2_0.3510_Params_alpha_0.0010.joblib) ---\n",
      "2025-12-29 17:02:10,346 - INFO - Loading model file: ../../results/05_p_m_a_model/p_model_4\\15stars_Ridge_CV-R2_0.7291_OOT-SMOOTH-R2_0.4643_OOT-RAW-R2_0.3510_Params_alpha_0.0010.joblib\n",
      "2025-12-29 17:02:10,354 - INFO - M8+3 - Feature count: 45, Stars: 15, Mode: 3D_V\n",
      "2025-12-29 17:02:10,354 - INFO - M8+3 - CV R²: 0.7291, OOT R²: 0.4643\n",
      "2025-12-29 17:02:10,355 - INFO - M8+3 using all 45 features for prediction.\n",
      "2025-12-29 17:02:10,364 - INFO - Performing full prediction...\n",
      "2025-12-29 17:02:10,376 - INFO - Calculating residuals and storing results for M8+3...\n",
      "2025-12-29 17:02:10,384 - INFO - Performing spectral analysis on Residuals for M8+3...\n",
      "2025-12-29 17:02:10,388 - INFO - M8+3 Residual spectral analysis complete: 10 valid periods found.\n",
      "2025-12-29 17:02:10,388 - INFO - Calculating spectrum for Fitted Values of M8+3...\n",
      "2025-12-29 17:02:10,395 - INFO - --- Processing Model: M0+3 (21stars_Ridge_CV-R2_0.7281_OOT-SMOOTH-R2_0.5876_OOT-RAW-R2_0.4431_Params_alpha_0.1339.joblib) ---\n",
      "2025-12-29 17:02:10,395 - INFO - Loading model file: ../../results/05_p_m_a_model/p_model_4\\21stars_Ridge_CV-R2_0.7281_OOT-SMOOTH-R2_0.5876_OOT-RAW-R2_0.4431_Params_alpha_0.1339.joblib\n",
      "2025-12-29 17:02:10,402 - INFO - M0+3 - Feature count: 63, Stars: 21, Mode: 3D\n",
      "2025-12-29 17:02:10,403 - INFO - M0+3 - CV R²: 0.7281, OOT R²: 0.5876\n",
      "2025-12-29 17:02:10,404 - INFO - M0+3 using all 63 features for prediction.\n",
      "2025-12-29 17:02:10,414 - INFO - Performing full prediction...\n",
      "2025-12-29 17:02:10,430 - INFO - Calculating residuals and storing results for M0+3...\n",
      "2025-12-29 17:02:10,439 - INFO - Performing spectral analysis on Residuals for M0+3...\n",
      "2025-12-29 17:02:10,443 - INFO - M0+3 Residual spectral analysis complete: 10 valid periods found.\n",
      "2025-12-29 17:02:10,443 - INFO - Calculating spectrum for Fitted Values of M0+3...\n",
      "2025-12-29 17:02:10,449 - INFO - --- Processing Model: M0+2 (25stars_Ridge_CV-R2_0.6885_OOT-SMOOTH-R2_0.6117_OOT-RAW-R2_0.4448_Params_alpha_0.3329.joblib) ---\n",
      "2025-12-29 17:02:10,450 - INFO - Loading model file: ../../results/05_p_m_a_model/p_model_4\\25stars_Ridge_CV-R2_0.6885_OOT-SMOOTH-R2_0.6117_OOT-RAW-R2_0.4448_Params_alpha_0.3329.joblib\n",
      "2025-12-29 17:02:10,457 - INFO - M0+2 - Feature count: 75, Stars: 25, Mode: 3D_V\n",
      "2025-12-29 17:02:10,457 - INFO - M0+2 - CV R²: 0.6885, OOT R²: 0.6117\n",
      "2025-12-29 17:02:10,458 - INFO - M0+2 using all 75 features for prediction.\n",
      "2025-12-29 17:02:10,470 - INFO - Performing full prediction...\n",
      "2025-12-29 17:02:10,487 - INFO - Calculating residuals and storing results for M0+2...\n",
      "2025-12-29 17:02:10,497 - INFO - Performing spectral analysis on Residuals for M0+2...\n",
      "2025-12-29 17:02:10,501 - INFO - M0+2 Residual spectral analysis complete: 10 valid periods found.\n",
      "2025-12-29 17:02:10,501 - INFO - Calculating spectrum for Fitted Values of M0+2...\n",
      "2025-12-29 17:02:10,507 - INFO - Generating comprehensive fitted results CSV...\n",
      "2025-12-29 17:02:10,890 - INFO - Saved complete results: ../../results/05_p_m_a_model/p_model_4/residual\\Summary_Fit_Results.csv\n",
      "2025-12-29 17:02:10,891 - INFO - File contains 11 columns: ['Raw_SSN', 'Smoothed_SSN', 'SIDC_SSN', 'Fit_SSN_M8+2', 'Residual_M8+2', 'Fit_SSN_M8+3', 'Residual_M8+3', 'Fit_SSN_M0+3', 'Residual_M0+3', 'Fit_SSN_M0+2', 'Residual_M0+2']\n",
      "2025-12-29 17:02:10,891 - INFO - Merging model info and metrics...\n",
      "2025-12-29 17:02:10,892 - INFO - Calculating model performance metrics...\n",
      "2025-12-29 17:02:10,894 - INFO - M8+2: RMSE=52.353, MAE=38.934, R²=0.541\n",
      "2025-12-29 17:02:10,897 - INFO - M8+3: RMSE=53.667, MAE=40.656, R²=0.517\n",
      "2025-12-29 17:02:10,900 - INFO - M0+3: RMSE=52.475, MAE=38.793, R²=0.539\n",
      "2025-12-29 17:02:10,902 - INFO - M0+2: RMSE=53.423, MAE=39.919, R²=0.522\n",
      "2025-12-29 17:02:10,908 - INFO - Saved combined stats: ../../results/05_p_m_a_model/p_model_4/residual\\Model_Comprehensive_Stats.csv\n",
      "2025-12-29 17:02:10,910 - INFO - Saved spectral stats: ../../results/05_p_m_a_model/p_model_4/residual\\Spectral_Stats.csv\n",
      "2025-12-29 17:02:10,911 - INFO - Plotting 1x3 Spectral Analysis (Welch)...\n",
      "2025-12-29 17:02:10,911 - INFO - Calculating spectrum for Raw, Smoothed, and SIDC SSN...\n",
      "2025-12-29 17:02:12,108 - INFO - Saved 1x3 Spectrum Plot: ../../results/05_p_m_a_model/p_model_4/residual\\Comparison_Spectrum_1x3.png\n",
      "2025-12-29 17:02:12,109 - INFO - Saved Periods CSV: ../../results/05_p_m_a_model/p_model_4/residual\\Comparison_Top100_Periods.csv\n",
      "2025-12-29 17:02:12,110 - INFO - Plotting ACF/PACF...\n",
      "2025-12-29 17:02:14,145 - INFO - Saved ACF/PACF Plot: ../../results/05_p_m_a_model/p_model_4/residual\\Comparison_ACF_PACF.png\n",
      "2025-12-29 17:02:14,145 - INFO - Batch analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Batch Analysis of Multiple Sunspot Prediction Models - Complete Feature Repair + Comprehensive Fit Results\n",
    "Function: Loads position and velocity data simultaneously, fixes feature matching issues, and generates comprehensive CSV results.\n",
    "     (v7: Spectral plot changed to 1x3 subplots: Base SSN | Fitted Values | Residuals)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from pybaselines.whittaker import asls\n",
    "import warnings\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# --- User Configuration ---\n",
    "MODEL_DIR = '../../results/05_p_m_a_model/p_model_4'\n",
    "MODEL_FILES = [\n",
    "    '12stars_Ridge_CV-R2_0.6880_OOT-SMOOTH-R2_0.4587_OOT-RAW-R2_0.3326_Params_alpha_0.1000.joblib',\n",
    "    '15stars_Ridge_CV-R2_0.7291_OOT-SMOOTH-R2_0.4643_OOT-RAW-R2_0.3510_Params_alpha_0.0010.joblib',\n",
    "    '21stars_Ridge_CV-R2_0.7281_OOT-SMOOTH-R2_0.5876_OOT-RAW-R2_0.4431_Params_alpha_0.1339.joblib',\n",
    "    '25stars_Ridge_CV-R2_0.6885_OOT-SMOOTH-R2_0.6117_OOT-RAW-R2_0.4448_Params_alpha_0.3329.joblib'\n",
    "]\n",
    "MODEL_LABELS = ['M8+2', 'M8+3', 'M0+3', 'M0+2']\n",
    "DATA_DIR = '../../data/ready'\n",
    "\n",
    "# Expecting CSV with columns: ['Day', 'SSN']\n",
    "SSN_FILE = os.path.join(DATA_DIR, 'ssn_daily_1849_2025.csv')\n",
    "\n",
    "# Expecting CSV with columns: ['Year', 'Month', 'SSN'] (Previously '年', '月', '黑子数')\n",
    "SIDC_MONTHLY_FILE = os.path.join(DATA_DIR, 'ssn_smoothed_monthly_1749_2025.csv')\n",
    "\n",
    "# Expecting Parquet with columns: ['date', 'SSB_x', 'SSB_y', 'SSB_z'...]\n",
    "PLANET_POSITION_FILE = os.path.join(DATA_DIR, '781_planets_dwarfs_asteroids_xyz.parquet')\n",
    "\n",
    "# Expecting Parquet with columns: ['date', 'SSB_vx', 'SSB_vy', 'SSB_vz'...]\n",
    "PLANET_VELOCITY_FILE = os.path.join(DATA_DIR, '781_planets_dwarfs_asteroids_velocity.parquet')\n",
    "\n",
    "OUTPUT_DIR = '../../results/05_p_m_a_model/p_model_4/residual'\n",
    "\n",
    "# --- Configuration End ---\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def setup_logging():\n",
    "    \"\"\"Configure logging.\"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    log_filename = os.path.join(OUTPUT_DIR, f'analysis_log_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt')\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename, encoding='utf-8'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "def load_planet_data(logger):\n",
    "    \"\"\"Load complete planet position and velocity data.\"\"\"\n",
    "    logger.info(\"Loading planet position and velocity data...\")\n",
    "    \n",
    "    try:\n",
    "        # Load position data\n",
    "        df_position = pd.read_parquet(PLANET_POSITION_FILE)\n",
    "        df_position = df_position.set_index(pd.to_datetime(df_position['date'])).drop('date', axis=1).sort_index()\n",
    "        \n",
    "        # Load velocity data\n",
    "        df_velocity = pd.read_parquet(PLANET_VELOCITY_FILE)\n",
    "        df_velocity = df_velocity.set_index(pd.to_datetime(df_velocity['date'])).drop('date', axis=1).sort_index()\n",
    "        \n",
    "        # Merge\n",
    "        if not df_position.index.equals(df_velocity.index):\n",
    "            logger.warning(\"Position and velocity file date indices do not match perfectly; taking intersection.\")\n",
    "        \n",
    "        df_combined = df_position.join(df_velocity, how='inner').sort_index()\n",
    "        logger.info(f\"Data merge complete: Position={len(df_position)}, Velocity={len(df_velocity)}, Combined={len(df_combined)}\")\n",
    "            \n",
    "        return df_combined\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Data file not found: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading planet data: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_sunspot_data(logger):\n",
    "    \"\"\"Load sunspot data.\"\"\"\n",
    "    logger.info(f\"Loading sunspot data: {SSN_FILE}\")\n",
    "    \n",
    "    try:\n",
    "        # Load CSV without parsing dates first to inspect columns\n",
    "        df = pd.read_csv(SSN_FILE)\n",
    "        \n",
    "        # Standardize all column names to lowercase (e.g., 'Date' -> 'date', 'SSN' -> 'ssn')\n",
    "        df.columns = [c.lower().strip() for c in df.columns]\n",
    "        \n",
    "        # Identify the date column\n",
    "        if 'date' in df.columns:\n",
    "            date_col = 'date'\n",
    "        elif 'day' in df.columns:\n",
    "            date_col = 'day'\n",
    "        else:\n",
    "            # If neither found, show error with available columns\n",
    "            raise ValueError(f\"Missing date column. Found columns: {list(df.columns)}\")\n",
    "            \n",
    "        # Identify the SSN value column\n",
    "        if 'ssn' in df.columns:\n",
    "            val_col = 'ssn'\n",
    "        elif 'sunspot' in df.columns:\n",
    "            val_col = 'sunspot'\n",
    "        else:\n",
    "             # Fallback: assume the second column is values if 'ssn' is missing\n",
    "             if len(df.columns) >= 2:\n",
    "                 val_col = df.columns[1]\n",
    "                 logger.warning(f\"Column 'ssn' not found. Using 2nd column '{val_col}' as SSN values.\")\n",
    "             else:\n",
    "                 raise ValueError(f\"Missing SSN column. Found columns: {list(df.columns)}\")\n",
    "\n",
    "        # Parse dates and set index\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        df_sunspot_raw = df.set_index(date_col)[val_col].asfreq('D').fillna(0)\n",
    "        \n",
    "        # Rename Series to standard 'SSN' for downstream compatibility\n",
    "        df_sunspot_raw.name = 'SSN'\n",
    "        \n",
    "        return df_sunspot_raw\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading sunspot data: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_sidc_interpolated_data(logger, file_path, full_date_range):\n",
    "    \"\"\"Load SIDC monthly smoothed data and interpolate to daily.\"\"\"\n",
    "    logger.info(f\"Loading SIDC monthly smoothed data: {file_path}\")\n",
    "    try:\n",
    "        monthly_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Updated to English column names: 'Year', 'Month', 'SSN'\n",
    "        # Parse dates\n",
    "        monthly_df['date'] = pd.to_datetime(\n",
    "            monthly_df['Year'].astype(int).astype(str) + '-' + \n",
    "            monthly_df['Month'].astype(int).astype(str) + '-01'\n",
    "        )\n",
    "        monthly_df.set_index('date', inplace=True)\n",
    "        monthly_df.rename(columns={'SSN': 'smoothed_number'}, inplace=True)\n",
    "        \n",
    "        # Filter invalid data\n",
    "        monthly_df = monthly_df[monthly_df['smoothed_number'] != -1]\n",
    "        \n",
    "        # Resample to daily and interpolate linearly\n",
    "        daily_sidc = monthly_df['smoothed_number'].resample('D').interpolate(method='linear')\n",
    "        \n",
    "        # Reindex to match the full planet data date range\n",
    "        daily_sidc = daily_sidc.reindex(full_date_range).rename('SIDC_SSN')\n",
    "        \n",
    "        logger.info(\"SIDC monthly smoothed data loaded and interpolated to daily.\")\n",
    "        return daily_sidc\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"SIDC monthly data file not found: {file_path}. Skipping this column.\")\n",
    "        return None\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Column name error in SIDC file: {e}. Please ensure columns are ['Year', 'Month', 'SSN'].\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading SIDC monthly data: {e}. Skipping this column.\")\n",
    "        return None\n",
    "\n",
    "def get_smoothed_sunspots(raw_sunspot_series, logger):\n",
    "    \"\"\"Calculate smoothed sunspot trend line.\"\"\"\n",
    "    logger.info(\"Calculating SSN smoothed trend line...\")\n",
    "    OPTIMAL_LAMBDA = 7e7\n",
    "    raw_values = raw_sunspot_series.values\n",
    "    smoothed_values, _ = asls(raw_values, lam=OPTIMAL_LAMBDA, p=0.5)\n",
    "    logger.info(\"SSN smoothed trend line calculation complete.\")\n",
    "    return pd.Series(smoothed_values, index=raw_sunspot_series.index)\n",
    "\n",
    "def calculate_psd_welch(data_series, fs=1.0, nperseg=365*22):\n",
    "    \"\"\"Calculate Power Spectral Density using Welch's method.\"\"\"\n",
    "    # Ensure no NaN\n",
    "    valid_data_series = data_series.dropna()\n",
    "    if valid_data_series.empty:\n",
    "        return np.array([]), np.array([])\n",
    "        \n",
    "    sig = valid_data_series.values - valid_data_series.mean()\n",
    "    frequencies, psd = signal.welch(sig, fs=fs, nperseg=nperseg)\n",
    "    \n",
    "    valid_mask = frequencies > 0\n",
    "    if not np.any(valid_mask):\n",
    "        return np.array([]), np.array([])\n",
    "        \n",
    "    periods_days = 1 / frequencies[valid_mask]\n",
    "    power_spectral_density = psd[valid_mask]\n",
    "    \n",
    "    sort_idx = np.argsort(periods_days)\n",
    "    return periods_days[sort_idx], power_spectral_density[sort_idx]\n",
    "\n",
    "def find_main_periods(periods, power, top_n=100, min_period=10, max_period=20000):\n",
    "    \"\"\"\n",
    "    Find main periods from the power spectrum.\n",
    "    (Sorted by power descending, i.e., significance)\n",
    "    \"\"\"\n",
    "    mask = (periods >= min_period) & (periods <= max_period)\n",
    "    periods_valid = periods[mask]\n",
    "    power_valid = power[mask]\n",
    "    \n",
    "    if len(periods_valid) == 0:\n",
    "        return np.full(top_n, np.nan)\n",
    "    \n",
    "    # --- Sort by power (significance) ---\n",
    "    idx = np.argsort(power_valid)[-top_n:][::-1]\n",
    "    main_periods = periods_valid[idx]\n",
    "    \n",
    "    if len(main_periods) < top_n:\n",
    "        main_periods = np.pad(main_periods, (0, top_n - len(main_periods)), constant_values=np.nan)\n",
    "    \n",
    "    return main_periods\n",
    "\n",
    "def enhanced_spectral_analysis(residuals_series, label, logger):\n",
    "    \"\"\"\n",
    "    Enhanced spectral analysis (for residuals).\n",
    "    (Returns periods, psd, stats for reuse)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Performing spectral analysis on Residuals for {label}...\")\n",
    "    periods, psd = calculate_psd_welch(residuals_series, nperseg=365*22)\n",
    "    \n",
    "    main_periods = find_main_periods(periods, psd, top_n=10)\n",
    "    valid_periods = main_periods[~np.isnan(main_periods)]\n",
    "    \n",
    "    stats = {\n",
    "        'model': label,\n",
    "        'total_variance': residuals_series.var(),\n",
    "        'spectral_peak_frequency': periods[np.argmax(psd)] if len(periods) > 0 else np.nan,\n",
    "        'top_periods': valid_periods,\n",
    "        'mean_period': np.mean(valid_periods) if len(valid_periods) > 0 else np.nan,\n",
    "        'period_std': np.std(valid_periods) if len(valid_periods) > 0 else np.nan,\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"{label} Residual spectral analysis complete: {len(valid_periods)} valid periods found.\")\n",
    "    return periods, psd, stats\n",
    "\n",
    "def calculate_model_metrics(all_residuals_ts, df_sunspot_raw, logger):\n",
    "    \"\"\"Calculate model performance metrics.\"\"\"\n",
    "    logger.info(\"Calculating model performance metrics...\")\n",
    "    metrics = []\n",
    "    \n",
    "    for label, residuals in all_residuals_ts.items():\n",
    "        aligned_data = pd.DataFrame({\n",
    "            'actual': df_sunspot_raw,\n",
    "            'residual': residuals\n",
    "        }).dropna()\n",
    "        \n",
    "        if len(aligned_data) == 0:\n",
    "            logger.warning(f\"Model {label} has no valid aligned data, skipping metrics.\")\n",
    "            continue\n",
    "            \n",
    "        actual = aligned_data['actual']\n",
    "        residual = aligned_data['residual']\n",
    "        \n",
    "        mse = np.mean(residual**2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(residual))\n",
    "        \n",
    "        model_metrics = {\n",
    "            'Model': label,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'Mean_Residual': np.mean(residual),\n",
    "            'Std_Residual': np.std(residual),\n",
    "            'Max_Residual': np.max(np.abs(residual)),\n",
    "            'R2_vs_actual': max(0, 1 - (np.sum(residual**2) / np.sum((actual - actual.mean())**2))),\n",
    "            'Data_Points': len(aligned_data),\n",
    "        }\n",
    "        metrics.append(model_metrics)\n",
    "        \n",
    "        logger.info(f\"{label}: RMSE={rmse:.3f}, MAE={mae:.3f}, R²={model_metrics['R2_vs_actual']:.3f}\")\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def extract_model_info(saved_data, label, logger):\n",
    "    \"\"\"Extract model information from saved data.\"\"\"\n",
    "    pipeline = saved_data.get('model_pipeline')\n",
    "    features = saved_data.get('features')\n",
    "    \n",
    "    if pipeline is None:\n",
    "        logger.error(f\"Model {label} missing 'model_pipeline'\")\n",
    "        return None, None, None\n",
    "    \n",
    "    if features is None:\n",
    "        logger.error(f\"Model {label} missing 'features'\")\n",
    "        return None, None, None\n",
    "    \n",
    "    model_info = {\n",
    "        'Model': label, \n",
    "        'cv_r2_score': saved_data.get('cv_r2_score'),\n",
    "        'oot_r2_score': saved_data.get('oot_r2_score'),\n",
    "        'best_params': saved_data.get('best_params'),\n",
    "        'star_count': saved_data.get('star_count'),\n",
    "        'dimension_mode': saved_data.get('dimension_mode')\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"{label} - Feature count: {len(features)}, Stars: {model_info['star_count']}, Mode: {model_info['dimension_mode']}\")\n",
    "    logger.info(f\"{label} - CV R²: {model_info.get('cv_r2_score', 'N/A'):.4f}, OOT R²: {model_info.get('oot_r2_score', 'N/A'):.4f}\")\n",
    "    \n",
    "    return pipeline, features, model_info\n",
    "\n",
    "# --- Main Analysis Flow ---\n",
    "def main_analysis():\n",
    "    \"\"\"Main analysis function.\"\"\"\n",
    "    logger = setup_logging()\n",
    "    logger.info(f\"Starting batch analysis of {len(MODEL_FILES)} models...\")\n",
    "    \n",
    "    try:\n",
    "        # Load base data\n",
    "        df_sunspot_raw = load_sunspot_data(logger)\n",
    "        df_planet_all = load_planet_data(logger)\n",
    "        \n",
    "        # Define global date index\n",
    "        full_date_range = df_planet_all.index\n",
    "        \n",
    "        # Calculate smoothed trend line\n",
    "        df_sunspot_smooth = get_smoothed_sunspots(df_sunspot_raw, logger)\n",
    "\n",
    "        # Load SIDC interpolated data\n",
    "        df_sidc_daily = load_sidc_interpolated_data(logger, SIDC_MONTHLY_FILE, full_date_range)\n",
    "\n",
    "        # Prepare result storage\n",
    "        all_results_for_csv = {} # Stores fitted values and residuals for final CSV\n",
    "        all_residuals_ts = {} # Stores valid residuals for statistics and spectrum\n",
    "        all_spectral_stats = []\n",
    "        all_model_info_list = [] # Stores model info dictionaries\n",
    "        all_psd_results = {} # Stores Residual PSD results\n",
    "        all_psd_fits_results = {} # (v7) Stores Fitted Value PSD results\n",
    "\n",
    "        # Loop through each model\n",
    "        for model_file, label in zip(MODEL_FILES, MODEL_LABELS):\n",
    "            logger.info(f\"--- Processing Model: {label} ({model_file}) ---\")\n",
    "            \n",
    "            try:\n",
    "                # Load model\n",
    "                model_path = os.path.join(MODEL_DIR, model_file)\n",
    "                logger.info(f\"Loading model file: {model_path}\")\n",
    "                \n",
    "                if not os.path.exists(model_path):\n",
    "                    logger.error(f\"Model file does not exist: {model_path}\")\n",
    "                    continue\n",
    "                    \n",
    "                saved_data = joblib.load(model_path)\n",
    "                pipeline, features, model_info = extract_model_info(saved_data, label, logger)\n",
    "                \n",
    "                if pipeline is None:\n",
    "                    continue\n",
    "                    \n",
    "                all_model_info_list.append(model_info)\n",
    "\n",
    "                # Check features\n",
    "                missing_features = set(features) - set(df_planet_all.columns)\n",
    "                if missing_features:\n",
    "                    logger.error(f\"{label} is missing {len(missing_features)} features. Model requires all original features.\")\n",
    "                    logger.error(f\"Example missing features: {list(missing_features)[:5]}\")\n",
    "                    logger.error(f\"Skipping model: {model_file}\")\n",
    "                    continue \n",
    "                \n",
    "                logger.info(f\"{label} using all {len(features)} features for prediction.\")\n",
    "                X_full = df_planet_all[features].copy()\n",
    "                \n",
    "                # Predict\n",
    "                logger.info(\"Performing full prediction...\")\n",
    "                ssn_pred = pipeline.predict(X_full)\n",
    "                ssn_pred_series = pd.Series(ssn_pred, index=X_full.index, name=f'Fit_SSN_{label}')\n",
    "                \n",
    "                logger.info(f\"Calculating residuals and storing results for {label}...\")\n",
    "                \n",
    "                # Calculate Residuals\n",
    "                df_temp_calc = pd.DataFrame({'pred': ssn_pred_series, 'actual': df_sunspot_raw})\n",
    "                residual_series = df_temp_calc['actual'] - df_temp_calc['pred']\n",
    "                residual_series.name = f'Residual_{label}'\n",
    "\n",
    "                # Store for comprehensive CSV\n",
    "                all_results_for_csv[label] = pd.concat([ssn_pred_series, residual_series], axis=1)\n",
    "                \n",
    "                # Extract valid residuals on 'df_sunspot_raw' original index\n",
    "                valid_residuals = residual_series.reindex(df_sunspot_raw.index).dropna()\n",
    "                all_residuals_ts[label] = valid_residuals\n",
    "\n",
    "                # Spectral Analysis (Residuals)\n",
    "                periods, psd, spectral_stats = enhanced_spectral_analysis(valid_residuals, label, logger)\n",
    "                all_spectral_stats.append(spectral_stats)\n",
    "                all_psd_results[label] = (periods, psd) \n",
    "\n",
    "                # (v7) Calculate Fit Spectrum\n",
    "                try:\n",
    "                    logger.info(f\"Calculating spectrum for Fitted Values of {label}...\")\n",
    "                    periods_fit, psd_fit = calculate_psd_welch(ssn_pred_series.dropna(), nperseg=365*22)\n",
    "                    if len(periods_fit) > 0:\n",
    "                        all_psd_fits_results[label] = (periods_fit, psd_fit)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error calculating Fit spectrum for {label}: {e}\")\n",
    "                \n",
    "                # Cleanup\n",
    "                del X_full, ssn_pred, ssn_pred_series, residual_series, df_temp_calc, valid_residuals\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing model {model_file}: {e}\", exc_info=True)\n",
    "                continue\n",
    "\n",
    "        if not all_residuals_ts:\n",
    "            logger.error(\"No models successfully processed. Analysis aborted.\")\n",
    "            return\n",
    "\n",
    "        # --- Generate Comprehensive Results CSV ---\n",
    "        \n",
    "        logger.info(\"Generating comprehensive fitted results CSV...\")\n",
    "        df_complete_results = pd.DataFrame(index=full_date_range)\n",
    "\n",
    "        # Base Data\n",
    "        df_complete_results['Raw_SSN'] = df_sunspot_raw\n",
    "        df_complete_results['Smoothed_SSN'] = df_sunspot_smooth\n",
    "        if df_sidc_daily is not None:\n",
    "            df_complete_results['SIDC_SSN'] = df_sidc_daily\n",
    "\n",
    "        # Add fits and residuals for all models\n",
    "        for label in MODEL_LABELS:\n",
    "            if label in all_results_for_csv:\n",
    "                df_complete_results = df_complete_results.join(all_results_for_csv[label])\n",
    "\n",
    "        # Save complete results\n",
    "        csv_complete_filename = os.path.join(OUTPUT_DIR, 'Summary_Fit_Results.csv')\n",
    "        df_complete_results.to_csv(csv_complete_filename, index=True, index_label='Date', encoding='utf-8-sig')\n",
    "        logger.info(f\"Saved complete results: {csv_complete_filename}\")\n",
    "        logger.info(f\"File contains {len(df_complete_results.columns)} columns: {list(df_complete_results.columns)}\")\n",
    "\n",
    "        # --- Merge Model Info and Metrics ---\n",
    "        logger.info(\"Merging model info and metrics...\")\n",
    "        try:\n",
    "            model_info_df = pd.DataFrame(all_model_info_list)\n",
    "            metrics_df = calculate_model_metrics(all_residuals_ts, df_sunspot_raw, logger)\n",
    "            \n",
    "            if not model_info_df.empty and not metrics_df.empty:\n",
    "                combined_stats_df = pd.merge(model_info_df, metrics_df, on='Model', how='outer')\n",
    "            elif not model_info_df.empty:\n",
    "                combined_stats_df = model_info_df\n",
    "            elif not metrics_df.empty:\n",
    "                combined_stats_df = metrics_df\n",
    "            else:\n",
    "                combined_stats_df = pd.DataFrame() \n",
    "                \n",
    "            if not combined_stats_df.empty:\n",
    "                stats_filename = os.path.join(OUTPUT_DIR, 'Model_Comprehensive_Stats.csv')\n",
    "                combined_stats_df.to_csv(stats_filename, index=False, encoding='utf-8-sig')\n",
    "                logger.info(f\"Saved combined stats: {stats_filename}\")\n",
    "            else:\n",
    "                logger.warning(\"Model info and metrics are empty. No stats file generated.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error merging stats: {e}\", exc_info=True)\n",
    "        \n",
    "        # Save Spectral Stats (Residual based)\n",
    "        spectral_stats_df = pd.DataFrame(all_spectral_stats)\n",
    "        spectral_stats_filename = os.path.join(OUTPUT_DIR, 'Spectral_Stats.csv')\n",
    "        spectral_stats_df.to_csv(spectral_stats_filename, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Saved spectral stats: {spectral_stats_filename}\")\n",
    "\n",
    "        # --- Spectral Analysis Visualization (v7) ---\n",
    "        logger.info(\"Plotting 1x3 Spectral Analysis (Welch)...\")\n",
    "        \n",
    "        # 1. Calculate Base SSN Spectrum\n",
    "        logger.info(\"Calculating spectrum for Raw, Smoothed, and SIDC SSN...\")\n",
    "        psd_results_base = {}\n",
    "        try:\n",
    "            periods_raw, psd_raw = calculate_psd_welch(df_sunspot_raw)\n",
    "            if len(periods_raw) > 0:\n",
    "                psd_results_base['Raw_SSN'] = (periods_raw, psd_raw)\n",
    "                \n",
    "            periods_smooth, psd_smooth = calculate_psd_welch(df_sunspot_smooth)\n",
    "            if len(periods_smooth) > 0:\n",
    "                psd_results_base['Smoothed_SSN'] = (periods_smooth, psd_smooth)\n",
    "            \n",
    "            if df_sidc_daily is not None:\n",
    "                periods_sidc, psd_sidc = calculate_psd_welch(df_sidc_daily)\n",
    "                if len(periods_sidc) > 0:\n",
    "                    psd_results_base['SIDC_SSN'] = (periods_sidc, psd_sidc)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating base SSN spectrum: {e}\", exc_info=True)\n",
    "        \n",
    "        # 2. Create 1x3 Subplots\n",
    "        fig_psd, axes = plt.subplots(1, 3, figsize=(24, 8), sharey=True)\n",
    "        all_periods_dict = {} \n",
    "        \n",
    "        # Styles\n",
    "        base_styles = {\n",
    "            'Raw_SSN': ('black', '--', 2.0),\n",
    "            'Smoothed_SSN': ('gray', ':', 2.0),\n",
    "            'SIDC_SSN': ('orange', '-.', 2.0)\n",
    "        }\n",
    "        colors = ['blue', 'red', 'green', 'purple'] \n",
    "        \n",
    "        # --- 3. Left Plot (Base SSN) ---\n",
    "        ax_left = axes[0]\n",
    "        for label, (periods, psd) in psd_results_base.items():\n",
    "            style = base_styles.get(label)\n",
    "            ax_left.loglog(periods, psd, label=label.replace('_', ' '), color=style[0], linestyle=style[1], linewidth=style[2], alpha=0.9)\n",
    "        \n",
    "        ax_left.set_title('Base SSN Spectrum')\n",
    "        ax_left.set_xlabel('Period (Days)', fontsize=12)\n",
    "        ax_left.set_ylabel('Power Spectral Density', fontsize=12)\n",
    "        ax_left.legend(fontsize=10)\n",
    "        ax_left.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\n",
    "\n",
    "        # --- 4. Middle Plot (Fits) ---\n",
    "        ax_mid = axes[1]\n",
    "        for i, (label, (periods, psd)) in enumerate(all_psd_fits_results.items()):\n",
    "            color = colors[i % len(colors)]\n",
    "            ax_mid.loglog(periods, psd, label=f'Fit - {label}', alpha=0.8, color=color, linewidth=1.5)\n",
    "\n",
    "        ax_mid.set_title('Model Fits Spectrum')\n",
    "        ax_mid.set_xlabel('Period (Days)', fontsize=12)\n",
    "        ax_mid.legend(fontsize=10)\n",
    "        ax_mid.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\n",
    "\n",
    "        # --- 5. Right Plot (Residuals) ---\n",
    "        ax_right = axes[2]\n",
    "        for i, (label, (periods, psd)) in enumerate(all_psd_results.items()):\n",
    "            color = colors[i % len(colors)]\n",
    "            ax_right.loglog(periods, psd, label=f'Resid - {label}', alpha=0.8, color=color, linewidth=1.5)\n",
    "            \n",
    "            # Find Top 100 periods for residuals\n",
    "            main_periods = find_main_periods(periods, psd, top_n=100, max_period=20000)\n",
    "            all_periods_dict[f'Period_{label}'] = main_periods\n",
    "\n",
    "        ax_right.set_title('Model Residuals Spectrum')\n",
    "        ax_right.set_xlabel('Period (Days)', fontsize=12)\n",
    "        ax_right.legend(fontsize=10)\n",
    "        ax_right.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\n",
    "        \n",
    "        # --- 6. Save Plot ---\n",
    "        fig_psd.suptitle(f'Spectral Comparison 1x3 (Welch Method)', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        plot_filename = os.path.join(OUTPUT_DIR, 'Comparison_Spectrum_1x3.png')\n",
    "        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig_psd)\n",
    "        logger.info(f\"Saved 1x3 Spectrum Plot: {plot_filename}\")\n",
    "\n",
    "        # Save Period Data (Residuals only)\n",
    "        all_periods_df = pd.DataFrame({k: pd.Series(v) for k, v in all_periods_dict.items()})\n",
    "        csv_periods_filename = os.path.join(OUTPUT_DIR, 'Comparison_Top100_Periods.csv')\n",
    "        all_periods_df.to_csv(csv_periods_filename, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Saved Periods CSV: {csv_periods_filename}\")\n",
    "\n",
    "        # ACF/PACF Analysis\n",
    "        logger.info(\"Plotting ACF/PACF...\")\n",
    "        num_models = len(all_residuals_ts)\n",
    "        if num_models == 0:\n",
    "            logger.warning(\"No models available for ACF/PACF analysis.\")\n",
    "        else:\n",
    "            fig_acf, axes = plt.subplots(num_models, 2, figsize=(16, 5 * num_models), squeeze=False) \n",
    "            fig_acf.suptitle('ACF and PACF Analysis of Model Residuals', fontsize=20, y=1.02)\n",
    "            \n",
    "            if num_models == 1:\n",
    "                fig_acf.set_size_inches(16, 5)\n",
    "                \n",
    "            for i, (label, residuals) in enumerate(all_residuals_ts.items()):\n",
    "                plot_acf(residuals, lags=60, ax=axes[i, 0], title=f'ACF - {label}')\n",
    "                axes[i, 0].set_xlabel('Lags')\n",
    "                plot_pacf(residuals, lags=60, ax=axes[i, 1], title=f'PACF - {label}', method='ywm')\n",
    "                axes[i, 1].set_xlabel('Lags')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            acf_plot_filename = os.path.join(OUTPUT_DIR, 'Comparison_ACF_PACF.png')\n",
    "            plt.savefig(acf_plot_filename, dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig_acf) \n",
    "            logger.info(f\"Saved ACF/PACF Plot: {acf_plot_filename}\")\n",
    "        \n",
    "        logger.info(\"Batch analysis complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during analysis: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Filter warnings\n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    \n",
    "    # Fix matplotlib logging spam\n",
    "    logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "    \n",
    "    main_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee77d8-1e10-4a72-8271-c1ba2ec5e092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
