{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b51a7fd-282b-41b5-b88e-2849ba72349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from: ../../results/05_p_m_a_model/a_model_4/LSTM_Full_Results_SEQLEN400_20251229_221056.csv\n",
      "Evaluating Model: M8+2...\n",
      "Evaluating Model: M8+3...\n",
      "Evaluating Model: M0+3...\n",
      "Evaluating Model: M0+2...\n",
      "\n",
      "===============================================================================================\n",
      "FINAL MODEL VERIFICATION SUMMARY TABLE\n",
      "===============================================================================================\n",
      "Model     R2    PSS  DM_Stat  p-value  DA_Daily%  DA_Weekly%  DA_Monthly%\n",
      " M8+2 0.9538 0.0441  41.1802   0.0000    45.3873     87.9211      95.6989\n",
      " M8+3 0.9602 0.0572  61.8111   0.0000    47.1535     89.9367      95.2381\n",
      " M0+3 0.9612 0.0827  55.6150   0.0000    47.5539     89.5617      95.2261\n",
      " M0+2 0.9555 0.0784  52.3949   0.0000    45.5047     88.1676      96.4158\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# ================= Configuration =================\n",
    "CSV_FILE_PATH = \"../../results/05_p_m_a_model/a_model_4/LSTM_Full_Results_SEQLEN400_20251229_221056.csv\"\n",
    "TARGET_COL = 'Raw_SSN'\n",
    "TEST_END_DATE = '2019-11-30'\n",
    "\n",
    "# Models and their specific OOT (Out-of-Sample) start dates\n",
    "MODEL_CONFIGS = {\n",
    "    'M8+2': '1996-08-01',\n",
    "    'M8+3': '1986-08-01',\n",
    "    'M0+3': '1986-09-01',\n",
    "    'M0+2': '1996-08-01'\n",
    "}\n",
    "\n",
    "# ================= Core Metric Functions =================\n",
    "\n",
    "def dm_test(actual, pred_bench, pred_hybrid, h=1, criterion='MSE'):\n",
    "    \"\"\"Diebold-Mariano Test for statistical significance[cite: 1, 10].\"\"\"\n",
    "    e1 = np.array(actual) - np.array(pred_bench)\n",
    "    e2 = np.array(actual) - np.array(pred_hybrid)\n",
    "    \n",
    "    if criterion == 'MSE':\n",
    "        d = e1**2 - e2**2\n",
    "    else:\n",
    "        d = np.abs(e1) - np.abs(e2)\n",
    "        \n",
    "    T = float(len(d))\n",
    "    mean_d = np.mean(d)\n",
    "    \n",
    "    def autocovariance(x, k):\n",
    "        if k == 0:\n",
    "            return np.var(x)\n",
    "        return np.sum((x[:-k] - np.mean(x)) * (x[k:] - np.mean(x))) / len(x)\n",
    "    \n",
    "    gamma = [autocovariance(d, lag) for lag in range(h)]\n",
    "    v_d = (gamma[0] + 2 * sum(gamma[1:])) / T\n",
    "    \n",
    "    if v_d <= 0: return 0.0, 1.0\n",
    "    dm_stat = mean_d / np.sqrt(v_d)\n",
    "    p_value = 2 * (1 - stats.norm.cdf(np.abs(dm_stat)))\n",
    "    return dm_stat, p_value\n",
    "\n",
    "def calculate_da(y_true, y_pred):\n",
    "    \"\"\"Directional Accuracy (Trend consistency)[cite: 18, 19].\"\"\"\n",
    "    actual_diff = np.sign(np.diff(y_true))\n",
    "    pred_diff = np.sign(np.diff(y_pred))\n",
    "    return np.mean(actual_diff == pred_diff) * 100\n",
    "\n",
    "def run_comprehensive_evaluation():\n",
    "    print(f\"Loading results from: {CSV_FILE_PATH}\")\n",
    "    df = pd.read_csv(CSV_FILE_PATH, parse_dates=['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "    \n",
    "    # Calculate Persistence (Naive) Baseline: y_t = y_{t-1} [cite: 50, 74]\n",
    "    df['Persistence_Pred'] = df[TARGET_COL].shift(1)\n",
    "    \n",
    "    summary_results = []\n",
    "\n",
    "    for model_name, split_date in MODEL_CONFIGS.items():\n",
    "        print(f\"Evaluating Model: {model_name}...\")\n",
    "        \n",
    "        # 1. Column Mapping\n",
    "        hybrid_col = f'Total_Pred_{model_name}'\n",
    "        p_model_col = f'Fit_SSN_{model_name}'\n",
    "        lgbm_col = f'LGBM_Pred_{model_name}'\n",
    "        # Benchmark for DM: P-Model + LGBM (The state before LSTM) [cite: 5]\n",
    "        bench_col = f'Bench_{model_name}'\n",
    "        df[bench_col] = df[p_model_col] + df[lgbm_col].fillna(0)\n",
    "        \n",
    "        # 2. Filter Test Set\n",
    "        mask = (df['Date'] > pd.to_datetime(split_date)) & (df['Date'] <= pd.to_datetime(TEST_END_DATE))\n",
    "        test_df = df.loc[mask].dropna(subset=[TARGET_COL, hybrid_col, 'Persistence_Pred'])\n",
    "        \n",
    "        y_true = test_df[TARGET_COL].values\n",
    "        y_hybrid = test_df[hybrid_col].values\n",
    "        y_bench = test_df[bench_col].values\n",
    "        y_pers = test_df['Persistence_Pred'].values\n",
    "        \n",
    "        # 3. Calculate Metrics\n",
    "        # PSS (Prediction Skill Score) vs Persistence [cite: 52, 62]\n",
    "        mse_hybrid = mean_squared_error(y_true, y_hybrid)\n",
    "        mse_pers = mean_squared_error(y_true, y_pers)\n",
    "        pss = 1 - (mse_hybrid / mse_pers)\n",
    "        \n",
    "        # DM Test (Hybrid vs Bench) [cite: 7]\n",
    "        dm_stat, p_val = dm_test(y_true, y_bench, y_hybrid)\n",
    "        \n",
    "        # Multi-scale DA [cite: 21]\n",
    "        da_daily = calculate_da(y_true, y_hybrid)\n",
    "        \n",
    "        # Weekly/Monthly Aggregation for DA\n",
    "        test_resampled = test_df.set_index('Date')[[TARGET_COL, hybrid_col]]\n",
    "        da_weekly = calculate_da(\n",
    "            test_resampled[TARGET_COL].resample('W').mean().values,\n",
    "            test_resampled[hybrid_col].resample('W').mean().values\n",
    "        )\n",
    "        da_monthly = calculate_da(\n",
    "            test_resampled[TARGET_COL].resample('ME').mean().values,\n",
    "            test_resampled[hybrid_col].resample('ME').mean().values\n",
    "        )\n",
    "        \n",
    "        # R2 Score\n",
    "        r2 = r2_score(y_true, y_hybrid)\n",
    "        \n",
    "        summary_results.append({\n",
    "            'Model': model_name,\n",
    "            'R2': r2,\n",
    "            'PSS': pss,\n",
    "            'DM_Stat': dm_stat,\n",
    "            'p-value': p_val,\n",
    "            'DA_Daily%': da_daily,\n",
    "            'DA_Weekly%': da_weekly,\n",
    "            'DA_Monthly%': da_monthly\n",
    "        })\n",
    "\n",
    "    # 4. Generate Final Table\n",
    "    res_df = pd.DataFrame(summary_results)\n",
    "    print(\"\\n\" + \"=\"*95)\n",
    "    print(\"FINAL MODEL VERIFICATION SUMMARY TABLE\")\n",
    "    print(\"=\"*95)\n",
    "    pd.options.display.float_format = '{:,.4f}'.format\n",
    "    print(res_df.to_string(index=False))\n",
    "    print(\"=\"*95)\n",
    "    \n",
    "    # Save to CSV for LaTeX formatting\n",
    "    # res_df.to_csv(\"model_verification_summary.csv\", index=False)\n",
    "    # print(\"\\nResults saved to 'model_verification_summary.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_comprehensive_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dcb552-26f4-44b4-849a-72a19827d2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py312sunspot)",
   "language": "python",
   "name": "py312_sunspot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
