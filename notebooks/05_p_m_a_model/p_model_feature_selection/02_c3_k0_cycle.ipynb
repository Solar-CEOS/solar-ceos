{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11961775-712d-4555-8a4a-0f8225555d66",
   "metadata": {},
   "source": [
    "# P Model Feature Selection - Solar Cycle CV Version\n",
    "\n",
    "## Current Configuration\n",
    "\n",
    "- **CV Strategy**: Group by Solar Cycle (GroupKFold by Solar Cycle)\n",
    "- **Training Set**: 1855-12-02 ~ 1996-08-01 (Reserve last 2 cycles for testing)\n",
    "- **Testing Set**: 1996-08-02 ~ 2019-11-30\n",
    "- **Base Planets**: No forced inclusion `[]`\n",
    "\n",
    "---\n",
    "\n",
    "## Configurable Parameters\n",
    "\n",
    "Modify in the 2nd code cell:\n",
    "\n",
    "### 1. Base Planet Features\n",
    "\n",
    "```python\n",
    "# Current: 0-planet mode (start from 0)\n",
    "BASE_PLANET_FEATURES = []\n",
    "\n",
    "# Optional: 8-planet mode\n",
    "# BASE_PLANET_FEATURES = ['199','299','399','499','599','699','799','899']\n",
    "```\n",
    "\n",
    "### 2. Train/Test Split\n",
    "\n",
    "```python\n",
    "# Current: Reserve last 2 cycles for testing\n",
    "train_end_date = '1996-08-01'\n",
    "\n",
    "# Optional: Reserve last 3 cycles for testing\n",
    "# train_end_date = '1986-09-01'\n",
    "```\n",
    "\n",
    "**Note**: For yearly CV strategy, use `01_c2_n8_year.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55e51a-1fb8-4eec-a9dd-97d74c1f8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import and Function Definitions\n",
    "\n",
    "# --- Core and Basic Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.lines import Line2D\n",
    "from io import StringIO\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Scientific Computing and Machine Learning Libraries ---\n",
    "from scipy.signal import find_peaks\n",
    "from pybaselines.whittaker import asls\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pygam import LinearGAM, s, l\n",
    "import statsmodels.api as sm\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "\n",
    "# --- Bayesian Optimization Libraries ---\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# --- Environment Settings ---\n",
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'WenQuanYi Micro Hei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# --- Helper Function Definitions ---\n",
    "\n",
    "def get_smoothed_sunspots(raw_sunspot_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Smooth the raw sunspot number series using Asymmetric Least Squares (asls).\"\"\"\n",
    "    OPTIMAL_LAMBDA = 7e7\n",
    "    raw_values = raw_sunspot_series.values\n",
    "    smoothed_values, _ = asls(raw_values, lam=OPTIMAL_LAMBDA, p=0.5)\n",
    "    return pd.Series(smoothed_values, index=raw_sunspot_series.index)\n",
    "\n",
    "def find_peaks_valleys(data_series: pd.Series, distance_days: int = 365 * 8, prominence_peaks: int = 40, prominence_valleys: int = 5):\n",
    "    \"\"\"Find peaks (maxima) and valleys (minima) in time series.\"\"\"\n",
    "    peaks_idx, _ = find_peaks(data_series, distance=distance_days, prominence=prominence_peaks)\n",
    "    df_peaks = pd.DataFrame({'date': data_series.index[peaks_idx], 'SSN': data_series.iloc[peaks_idx]}).set_index('date')\n",
    "    \n",
    "    valleys_idx, _ = find_peaks(-data_series, distance=distance_days, prominence=prominence_valleys)\n",
    "    df_valleys = pd.DataFrame({'date': data_series.index[valleys_idx], 'SSN': data_series.iloc[valleys_idx]}).set_index('date')\n",
    "    \n",
    "    return df_peaks, df_valleys\n",
    "\n",
    "def calculate_deviation(model_extrema: pd.DataFrame, known_extrema: pd.DataFrame):\n",
    "    \"\"\"Calculate phase and amplitude deviations between model-predicted and known extrema.\"\"\"\n",
    "    if model_extrema.empty or known_extrema.empty: \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    left_df = model_extrema.sort_index().reset_index().rename(columns={model_extrema.index.name or 'date': 'model_date', 'SSN': 'model_SSN'})\n",
    "    right_df = known_extrema.sort_index().reset_index().rename(columns={known_extrema.index.name or 'date': 'known_date', 'SSN': 'known_SSN'})\n",
    "    \n",
    "    merged_df = pd.merge_asof(left_df, right_df, left_on='model_date', right_on='known_date', direction='nearest', tolerance=pd.Timedelta(days=365 * 5.5)).dropna()\n",
    "    \n",
    "    if merged_df.empty: \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    merged_df['phase_deviation_days'] = (merged_df['model_date'] - merged_df['known_date']).dt.days\n",
    "    merged_df['amplitude_deviation'] = merged_df['model_SSN'] - merged_df['known_SSN']\n",
    "    \n",
    "    result_df = merged_df[['model_date', 'model_SSN', 'known_date', 'known_SSN', 'phase_deviation_days', 'amplitude_deviation']]\n",
    "    result_df.columns = ['Model Date', 'Model SSN', 'Known Date', 'Known SSN', 'Phase Deviation (days)', 'Amplitude Deviation']\n",
    "    return result_df\n",
    "\n",
    "def plot_results(filename: Path, raw_ssn: pd.Series, smoothed_ssn: pd.Series, fitted_ssn: pd.Series,\n",
    "                 train_range: tuple, test_range: tuple, pred_range: tuple,\n",
    "                 known_extrema: pd.DataFrame, next_peak_info: dict, eval_metrics: dict, best_params: dict):\n",
    "    \"\"\"Generate and save comprehensive results plot.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(25, 10))\n",
    "    \n",
    "    # Plot various data series\n",
    "    ax.plot(raw_ssn.index, raw_ssn, '.', color='gray', alpha=0.5, label='Raw SSN')\n",
    "    ax.plot(smoothed_ssn.index, smoothed_ssn, 'b-', label='Smoothed SSN (Historical)')\n",
    "    \n",
    "    # Historical fit with solid line\n",
    "    fit_part = fitted_ssn.loc[train_range[0]:test_range[1]]\n",
    "    ax.plot(fit_part.index, fit_part, 'r-', linewidth=2, label='Fitted SSN')\n",
    "    \n",
    "    # Future prediction with dashed line\n",
    "    pred_part = fitted_ssn.loc[pred_range[0]:pred_range[1]]\n",
    "    ax.plot(pred_part.index, pred_part, 'r--', linewidth=2, label='Predicted SSN')\n",
    "    \n",
    "    # Format axes\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator(10))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.set_xlim(pd.Timestamp('1845-01-01'), pd.Timestamp('2055-12-31'))\n",
    "    \n",
    "    # Prepare title information\n",
    "    peak_ssn_val = next_peak_info['ssn']\n",
    "    peak_ssn_str = f\"{peak_ssn_val:.1f}\" if isinstance(peak_ssn_val, (int, float)) else peak_ssn_val\n",
    "    \n",
    "    # Extract parameters from best_params for display\n",
    "    display_params = {}\n",
    "    for k, v in best_params.items():\n",
    "        simple_key = k.split('__')[-1]\n",
    "        display_params[simple_key] = v\n",
    "    params_str = ', '.join([f'{k}={v:.4f}' if isinstance(v, float) else f'{k}={v}' for k, v in display_params.items()])\n",
    "\n",
    "    # Set multi-line title\n",
    "    title_line1 = f\"Model: {filename.stem}\"\n",
    "    title_line2 = f\"Best Params: {params_str} | CV R²: {eval_metrics['r2']:.4f} | OOT R²: {eval_metrics['oot_r2']:.4f}\"\n",
    "    title_line3 = f\"Next Peak Prediction: {next_peak_info['date']} (SSN: {peak_ssn_str})\"\n",
    "    ax.set_title(f\"{title_line1}\\n{title_line2}\\n{title_line3}\", fontsize=14)\n",
    "    \n",
    "    # Set labels and grid\n",
    "    ax.set_xlabel(\"Year\", fontsize=12)\n",
    "    ax.set_ylabel(\"Sunspot Number (SSN)\", fontsize=12)\n",
    "    ax.grid(True, which='major', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot known extrema as reference vertical lines\n",
    "    min_dates = known_extrema['Min_Date'].dropna()\n",
    "    max_dates = known_extrema['Max_Date'].dropna()\n",
    "    for date in min_dates: ax.axvline(date, color='green', linestyle=':', alpha=0.8)\n",
    "    for date in max_dates: ax.axvline(date, color='orange', linestyle=':', alpha=0.8)\n",
    "    \n",
    "    # Create and display legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    extra_handles = [Line2D([0], [0], color='green', linestyle=':', label='Known Minima'), Line2D([0], [0], color='orange', linestyle=':', label='Known Maxima')]\n",
    "    ax.legend(handles=handles + extra_handles, loc='upper right', fontsize=10)\n",
    "    \n",
    "    # Save image and close figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=150)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6dbb22-b55e-43b3-bdb0-6b69575b37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Main Execution Script (Full 3D/6D Dual Mode + Survey & Fine-tuning - Fixed)\n",
    "# [8 Planets + k Additional Bodies - Modified Version]\n",
    "\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import joblib # Ensure joblib is imported\n",
    "import warnings # Ensure warnings is imported\n",
    "\n",
    "# --- 1. Global Parameter Settings (Fixed) ---\n",
    "\n",
    "# Phase 1: Sparse survey star count range (step size 5)\n",
    "# [Modified] This list now represents \"total star count\", will be used to calculate \"additional star count\"\n",
    "SURVEY_STAR_RANGE = range(10, 51, 5)  # i.e. 10, 15, 20, 25, 30\n",
    "\n",
    "# Phase 2: R² threshold for fine-tuning\n",
    "R2_THRESHOLD = 0.4\n",
    "\n",
    "# Phase 2: Fine-tuning expansion range\n",
    "DENSE_FIT_EXPANSION = 4\n",
    "\n",
    "# --- 1. Define base planet list before your code loop starts ---\n",
    "# (!! Names in list must be exact column names in X_all_raw !!)\n",
    "BASE_PLANET_FEATURES = [ ]\n",
    "# Ensure you actually have 8\n",
    "base_feature_count = len(BASE_PLANET_FEATURES)\n",
    "print(f\"--- [8-Planet Mode] Activated: Forcing inclusion of {base_feature_count} base bodies ---\")\n",
    "\n",
    "\n",
    "# Conjunction ranking data file\n",
    "RANKING_EXCEL_FILE = Path(\"../../../results/04_conj_enh_opp_sup/sg/sg_781_raw_count_area.csv\")\n",
    "\n",
    "# Sorting criteria\n",
    "SORT_CRITERIA_MAP = {\n",
    "    'Conjunction Count': 'Raw_Count', \n",
    "    'Conjunction Area': 'Total_Area', \n",
    "    'Average Conjunction Area': 'Avg_Area'\n",
    "}\n",
    "\n",
    "# Other date parameters\n",
    "train_start_date = '1855-12-02'; train_end_date = '1986-09-01'\n",
    "test_start_date = '1986-09-02'; test_end_date = '2019-11-30'\n",
    "pred_start_date = '2019-12-01'; pred_end_date = '2050-12-31'\n",
    "\n",
    "# CV strategy identifier\n",
    "CV_STRATEGY = 'cycle'  # 'cycle' for solar cycle CV, 'year' for yearly CV\n",
    "\n",
    "# --- Helper Function: Prepare Features and Training Data ---\n",
    "# [Modification Point 1: Function signature and logic]\n",
    "def prepare_features_and_target(final_feature_list, df_781_features_ALL, FEATURES_TO_LOAD, \n",
    "                                 df_sunspot_raw, train_start_date, train_end_date):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix and target variable, return (X_full, X_historical, y_historical, groups)\n",
    "    [Modified] This function now receives a final_feature_list, instead of all_ranked_stars and star_count\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # top_stars = all_ranked_stars[:star_count] # <--- [Modified] Removed\n",
    "        new_columns_data = {}\n",
    "        missing_stars = []\n",
    "        \n",
    "        for star_id in final_feature_list: # <--- [Modified] Loop through final_feature_list\n",
    "            all_coords_found = True\n",
    "            for component in FEATURES_TO_LOAD:\n",
    "                col_name = f'{star_id}_{component}'\n",
    "                if col_name in df_781_features_ALL.columns:\n",
    "                    new_columns_data[col_name] = df_781_features_ALL[col_name]\n",
    "                else:\n",
    "                    all_coords_found = False\n",
    "            if not all_coords_found:\n",
    "                missing_stars.append(star_id)\n",
    "        \n",
    "        if missing_stars:\n",
    "            preview = ', '.join(missing_stars[:5])\n",
    "            suffix = '...' if len(missing_stars) > 5 else ''\n",
    "            # Check if base planets are missing\n",
    "            missing_base = [s for s in missing_stars if s in BASE_PLANET_FEATURES]\n",
    "            if missing_base:\n",
    "                print(f\"        CRITICAL WARNING: Base planets missing {missing_base}, this model may be invalid!\")\n",
    "            else:\n",
    "                print(f\"        Warning: Following 'additional' bodies missing specified components, will be ignored: {preview}{suffix}\")\n",
    "        \n",
    "        if not new_columns_data:\n",
    "            return None, None, None, None\n",
    "        \n",
    "        X_full = pd.DataFrame(new_columns_data, index=df_781_features_ALL.index).sort_index()\n",
    "        \n",
    "        if X_full.empty:\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # Prepare training data\n",
    "        y_historical_raw = df_sunspot_raw.loc[train_start_date:train_end_date]\n",
    "        y_historical_smoothed = get_smoothed_sunspots(y_historical_raw) # Assume get_smoothed_sunspots in Cell 1\n",
    "        Xy_historical_aligned = X_full.join(y_historical_smoothed.rename('SSN')).dropna()\n",
    "        X_historical = Xy_historical_aligned.drop('SSN', axis=1)\n",
    "        y_historical = Xy_historical_aligned['SSN']\n",
    "        \n",
    "        if X_historical.empty or y_historical.empty:\n",
    "            return None, None, None, None\n",
    "        \n",
    "        return X_full, X_historical, y_historical, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"        Failed to prepare feature data: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# --- Helper Function: Execute Bayesian Optimization ---\n",
    "def run_bayesian_optimization(X_historical, y_historical, groups, model_config, n_iter=50):  # Increased from 32 to 50\n",
    "    \"\"\"Execute Bayesian optimization, return (best_score_cv, best_params, best_model)\"\"\"\n",
    "    try:\n",
    "        # Prepare CV grouping\n",
    "        cycle_lookup = df_solar_cycle[['Min_Date', 'SC']].dropna().sort_values('Min_Date')\n",
    "        y_historical.index.name = 'Day'\n",
    "        historical_dates = y_historical.reset_index()\n",
    "        merged_data = pd.merge_asof(historical_dates, cycle_lookup, left_on='Day', right_on='Min_Date', direction='backward')\n",
    "        groups = pd.Series(merged_data['SC'].values, index=y_historical.index)\n",
    "        \n",
    "        num_unique_cycles = merged_data['SC'].nunique()\n",
    "        n_splits = 4 if num_unique_cycles >= 12 else (3 if num_unique_cycles >= 8 else 2)\n",
    "        gkf = GroupKFold(n_splits=n_splits)\n",
    "        \n",
    "        bayes_search = BayesSearchCV(\n",
    "            estimator=model_config[1],\n",
    "            search_spaces=model_config[2],\n",
    "            n_iter=n_iter,\n",
    "            scoring='r2',\n",
    "            cv=gkf,\n",
    "            n_jobs=-2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        bayes_search.fit(X_historical, y_historical, groups=groups)\n",
    "        \n",
    "        return bayes_search.best_score_, bayes_search.best_params_, bayes_search.best_estimator_\n",
    "    \n",
    "    except Exception as opt_error:\n",
    "        print(f\"        Optimization failed: {opt_error}\")\n",
    "        return None, None, None\n",
    "\n",
    "# --- 2. Load Base Data ---\n",
    "try:\n",
    "    print(\"\\nLoading base data (sunspot numbers, cycles, body coordinates)...\")\n",
    "    \n",
    "    df_sunspot_raw = pd.read_csv('../../../data/ready/ssn_daily_1849_2025.csv', parse_dates=['date']).set_index('date')['ssn'].asfreq('D').fillna(0)\n",
    "    df_solar_cycle = pd.read_csv('../../../data/ready/solar_cycle_minmax.csv')\n",
    "    \n",
    "    # Fix date parsing warning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        df_solar_cycle['Min_Date'] = pd.to_datetime(df_solar_cycle['start_Min'], format='%Y-%m')\n",
    "        df_solar_cycle['Max_Date'] = pd.to_datetime(df_solar_cycle['Max'], format='%Y-%m')\n",
    "    \n",
    "    # Load body position and velocity components, align on date index\n",
    "    df_781_coords = pd.read_parquet('../../../data/ready/781_planets_dwarfs_asteroids_xyz.parquet')\n",
    "    df_781_coords = df_781_coords.set_index(pd.to_datetime(df_781_coords['date'])).drop('date', axis=1).sort_index()\n",
    "    df_781_velocity = pd.read_parquet('../../../data/ready/781_planets_dwarfs_asteroids_velocity.parquet')\n",
    "    df_781_velocity = df_781_velocity.set_index(pd.to_datetime(df_781_velocity['date'])).drop('date', axis=1).sort_index()\n",
    "    if not df_781_coords.index.equals(df_781_velocity.index):\n",
    "        print(\"Warning: Position and velocity file date indices don't fully match, aligned using intersection.\")\n",
    "    df_781_features_ALL = df_781_coords.join(df_781_velocity, how='inner').sort_index()\n",
    "    print(f\"Body position and velocity data rows: Position={len(df_781_coords)}, Velocity={len(df_781_velocity)}, Merged={len(df_781_features_ALL)}\")\n",
    "    \n",
    "    known_peaks = df_solar_cycle.dropna(subset=['Max_Date', 'Max_SSN']).set_index('Max_Date')[['Max_SSN']].rename(columns={'Max_SSN': 'SSN'})\n",
    "    known_valleys = df_solar_cycle.dropna(subset=['Min_Date', 'Min_SSN']).set_index('Min_Date')[['Min_SSN']].rename(columns={'Min_SSN': 'SSN'})\n",
    "    \n",
    "    print(\"Base data loading complete.\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Cannot find base data file: {e}\")\n",
    "    df_sunspot_raw = None \n",
    "except Exception as e:\n",
    "    print(f\"Error loading base data: {e}\")\n",
    "    df_sunspot_raw = None\n",
    "\n",
    "# --- 3. Define Model (Ridge Only) ---\n",
    "ridge_config = (\n",
    "    'Ridge',\n",
    "    Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('model', Ridge())\n",
    "    ]),\n",
    "    {'model__alpha': Real(0.1, 100000, prior='log-uniform')}  # Fine-tuning range\n",
    ")\n",
    "model_search_spaces = [ridge_config]\n",
    "\n",
    "# --- 4. Auto-iterate Multiple Modes ---\n",
    "# Test modes: Run 3D (position), 3D_V (velocity), 6D (position+velocity)\n",
    "for dim_mode in [\"3D\", \"3D_V\", \"6D\"]:  # Test three modes\n",
    "    \n",
    "    print(f\"\\n{'#'*100}\")\n",
    "    print(f\"### [Starting {dim_mode} Mode - Test Run] ###\")\n",
    "    print(f\"{'#'*100}\")\n",
    "    \n",
    "    # Dynamically set mode parameters\n",
    "    DIMENSION_MODE = dim_mode\n",
    "    \n",
    "    # Generate directory name based on planet mode, dimension mode, and CV strategy\n",
    "    planet_mode = f\"{base_feature_count}planet\"\n",
    "    dim_mode_lower = dim_mode.lower().replace('_', '')  # 3d, 3dv, 6d\n",
    "    MASTER_OUTPUT_DIR = Path(f\"../../../results/05_p_m_a_model/p_model_feature_selection/{planet_mode}_{dim_mode_lower}_{CV_STRATEGY}\")\n",
    "    \n",
    "    # Dynamically select features to load\n",
    "    if DIMENSION_MODE == \"3D\":\n",
    "        FEATURES_TO_LOAD = ['x', 'y', 'z']\n",
    "    elif DIMENSION_MODE == \"3D_V\":\n",
    "        FEATURES_TO_LOAD = ['vx', 'vy', 'vz']  # Only use velocity\n",
    "    else:  # 6D\n",
    "        FEATURES_TO_LOAD = ['x', 'y', 'z', 'vx', 'vy', 'vz']\n",
    "    \n",
    "    print(f\"--- Mode: {DIMENSION_MODE} ({', '.join(FEATURES_TO_LOAD)}) ---\")\n",
    "    print(f\"--- Results will be saved to: {MASTER_OUTPUT_DIR} ---\")\n",
    "    \n",
    "    # --- Main Loop ---\n",
    "    if df_sunspot_raw is not None:\n",
    "        \n",
    "        try:\n",
    "            # 1. Load the complete CSV file once\n",
    "            df_raw_all = pd.read_csv(RANKING_EXCEL_FILE)\n",
    "            print(f\"Loaded ranking data from: {RANKING_EXCEL_FILE}\")\n",
    "            \n",
    "            # 2. Define target windows (we only want window 4 and 5)\n",
    "            target_windows = [4, 5]\n",
    "            \n",
    "            # 3. Verify which targets actually exist in the data\n",
    "            windows_to_process = [w for w in target_windows if w in df_raw_all['Window'].unique()]\n",
    "            \n",
    "            if not windows_to_process:\n",
    "                print(f\"Warning: Target windows {target_windows} not found in CSV.\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load CSV file '{RANKING_EXCEL_FILE}': {e}\")\n",
    "            windows_to_process = []\n",
    "        \n",
    "        # 4. Loop through the windows (Replaces the original 'for sheet_name in sheet_names_to_process:')\n",
    "        for w_val in windows_to_process:\n",
    "        \n",
    "            # Construct a virtual sheet_name to maintain compatibility with existing folder naming/regex logic\n",
    "            sheet_name = f\"window_w{w_val}\"\n",
    "        \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"--- [Main Loop {DIMENSION_MODE}] Processing Window: {w_val} (Simulated Sheet: {sheet_name}) ---\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            try:\n",
    "                # [Crucial Step] Filter data for current Window AND 'Conjunction' type\n",
    "                # The previous Excel logic had separate sheets; now we slice the big dataframe.\n",
    "                df_ranking = df_raw_all[\n",
    "                    (df_raw_all['Window'] == w_val) & \n",
    "                    (df_raw_all['Type'] == 'Conjunction')\n",
    "                ].copy()\n",
    "                \n",
    "                # [Crucial Step] Rename 'Body' to 'body_id'\n",
    "                # Code 2 expects 'body_id' (or takes the first column), but Code 1 outputs 'Body'.\n",
    "                if 'Body' in df_ranking.columns:\n",
    "                    df_ranking = df_ranking.rename(columns={'Body': 'body_id'})\n",
    "                \n",
    "                print(f\"  (Filtered {len(df_ranking)} conjunction records for window {w_val})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing window {w_val}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # [Modification Point] Only iterate through \"conjunction\" related criteria\n",
    "            sorting_standards_to_process = {\n",
    "                'Conjunction Count': SORT_CRITERIA_MAP['Conjunction Count'], \n",
    "                'Conjunction Area': SORT_CRITERIA_MAP['Conjunction Area'],\n",
    "                'Average Conjunction Area': SORT_CRITERIA_MAP['Average Conjunction Area']\n",
    "            }\n",
    "            \n",
    "            # Iterate through 3 sorting criteria\n",
    "            for run_count, (run_name, sort_key_info) in enumerate(sorting_standards_to_process.items(), 1):\n",
    "                \n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"--- [Run {run_count}/3] Sorting Criterion: [{run_name}] ---\")\n",
    "                print(f\"{'='*80}\")\n",
    "                \n",
    "                # Memory cache for survey results\n",
    "                survey_results_cache = {}\n",
    "                run_start_time = time.time()\n",
    "                \n",
    "                # Results directory\n",
    "                current_results_dir = MASTER_OUTPUT_DIR / sheet_name / run_name \n",
    "                current_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                print(f\"\\n--- [Run {DIMENSION_MODE}] Sort: [{run_name}] ---\")\n",
    "                print(f\"  Results will be saved to: {current_results_dir}\")\n",
    "                \n",
    "                # --- Dynamic Sorting ---\n",
    "                w_match = re.search(r'w(\\d+)$', sheet_name)\n",
    "                if not w_match:\n",
    "                    print(f\"  Warning: Cannot extract 'w' from worksheet name '{sheet_name}'. Skipping.\")\n",
    "                    continue\n",
    "                w_value = w_match.group(1)\n",
    "                \n",
    "                temp_combined_col = \"__temp_combined_col\"\n",
    "                df_ranking_to_sort = df_ranking.copy()\n",
    "                \n",
    "                if isinstance(sort_key_info, str):\n",
    "                    sort_column_name = sort_key_info.format(w=w_value)\n",
    "                elif isinstance(sort_key_info, tuple):\n",
    "                    col_1, col_2 = sort_key_info[0].format(w=w_value), sort_key_info[1].format(w=w_value)\n",
    "                    if col_1 not in df_ranking_to_sort.columns or col_2 not in df_ranking_to_sort.columns:\n",
    "                        print(f\"  Error: Cannot create combined column. '{col_1}' or '{col_2}' not found in '{sheet_name}'. Skipping.\")\n",
    "                        continue\n",
    "                    try:\n",
    "                        df_ranking_to_sort[temp_combined_col] = df_ranking_to_sort[col_1].fillna(0) + df_ranking_to_sort[col_2].fillna(0)\n",
    "                        sort_column_name = temp_combined_col\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error: Failed to combine columns {col_1} and {col_2}: {e}. Skipping.\")\n",
    "                        continue\n",
    "                \n",
    "                if sort_column_name not in df_ranking_to_sort.columns:\n",
    "                    print(f\"  Error: Sort column '{sort_column_name}' not found in worksheet '{sheet_name}'. Skipping.\")\n",
    "                    continue\n",
    "                        \n",
    "                try:\n",
    "                    df_ranking_sorted = df_ranking_to_sort.sort_values(by=sort_column_name, ascending=False)\n",
    "                    print(f\"  Sorted bodies by column '{sort_column_name}' (descending).\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error: Sorting by '{sort_column_name}' failed: {e}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                star_id_col = 'body_id' if 'body_id' in df_ranking_sorted.columns else df_ranking_sorted.columns[0]\n",
    "                all_ranked_stars = df_ranking_sorted[star_id_col].astype(str).tolist()\n",
    "                print(f\"  Loaded {len(all_ranked_stars)} sorted bodies from column '{star_id_col}'.\")\n",
    "                \n",
    "                # --- [Modification Point 2: 8-Planet Logic - New Code Block] ---\n",
    "                # 2.2 [Key Step]: Remove these 8 planets from candidate list to prevent duplication\n",
    "                candidate_additional_features = [\n",
    "                    star for star in all_ranked_stars \n",
    "                    if star not in BASE_PLANET_FEATURES\n",
    "                ]\n",
    "                print(f\"    Base planets: {base_feature_count}. Additional candidate bodies: {len(candidate_additional_features)}.\")\n",
    "                \n",
    "                # 2.3 Redefine survey \"additional\" star counts\n",
    "                # Original total star count (SURVEY_STAR_RANGE): [10, 15, 20, 25, 30]\n",
    "                # Additional star count (k_add) = total star count - base_feature_count\n",
    "                k_additional_survey_range = [k - base_feature_count for k in SURVEY_STAR_RANGE if k > base_feature_count] \n",
    "                \n",
    "                # Ensure k_add doesn't exceed total candidate bodies\n",
    "                k_additional_survey_range = [k for k in k_additional_survey_range if k > 0 and k <= len(candidate_additional_features)]\n",
    "                # k_additional_survey_range = [0] # [Modified] Force test only base_feature_count planets + 0 additional baseline model\n",
    "                \n",
    "                print(f\"    [Survey] Will test {len(k_additional_survey_range)} additional star count combinations (additional stars): {k_additional_survey_range}\")\n",
    "                # --- [8-Planet Logic End] ---\n",
    "\n",
    "                # --- [Phase 1: Survey] ---\n",
    "                print(f\"--- [Phase 1 Survey {DIMENSION_MODE}] Start: [{run_name}] (additional star count) ---\")\n",
    "                \n",
    "                # [Modification Point 3: Survey Loop]\n",
    "                for k_add in tqdm(k_additional_survey_range, desc=\"Survey Phase\"):\n",
    "                    \n",
    "                    total_star_count = base_feature_count + k_add\n",
    "                    final_features_list = BASE_PLANET_FEATURES + candidate_additional_features[:k_add]\n",
    "\n",
    "                    # (Original star_count > len(all_ranked_stars) check handled in k_additional_survey_range generation, simplified here)\n",
    "                    # print(f\"\\n      [Survey] Testing total_star_count={total_star_count} ({base_feature_count} planets + {k_add} additional)\")\n",
    "\n",
    "                    # Use helper function to prepare data\n",
    "                    X_full, X_historical, y_historical, _ = prepare_features_and_target(\n",
    "                        final_features_list, df_781_features_ALL, FEATURES_TO_LOAD, # <--- [Modified] \n",
    "                        df_sunspot_raw, train_start_date, train_end_date\n",
    "                    )\n",
    "                    \n",
    "                    if X_full is None or X_historical is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Use helper function to execute optimization (conservative: 50 iterations)\n",
    "                    best_score_cv, best_params, best_model = run_bayesian_optimization(\n",
    "                        X_historical, y_historical, None, model_search_spaces[0], n_iter=50  # Changed from 32 to 50\n",
    "                    )\n",
    "                    \n",
    "                    if best_score_cv is None:\n",
    "                        print(f\"          [Survey] Total star count {total_star_count} optimization failed\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Cache survey results\n",
    "                    survey_results_cache[total_star_count] = (best_score_cv, best_params, best_model) # <--- [Modified]\n",
    "                \n",
    "                # --- [Analysis and Fine-tuning Task Definition] ---\n",
    "                if not survey_results_cache:\n",
    "                    print(f\"--- [Skip] {run_name} {DIMENSION_MODE} survey produced no valid results. ---\")\n",
    "                    continue\n",
    "                \n",
    "                df_survey = pd.DataFrame.from_dict(\n",
    "                    survey_results_cache, \n",
    "                    orient='index', \n",
    "                    columns=['cv_r2', 'best_params', 'best_model']\n",
    "                )\n",
    "                df_survey.index.name = 'star_count'\n",
    "                \n",
    "                print(f\"--- [Phase 1 Summary {DIMENSION_MODE}] Survey Results: [{run_name}] ---\")\n",
    "                print(df_survey[['cv_r2']].sort_values(by='cv_r2', ascending=False).head(5))\n",
    "                \n",
    "                df_good = df_survey[df_survey['cv_r2'] > R2_THRESHOLD]\n",
    "                \n",
    "                if df_good.empty:\n",
    "                    print(f\"--- [Skip Fine-tuning] {run_name} {DIMENSION_MODE} no models found with R² > {R2_THRESHOLD}. ---\")\n",
    "                    continue\n",
    "                \n",
    "                # Strategy: Fine-tune all star count points with R² > 0.4\n",
    "                # Fine-tune within ±4 star count range around each good point\n",
    "                n_list = df_good.index.tolist()\n",
    "                \n",
    "                print(f\"  Found {len(n_list)} star count points with R² > {R2_THRESHOLD}: {n_list}\")\n",
    "                print(f\"  Corresponding R² values: {df_good['cv_r2'].tolist()}\")\n",
    "                \n",
    "                # Calculate fine-tuning range: merge ranges around all good points\n",
    "                all_dense_points = set()\n",
    "                for n in n_list:\n",
    "                    # Ensure n_min doesn't go below base_feature_count + 1\n",
    "                    n_min_allowed = base_feature_count + 1\n",
    "                    n_min = max(n_min_allowed, n - DENSE_FIT_EXPANSION) \n",
    "                    n_max = n + DENSE_FIT_EXPANSION\n",
    "                    all_dense_points.update(range(n_min, n_max + 1))\n",
    "                \n",
    "                # Convert to sorted list\n",
    "                dense_star_range = sorted(all_dense_points)\n",
    "                \n",
    "                n_min_dense = min(dense_star_range)\n",
    "                n_max_dense = max(dense_star_range)\n",
    "                \n",
    "                print(f\"--- [Phase 2 Fine-tuning {DIMENSION_MODE}] Start: [{run_name}] ---\")\n",
    "                print(f\"  Will fine-tune around {len(n_list)} good points\")\n",
    "                print(f\"  Fine-tuning range: {n_min_dense} to {n_max_dense} (total {len(dense_star_range)} star count points)\")\n",
    "                print(f\"  Specific star counts: {dense_star_range[:10]}{'...' if len(dense_star_range) > 10 else ''}\")\n",
    "                \n",
    "                # --- [Phase 2: Fine-tuning] ---\n",
    "                for star_count in tqdm(dense_star_range, desc=\"Fine-tuning Phase\"):\n",
    "                    \n",
    "                    # Check cache\n",
    "                    if star_count in survey_results_cache:\n",
    "                        best_score_cv, best_params, best_model = survey_results_cache[star_count]\n",
    "                    else:\n",
    "                        # [Modification Point 4: Fine-tuning Loop Logic]\n",
    "                        k_add = star_count - base_feature_count # <--- [Modified] Calculate additional star count\n",
    "                        \n",
    "                        if k_add <= 0:\n",
    "                            # print(f\"  [Fine-tune] Star count {star_count} <= base planet count {base_feature_count}, skipping.\")\n",
    "                            continue\n",
    "                        if k_add > len(candidate_additional_features):\n",
    "                            # print(f\"  [Fine-tune] Additional star count {k_add} > candidate count {len(candidate_additional_features)}, skipping.\")\n",
    "                            continue\n",
    "\n",
    "                        final_features_list = BASE_PLANET_FEATURES + candidate_additional_features[:k_add] # <--- [Modified]\n",
    "                        \n",
    "                        # Use helper function to prepare data\n",
    "                        X_full, X_historical, y_historical, _ = prepare_features_and_target(\n",
    "                            final_features_list, df_781_features_ALL, FEATURES_TO_LOAD, # <--- [Modified]\n",
    "                            df_sunspot_raw, train_start_date, train_end_date\n",
    "                        )\n",
    "                        \n",
    "                        if X_full is None or X_historical is None:\n",
    "                            continue\n",
    "                        \n",
    "                        # Use helper function to execute optimization (conservative: 50 iterations)\n",
    "                        best_score_cv, best_params, best_model = run_bayesian_optimization(\n",
    "                            X_historical, y_historical, None, model_search_spaces[0], n_iter=50  # Changed from 32 to 50\n",
    "                        )\n",
    "                        \n",
    "                        if best_score_cv is None:\n",
    "                            print(f\"        [Fine-tune] Star count {star_count} optimization failed\")\n",
    "                            continue\n",
    "                    \n",
    "                    # --- Save Logic ---\n",
    "                    if best_score_cv > R2_THRESHOLD:\n",
    "                        print(f\"        **** [Fine-tune-Save {DIMENSION_MODE}] Found a good model! Star count={star_count}, CV R² = {best_score_cv:.4f} ****\")\n",
    "                        \n",
    "                        try:\n",
    "                            # [Modification Point 5: Save Logic]\n",
    "                            # Re-prepare complete data for saving (including X_full)\n",
    "                            k_add = star_count - base_feature_count # <--- [Modified]\n",
    "                            if k_add <= 0: # Additional safety check\n",
    "                                print(f\"        Error: k_add <= 0 when saving, skipping\")\n",
    "                                continue\n",
    "                            \n",
    "                            final_features_list = BASE_PLANET_FEATURES + candidate_additional_features[:k_add] # <--- [Modified]\n",
    "                            \n",
    "                            X_full, X_historical, y_historical, _ = prepare_features_and_target(\n",
    "                                final_features_list, df_781_features_ALL, FEATURES_TO_LOAD, # <--- [Modified]\n",
    "                                df_sunspot_raw, train_start_date, train_end_date\n",
    "                            )\n",
    "                            \n",
    "                            if X_full is None or X_historical is None:\n",
    "                                print(f\"        Error: Cannot regenerate feature data, skipping save\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Prepare complete historical data range (for plotting and OOT testing)\n",
    "                            X_historical_raw_for_oot = X_full.loc[train_start_date:test_end_date].dropna()\n",
    "                            y_historical_raw_for_oot = df_sunspot_raw.loc[train_start_date:test_end_date]\n",
    "                            \n",
    "                            # Fix data leakage: Segmented smoothing for plotting (avoid test set info affecting train set trend)\n",
    "                            y_train_smoothed_for_plot = get_smoothed_sunspots(df_sunspot_raw.loc[train_start_date:train_end_date])\n",
    "                            y_test_smoothed_for_plot = get_smoothed_sunspots(df_sunspot_raw.loc[test_start_date:test_end_date])\n",
    "                            y_historical_smoothed_for_plot = pd.concat([y_train_smoothed_for_plot, y_test_smoothed_for_plot])\n",
    "                            \n",
    "                            # Prepare prediction data\n",
    "                            X_pred = X_full.loc[pred_start_date:pred_end_date].dropna()\n",
    "                            \n",
    "                            # Fix: Align prediction data with training feature columns\n",
    "                            if not X_pred.empty:\n",
    "                                train_cols = X_historical.columns\n",
    "                                # Only keep columns present during training, fill missing columns with 0\n",
    "                                X_pred = X_pred.reindex(columns=train_cols, fill_value=0)\n",
    "                            \n",
    "                            # OOT out-of-sample testing\n",
    "                            train_end_for_oot = '1986-09-01'\n",
    "                            test_start_for_oot = '1986-09-02'\n",
    "                            \n",
    "                            oot_r2_score = np.nan\n",
    "                            train_r2_score_vs_smooth = np.nan\n",
    "                            train_r2_score_vs_raw = np.nan\n",
    "                            test_r2_score_vs_raw = np.nan\n",
    "                            \n",
    "                            if test_start_for_oot in X_historical_raw_for_oot.index and train_end_for_oot in X_historical_raw_for_oot.index:\n",
    "                                X_train_raw = X_historical_raw_for_oot.loc[:train_end_for_oot]\n",
    "                                X_test_raw = X_historical_raw_for_oot.loc[test_start_for_oot:]\n",
    "                                \n",
    "                                # Fix data leakage: Smooth train and test sets separately to avoid test info leaking to train\n",
    "                                y_train_oot_raw = y_historical_raw_for_oot.loc[:train_end_for_oot]\n",
    "                                y_train_oot = get_smoothed_sunspots(y_train_oot_raw)  # Smooth using only train data\n",
    "                                \n",
    "                                y_test_oot_raw = y_historical_raw_for_oot.loc[test_start_for_oot:]\n",
    "                                y_test_oot = get_smoothed_sunspots(y_test_oot_raw)  # Smooth using only test data\n",
    "                                \n",
    "                                aligned_train_index = X_train_raw.index.intersection(y_train_oot.index)\n",
    "                                X_train_raw = X_train_raw.loc[aligned_train_index]\n",
    "                                y_train_oot = y_train_oot.loc[aligned_train_index]\n",
    "                                \n",
    "                                aligned_test_index = X_test_raw.index.intersection(y_test_oot.index)\n",
    "                                X_test_raw = X_test_raw.loc[aligned_test_index]\n",
    "                                y_test_oot = y_test_oot.loc[aligned_test_index]\n",
    "                                \n",
    "                                model_step_name = best_model.steps[-1][0]\n",
    "                                preprocessor_steps = [(name, clone(step)) for name, step in best_model.steps if name != model_step_name]\n",
    "                                \n",
    "                                if preprocessor_steps:\n",
    "                                    X_preprocessor = Pipeline(preprocessor_steps)\n",
    "                                    X_train_oot = X_preprocessor.fit_transform(X_train_raw)\n",
    "                                    X_test_oot = X_preprocessor.transform(X_test_raw)\n",
    "                                    final_model = clone(best_model.named_steps[model_step_name])\n",
    "                                else:\n",
    "                                    X_train_oot = X_train_raw.values\n",
    "                                    X_test_oot = X_test_raw.values\n",
    "                                    final_model = clone(best_model.named_steps[model_step_name])\n",
    "                        \n",
    "                                if X_train_oot.shape[0] > 0 and X_test_oot.shape[0] > 0:\n",
    "                                    final_model.fit(X_train_oot, y_train_oot)\n",
    "                                    y_pred_oot = final_model.predict(X_test_oot)\n",
    "                                    oot_r2_score = r2_score(y_test_oot, y_pred_oot)\n",
    "                                    y_pred_train_oot = final_model.predict(X_train_oot)\n",
    "                                    train_r2_score_vs_smooth = r2_score(y_train_oot, y_pred_train_oot)\n",
    "                                    \n",
    "                                    raw_ssn_train_oot = y_historical_raw_for_oot.loc[y_train_oot.index]\n",
    "                                    raw_ssn_test_oot = y_historical_raw_for_oot.loc[y_test_oot.index]\n",
    "                                    train_r2_score_vs_raw = r2_score(raw_ssn_train_oot, y_pred_train_oot)\n",
    "                                    test_r2_score_vs_raw = r2_score(raw_ssn_test_oot, y_pred_oot)\n",
    "                            \n",
    "                            # Save model\n",
    "                            params_list = [f\"{key.split('__')[-1]}_{value:.4f}\" if isinstance(value, float) else f\"{key.split('__')[-1]}_{value}\" for key, value in best_params.items()]\n",
    "                            params_str_for_filename = \"_\".join(params_list)\n",
    "                            \n",
    "                            # Handle OOT R² NaN cases\n",
    "                            oot_r2_str = f\"{oot_r2_score:.4f}\" if not np.isnan(oot_r2_score) else \"N/A\"\n",
    "                            # [!! New Code !!] Create string for \"raw R2\" (test_r2_score_vs_raw)\n",
    "                            oot_raw_r2_str = f\"{test_r2_score_vs_raw:.4f}\" if not np.isnan(test_r2_score_vs_raw) else \"N/A\"\n",
    "                            # [!! Modified Code !!] Write both OOT R² to filename\n",
    "                            filename_base = f\"{star_count}stars_{model_search_spaces[0][0]}_CV-R2_{best_score_cv:.4f}_OOT-SMOOTH-R2_{oot_r2_str}_OOT-RAW-R2_{oot_raw_r2_str}_Params_{params_str_for_filename}\"\n",
    "                            \n",
    "                            model_save_path = current_results_dir / f\"{filename_base}.joblib\"\n",
    "                            excel_path = current_results_dir / f\"{filename_base}.xlsx\"\n",
    "                            plot_path = current_results_dir / f\"{filename_base}.png\"\n",
    "                            \n",
    "                            objects_to_save = {\n",
    "                                'model_pipeline': best_model,\n",
    "                                'features': X_full.columns.tolist(), # <--- [Modified] Ensure this is X_full columns\n",
    "                                'best_params': best_params,\n",
    "                                'cv_r2_score': best_score_cv,\n",
    "                                'oot_r2_score': oot_r2_score if not np.isnan(oot_r2_score) else None,\n",
    "                                'dimension_mode': DIMENSION_MODE,\n",
    "                                'features_loaded': FEATURES_TO_LOAD,\n",
    "                                'star_count': star_count, # star_count is still total (base_feature_count+k_add)\n",
    "                                'sheet_name': sheet_name,\n",
    "                                'run_name': run_name\n",
    "                            }\n",
    "                            joblib.dump(objects_to_save, model_save_path)\n",
    "                            print(f\"        Model saved to: {model_save_path}\")\n",
    "                        \n",
    "                            # Evaluation and plotting - Align features and target variables\n",
    "                            # Use aligned historical range data\n",
    "                            Xy_historical_aligned = X_historical_raw_for_oot.join(y_historical_smoothed_for_plot.rename('SSN')).dropna()\n",
    "                            X_historical_for_fit = Xy_historical_aligned.drop('SSN', axis=1)\n",
    "                            y_historical_for_fit = Xy_historical_aligned['SSN']\n",
    "                            \n",
    "                            # Ensure column alignment\n",
    "                            train_cols = X_historical.columns\n",
    "                            X_historical_for_fit = X_historical_for_fit.reindex(columns=train_cols, fill_value=0)\n",
    "                            \n",
    "                            y_fit_historical = pd.Series(best_model.predict(X_historical_for_fit), index=y_historical_for_fit.index)\n",
    "                            \n",
    "                            # Check if X_pred is empty\n",
    "                            if X_pred.empty:\n",
    "                                print(f\"        Warning: Prediction data is empty, skipping future prediction part\")\n",
    "                                y_fit_pred = pd.Series(dtype=float)\n",
    "                                full_fit = y_fit_historical\n",
    "                            else:\n",
    "                                y_fit_pred = pd.Series(best_model.predict(X_pred), index=X_pred.index)\n",
    "                                full_fit = pd.concat([y_fit_historical, y_fit_pred])\n",
    "                            \n",
    "                            # Assume find_peaks_valleys and calculate_deviation defined in Cell 1\n",
    "                            model_peaks, model_valleys = find_peaks_valleys(y_fit_historical)\n",
    "                            df_peak_dev = calculate_deviation(model_peaks, known_peaks)\n",
    "                            df_valley_dev = calculate_deviation(model_valleys, known_valleys)\n",
    "                            \n",
    "                            peak_phase_dev_mean = df_peak_dev['Phase Deviation (days)'].abs().mean() if not df_peak_dev.empty else np.nan\n",
    "                            peak_amp_dev_mean = df_peak_dev['Amplitude Deviation'].abs().mean() if not df_peak_dev.empty else np.nan\n",
    "                            valley_phase_dev_mean = df_valley_dev['Phase Deviation (days)'].abs().mean() if not df_valley_dev.empty else np.nan\n",
    "                            valley_amp_dev_mean = df_valley_dev['Amplitude Deviation'].abs().mean() if not df_valley_dev.empty else np.nan\n",
    "                            \n",
    "                            summary_data = [] \n",
    "                            if not np.isnan(oot_r2_score):\n",
    "                                summary_data.append({'Dataset': 'Out-of-Sample Test (30yr)', 'R2_vs_Smoothed': oot_r2_score, 'R2_vs_Raw': test_r2_score_vs_raw})\n",
    "                                summary_data.append({'Dataset': 'Corresponding Train Set (130yr)', 'R2_vs_Smoothed': train_r2_score_vs_smooth, 'R2_vs_Raw': train_r2_score_vs_raw})\n",
    "                            \n",
    "                            r2_historical_vs_smooth = r2_score(y_historical_for_fit, y_fit_historical)\n",
    "                            r2_historical_vs_raw = r2_score(df_sunspot_raw.loc[y_historical_for_fit.index], y_fit_historical)\n",
    "                            \n",
    "                            summary_data.append({\n",
    "                                'Dataset': 'Historical Data Combined Fit', 'R2_vs_Smoothed': r2_historical_vs_smooth,\n",
    "                                'R2_vs_Raw': r2_historical_vs_raw, 'CV_R2': best_score_cv,\n",
    "                                'Avg_Peak_Phase_Dev': peak_phase_dev_mean, 'Avg_Peak_Amplitude_Dev': peak_amp_dev_mean, \n",
    "                                'Avg_Valley_Phase_Dev': valley_phase_dev_mean, 'Avg_Valley_Amplitude_Dev': valley_amp_dev_mean\n",
    "                            })\n",
    "                            df_summary = pd.DataFrame(summary_data)\n",
    "                            \n",
    "                            # Find future peaks (only when prediction data exists)\n",
    "                            if not y_fit_pred.empty:\n",
    "                                model_future_peaks, model_future_valleys = find_peaks_valleys(y_fit_pred)\n",
    "                                next_peak = model_future_peaks.head(1)\n",
    "                            else:\n",
    "                                next_peak = pd.DataFrame()\n",
    "                            \n",
    "                            next_peak_info = {'date': next_peak.index[0].strftime('%Y-%m-%d') if not next_peak.empty else 'N/A', \n",
    "                                              'ssn': next_peak['SSN'].iloc[0] if not next_peak.empty else 'N/A'}\n",
    "                            \n",
    "                            # Assume plot_results defined in Cell 1\n",
    "                            plot_results(plot_path, df_sunspot_raw, y_historical_smoothed_for_plot, full_fit, \n",
    "                                         (train_start_date, test_end_date), (train_start_date, test_end_date), \n",
    "                                         (pred_start_date, pred_end_date), df_solar_cycle, next_peak_info, \n",
    "                                         {'r2': best_score_cv, 'oot_r2': oot_r2_score if not np.isnan(oot_r2_score) else np.nan}, best_params)\n",
    "                            \n",
    "                            # Write Excel\n",
    "                            with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
    "                                df_summary.to_excel(writer, sheet_name='Model Evaluation Summary', index=False)\n",
    "                                df_peak_dev.to_excel(writer, sheet_name='Peak Deviation_Historical', index=False)\n",
    "                                df_valley_dev.to_excel(writer, sheet_name='Valley Deviation_Historical', index=False)\n",
    "                                \n",
    "                                # Only save future peaks when prediction data exists\n",
    "                                if not y_fit_pred.empty:\n",
    "                                    model_future_peaks.to_excel(writer, sheet_name='Predicted Peaks_Future')\n",
    "                                    model_future_valleys.to_excel(writer, sheet_name='Predicted Valleys_Future')\n",
    "                                \n",
    "                                if model_search_spaces[0][0] in ['Ridge']:\n",
    "                                    model_coeffs = best_model.named_steps['model'].coef_\n",
    "                                    df_coeffs = pd.DataFrame({'Feature': X_historical_for_fit.columns, 'Coefficient': model_coeffs}).sort_values(by='Coefficient', key=abs, ascending=False)\n",
    "                                    df_coeffs.to_excel(writer, sheet_name='Model Coefficients', index=False)\n",
    "                                    \n",
    "                                worksheet_img = writer.book.add_worksheet('Fit Quality Plot')\n",
    "                                worksheet_img.insert_image('A1', str(plot_path))\n",
    "                        \n",
    "                            print(f\"        Excel report saved to: {excel_path}\")\n",
    "                        \n",
    "                        except Exception as save_error:\n",
    "                            print(f\"        Error saving model/results: {save_error}\")\n",
    "                            import traceback\n",
    "                            traceback.print_exc()\n",
    "                \n",
    "                run_end_time = time.time()\n",
    "                print(f\"--- [Complete {DIMENSION_MODE}] {run_name} (Total time: {(run_end_time - run_start_time)/60:.2f} minutes) ---\")\n",
    "        \n",
    "        print(f\"\\n--- {DIMENSION_MODE} Mode Run Complete ---\")\n",
    "    \n",
    "print(\"\\n--- [!!! ALL COMPLETE !!!] 3D/3D_V/6D Three Modes - All Worksheets - All Sorting Criteria Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d182824-5f7d-4c23-bfcd-d88f57cced7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ecb1a2-7613-411b-b2f9-0120f36be043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Summarize All Result Files Information (Updated to sort by OOT-RAW-R2)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np # Ensure numpy is imported\n",
    "\n",
    "print(\"Starting to scan result files...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Auto-scan all subdirectories under p_model_feature_selection directory\n",
    "base_result_dir = Path('../../../results/05_p_m_a_model/p_model_feature_selection')\n",
    "result_dirs = []\n",
    "\n",
    "if base_result_dir.exists():\n",
    "    # Find all subdirectories matching naming convention (e.g., 0planet_3d_cycle, 8planet_6d_year)\n",
    "    for subdir in base_result_dir.iterdir():\n",
    "        if subdir.is_dir() and not subdir.name.startswith('.'):\n",
    "            result_dirs.append(str(subdir))\n",
    "    print(f\"✓ Found {len(result_dirs)} result directories under {base_result_dir}\")\n",
    "else:\n",
    "    print(f\"⚠️  Base directory does not exist: {base_result_dir}\")\n",
    "\n",
    "# Store all file information\n",
    "all_files_info = []\n",
    "\n",
    "# Iterate through all result directories\n",
    "for result_dir_path in result_dirs:\n",
    "    result_dir = Path(result_dir_path)\n",
    "    \n",
    "    if not result_dir.exists():\n",
    "        print(f\"⚠️  Directory does not exist, skipping: {result_dir}\")\n",
    "        continue\n",
    "    \n",
    "    # Parse configuration info from directory name (e.g., 0planet_3d_cycle)\n",
    "    dir_name = result_dir.name\n",
    "    config_parts = dir_name.split('_')\n",
    "    \n",
    "    if len(config_parts) >= 3:\n",
    "        planet_mode = config_parts[0]  # 0planet, 8planet, etc.\n",
    "        dimension_mode = config_parts[1].upper()  # 3D, 3DV, 6D\n",
    "        cv_strategy = config_parts[2]  # cycle, year\n",
    "    else:\n",
    "        planet_mode = dir_name\n",
    "        dimension_mode = 'unknown'\n",
    "        cv_strategy = 'unknown'\n",
    "    \n",
    "    # Find all PNG files\n",
    "    png_files = list(result_dir.rglob('*.png'))\n",
    "    print(f\"\\n✓ {result_dir.name}: Found {len(png_files)} PNG files\")\n",
    "    \n",
    "    for png_file in png_files:\n",
    "        # Get relative path (from result directory)\n",
    "        rel_path = png_file.relative_to(result_dir)\n",
    "        path_parts = rel_path.parts\n",
    "        \n",
    "        # Extract path information\n",
    "        if len(path_parts) >= 3:\n",
    "            sheet_name = path_parts[0]  # e.g., window_w4\n",
    "            sort_criterion = path_parts[1]  # e.g., Conjunction Area\n",
    "        else:\n",
    "            sheet_name = path_parts[0] if len(path_parts) >= 1 else ''\n",
    "            sort_criterion = path_parts[1] if len(path_parts) >= 2 else ''\n",
    "        \n",
    "        # Parse filename\n",
    "        filename = png_file.stem  # without extension\n",
    "        \n",
    "        # Update regex to match new filename format\n",
    "        # Format: 12stars_Ridge_CV-R2_0.4693_OOT-SMOOTH-R2_-0.0128_OOT-RAW-R2_0.2291_Params_alpha_167.5751\n",
    "        match = re.match(\n",
    "            r'(\\d+)stars_Ridge_CV-R2_([-\\d.]+)_OOT-SMOOTH-R2_((?:[-\\d.]+|N/A))_OOT-RAW-R2_((?:[-\\d.]+|N/A))_Params_alpha_([-\\d.]+)', \n",
    "            filename\n",
    "        )\n",
    "        \n",
    "        if match:\n",
    "            num_stars = int(match.group(1))\n",
    "            cv_r2 = float(match.group(2))\n",
    "            \n",
    "            # Extract OOT (vs smoothed) R2\n",
    "            oot_r2_smooth_str = match.group(3)\n",
    "            oot_r2_smooth = float(oot_r2_smooth_str) if oot_r2_smooth_str != 'N/A' else np.nan\n",
    "            \n",
    "            # Extract OOT (vs raw) R2\n",
    "            oot_r2_raw_str = match.group(4)\n",
    "            oot_r2_raw = float(oot_r2_raw_str) if oot_r2_raw_str != 'N/A' else np.nan\n",
    "            \n",
    "            alpha = float(match.group(5))\n",
    "            \n",
    "            # Extract w value (if present)\n",
    "            w_match = re.search(r'w(\\d+)$', sheet_name)\n",
    "            w_value = int(w_match.group(1)) if w_match else None\n",
    "            \n",
    "            # Store information\n",
    "            file_info = {\n",
    "                'Planet_Mode': planet_mode,\n",
    "                'Dimension_Mode': dimension_mode,\n",
    "                'CV_Strategy': cv_strategy,\n",
    "                'Sheet_Name': sheet_name,\n",
    "                'w_Value': w_value,\n",
    "                'Sort_Criterion': sort_criterion,\n",
    "                'Star_Count': num_stars,\n",
    "                'CV_R2': cv_r2,\n",
    "                'OOT_SMOOTH_R2': oot_r2_smooth, # (vs smoothed values)\n",
    "                'OOT_RAW_R2': oot_r2_raw,       # (vs raw values - gold standard)\n",
    "                'alpha': alpha,\n",
    "                'Full_Filename': png_file.name,\n",
    "                'Relative_Path': str(rel_path),\n",
    "                'Full_Path': str(png_file.absolute())\n",
    "            }\n",
    "            \n",
    "            all_files_info.append(file_info)\n",
    "        else:\n",
    "            # Also record if regex matching fails\n",
    "            file_info = {\n",
    "                'Planet_Mode': planet_mode,\n",
    "                'Dimension_Mode': dimension_mode,\n",
    "                'CV_Strategy': cv_strategy,\n",
    "                'Sheet_Name': sheet_name,\n",
    "                'w_Value': None,\n",
    "                'Sort_Criterion': sort_criterion,\n",
    "                'Star_Count': None,\n",
    "                'CV_R2': None,\n",
    "                'OOT_SMOOTH_R2': None,\n",
    "                'OOT_RAW_R2': None,\n",
    "                'alpha': None,\n",
    "                'Full_Filename': png_file.name,\n",
    "                'Relative_Path': str(rel_path),\n",
    "                'Full_Path': str(png_file.absolute())\n",
    "            }\n",
    "            all_files_info.append(file_info)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"✓ Collected total of {len(all_files_info)} result files\\n\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pd.DataFrame(all_files_info)\n",
    "\n",
    "# Sort by OOT_RAW_R2 (out-of-sample test R² vs raw values) descending\n",
    "df_results_sorted = df_results.sort_values(by='OOT_RAW_R2', ascending=False)\n",
    "\n",
    "# Save to CSV\n",
    "output_csv = './results_file_summary.csv'\n",
    "df_results_sorted.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "print(f\"✓ Saved to: {output_csv}\\n\")\n",
    "\n",
    "# Display statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"📊 Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"By Planet Mode:\")\n",
    "print(df_results_sorted['Planet_Mode'].value_counts().to_string())\n",
    "print()\n",
    "\n",
    "print(f\"By Dimension Mode:\")\n",
    "print(df_results_sorted['Dimension_Mode'].value_counts().to_string())\n",
    "print()\n",
    "\n",
    "print(f\"By CV Strategy:\")\n",
    "print(df_results_sorted['CV_Strategy'].value_counts().to_string())\n",
    "print()\n",
    "\n",
    "if 'Sort_Criterion' in df_results_sorted.columns:\n",
    "    print(f\"By Sort Criterion:\")\n",
    "    print(df_results_sorted['Sort_Criterion'].value_counts().to_string())\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "# Print best results sorted by OOT_RAW_R2\n",
    "print(\"\\n🏆🏆🏆 Best Models Sorted by OOT_RAW_R2 (Top 10): 🏆🏆🏆\")\n",
    "preview_cols = ['Planet_Mode', 'Dimension_Mode', 'CV_Strategy', 'Sheet_Name', 'Sort_Criterion', 'Star_Count', 'CV_R2', 'OOT_RAW_R2', 'OOT_SMOOTH_R2', 'alpha']\n",
    "print(df_results_sorted[preview_cols].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ Complete!\")\n",
    "print(\"\\n💡 Tips:\")\n",
    "print(\"  • Open 'results_file_summary.csv' to view all results\")\n",
    "print(f\"  • File is sorted by 'OOT_RAW_R2' (raw R2), #1 is the best model\")\n",
    "print(\"  • Filter by Planet_Mode, Dimension_Mode, CV_Strategy, Sort_Criterion, Star_Count, etc.\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
