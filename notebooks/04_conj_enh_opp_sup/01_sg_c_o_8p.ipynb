{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807dd6ff-7c4a-4d80-bed3-a30d6fcfd45e",
   "metadata": {},
   "source": [
    "## Step 0: Global Configuration and Switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a27ca8-53e3-4394-8e7f-edac0855688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Mode] Production Mode\n",
      "    - Algo 1/2 Simulation Count: 10000\n",
      "    - Algo 3 Simulation Count: 1000\n",
      "    - Core Count: 30\n",
      "--- Configuration Loaded ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import importlib\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import scipy.stats as stats  # For fast P-value calculation\n",
    "\n",
    "# Try to import external worker, ignore if not found (inline logic provided below)\n",
    "try:\n",
    "    import algo_workers\n",
    "    importlib.reload(algo_workers)\n",
    "except ImportError:\n",
    "    algo_workers = None\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Global Configuration and Switches\n",
    "# =============================================================================\n",
    "\n",
    "# [Core Switch] True = Test Mode (Fast, 100 simulations); False = Production Mode (Rigorous, 1000/10000 simulations)\n",
    "TEST_MODE = False \n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\">>> [Mode] Test Mode\")\n",
    "    print(\"    - Algo 1/2 Simulation Count: 100\")\n",
    "    print(\"    - Algo 3 Simulation Count: 100 (or use binomial distribution)\")\n",
    "    print(\"    - Core Count: 1 (avoid debugging deadlocks)\")\n",
    "    N_SIM_ALGO12 = 100\n",
    "    N_SIM_ALGO3 = 100\n",
    "    N_WORKERS = 1\n",
    "else:\n",
    "    print(\">>> [Mode] Production Mode\")\n",
    "    print(\"    - Algo 1/2 Simulation Count: 10000\")\n",
    "    print(\"    - Algo 3 Simulation Count: 1000\")\n",
    "    # Leave 2 cores for the system to prevent freezing\n",
    "    N_WORKERS = max(1, cpu_count() - 2)\n",
    "    N_SIM_ALGO12 = 10000\n",
    "    N_SIM_ALGO3 = 1000\n",
    "    print(f\"    - Core Count: {N_WORKERS}\")\n",
    "\n",
    "# Path Configuration\n",
    "BASE_INPUT_DIR = '../../data/ready'\n",
    "BASE_OUTPUT_DIR = '../../results/04_conj_enh_opp_sup/sg'\n",
    "CACHE_DIR = os.path.join(BASE_OUTPUT_DIR, 'cache_data')\n",
    "\n",
    "# Original large file path (contains 781 bodies)\n",
    "EPHEMERIS_FILE = os.path.join(BASE_INPUT_DIR, '781_planets_dwarfs_asteroids_lonlat.parquet')\n",
    "\n",
    "# Output File Paths\n",
    "OUTPUT_FILE_ALGO1 = os.path.join(BASE_OUTPUT_DIR, 'sg_algo1_total_pairs.csv')\n",
    "OUTPUT_FILE_ALGO2 = os.path.join(BASE_OUTPUT_DIR, 'sg_algo2_at_least_one.csv')\n",
    "OUTPUT_FILE_ALGO3 = os.path.join(BASE_OUTPUT_DIR, 'sg_algo3_single_body_781.csv')\n",
    "OUTPUT_FILE_KUIPER = os.path.join(BASE_OUTPUT_DIR, 'sg_algo_kuiper_test.csv')\n",
    "\n",
    "# 8 Major Planets column names (for Algo 1 & 2)\n",
    "PLANET_COLS = ['199_lon', '299_lon', '399_lon', '499_lon', '599_lon', '699_lon', '799_lon', '899_lon']\n",
    "THRESHOLDS = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists(BASE_OUTPUT_DIR): os.makedirs(BASE_OUTPUT_DIR)\n",
    "if not os.path.exists(CACHE_DIR): os.makedirs(CACHE_DIR)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 2. General Utility Functions\n",
    "# =============================================================================\n",
    "\n",
    "def categorize_area(area):\n",
    "    \"\"\"Sunspot area categorization\"\"\"\n",
    "    if area < 100: return 'Small <100'\n",
    "    elif 100 <= area < 500: return 'Medium 100-500'\n",
    "    elif 500 <= area < 2000: return 'Large 500-2000'\n",
    "    else: return 'XLarge >2000'\n",
    "\n",
    "def interpolate_angle(angle1, angle2, fraction):\n",
    "    \"\"\"Vectorized angle interpolation (handles 0-360 wrap-around)\"\"\"\n",
    "    rad1 = np.deg2rad(angle1)\n",
    "    rad2 = np.deg2rad(angle2)\n",
    "    # Calculate shortest path difference\n",
    "    delta = (rad2 - rad1 + np.pi) % (2 * np.pi) - np.pi\n",
    "    interpolated_rad = rad1 + fraction * delta\n",
    "    return np.degrees(interpolated_rad) % 360.0\n",
    "\n",
    "print(\"--- Configuration Loaded ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d5b7c-6018-48ab-8f18-955f3f92fb3e",
   "metadata": {},
   "source": [
    "## Step 1: Optimized Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce11acd-c728-4df0-be90-828318be91bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 1: Data Pre-processing (Full Align + Lookup)\n",
      "============================================================\n",
      "Loading FULL Ephemeris from: ../../data/ready/781_planets_dwarfs_asteroids_lonlat.parquet ...\n",
      "Loaded Ephemeris: 73780 days, 781 bodies.\n",
      "Calculating Kepler probability maps (for 8 planets)...\n",
      "\n",
      "Processing 'All' dataset (Heavy Interpolation)...\n",
      "  [Info] Dropped 35 duplicate records from All dataset.\n",
      "  Interpolating 256824 records against 781 bodies...\n",
      "  Saved FULL aligned data to: ../../results/04_conj_enh_opp_sup/sg/cache_data/ready_All.parquet\n",
      "\n",
      "Processing subset files (Lookup method)...\n",
      "  Extracting Stage: Daily from All data...\n",
      "    Saved 8301 records to ../../results/04_conj_enh_opp_sup/sg/cache_data/ready_Daily.parquet\n",
      "  Extracting Stage: Dissipation from All data...\n",
      "    Saved 27870 records to ../../results/04_conj_enh_opp_sup/sg/cache_data/ready_Dissipation.parquet\n",
      "  Extracting Stage: Duration from All data...\n",
      "    Saved 75678 records to ../../results/04_conj_enh_opp_sup/sg/cache_data/ready_Duration.parquet\n",
      "  Extracting Stage: Onset from All data...\n",
      "    Saved 33278 records to ../../results/04_conj_enh_opp_sup/sg/cache_data/ready_Onset.parquet\n",
      "Step 1 Completed. Total Time: 11.8s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Optimized Data Pre-processing\n",
    "# Strategy: Load full ephemeris -> Interpolate 'All' stage (deduplication) -> Lookup for other stages -> Save cache\n",
    "# =============================================================================\n",
    "\n",
    "def step1_data_preparation_optimized():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Step 1: Data Pre-processing (Full Align + Lookup)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- 1. Load Full Ephemeris (all 781 bodies) ---\n",
    "    print(f\"Loading FULL Ephemeris from: {EPHEMERIS_FILE} ...\")\n",
    "    if not os.path.exists(EPHEMERIS_FILE):\n",
    "        print(\"Error: Ephemeris file not found!\")\n",
    "        return\n",
    "\n",
    "    df_ephem = pd.read_parquet(EPHEMERIS_FILE)\n",
    "    if 'date' in df_ephem.columns:\n",
    "        df_ephem['date'] = pd.to_datetime(df_ephem['date'])\n",
    "        df_ephem.set_index('date', inplace=True)\n",
    "    \n",
    "    # Normalize index (daily 00:00)\n",
    "    df_ephem.index = df_ephem.index.normalize()\n",
    "    df_ephem = df_ephem[~df_ephem.index.duplicated(keep='first')]\n",
    "    df_ephem.sort_index(inplace=True)\n",
    "    \n",
    "    # Identify all longitude columns (assuming ending with _lon)\n",
    "    all_body_cols = [c for c in df_ephem.columns if str(c).endswith('_lon')]\n",
    "    print(f\"Loaded Ephemeris: {len(df_ephem)} days, {len(all_body_cols)} bodies.\")\n",
    "    \n",
    "    # Extract daily 00:00 data matrix (for interpolation)\n",
    "    # Use float32 to save memory\n",
    "    ephem_matrix_all = df_ephem[all_body_cols].values.astype(np.float32)\n",
    "    \n",
    "    # [Key] Save daily data for 8 major planets for Algo 1/2 CTS background simulation\n",
    "    path_matrix_8p = os.path.join(CACHE_DIR, 'ephem_matrix_8p.npy')\n",
    "    np.save(path_matrix_8p, df_ephem[PLANET_COLS].values.astype(np.float64))\n",
    "    \n",
    "    # [Key] Calculate Kepler Maps (only for 8 major planets, required by Algo 1/2)\n",
    "    print(\"Calculating Kepler probability maps (for 8 planets)...\")\n",
    "    prob_maps = {}\n",
    "    for col in PLANET_COLS:\n",
    "        hist, _ = np.histogram(df_ephem[col], bins=360, range=(0, 360), density=True)\n",
    "        prob_maps[col] = hist\n",
    "    with open(os.path.join(CACHE_DIR, 'kepler_prob_maps.pkl'), 'wb') as f:\n",
    "        pickle.dump(prob_maps, f)\n",
    "\n",
    "    # --- 2. Process 'All' file (core interpolation) ---\n",
    "    print(\"\\nProcessing 'All' dataset (Heavy Interpolation)...\")\n",
    "    file_all = 'sg_1874_2025_all.csv'\n",
    "    path_all = os.path.join(BASE_INPUT_DIR, file_all)\n",
    "    \n",
    "    if not os.path.exists(path_all):\n",
    "        print(f\"Error: {file_all} not found!\")\n",
    "        return\n",
    "\n",
    "    df_sun_all = pd.read_csv(path_all, usecols=['date', 'hme_lon', 'area'])\n",
    "    df_sun_all['date'] = pd.to_datetime(df_sun_all['date'])\n",
    "    df_sun_all.dropna(subset=['hme_lon', 'area'], inplace=True)\n",
    "    \n",
    "    # [New] Core deduplication logic: based on test results, clear duplicates in 'All' to prevent data bloat\n",
    "    initial_len = len(df_sun_all)\n",
    "    df_sun_all.drop_duplicates(subset=['date', 'hme_lon', 'area'], inplace=True)\n",
    "    dropped_len = initial_len - len(df_sun_all)\n",
    "    if dropped_len > 0:\n",
    "        print(f\"  [Info] Dropped {dropped_len} duplicate records from All dataset.\")\n",
    "\n",
    "    # Filter time range\n",
    "    min_date, max_date = df_ephem.index.min(), df_ephem.index.max()\n",
    "    df_sun_all = df_sun_all[(df_sun_all['date'] >= min_date) & (df_sun_all['date'] <= max_date)].copy()\n",
    "    \n",
    "    # Interpolation calculation\n",
    "    print(f\"  Interpolating {len(df_sun_all)} records against {len(all_body_cols)} bodies...\")\n",
    "    \n",
    "    day_t = df_sun_all['date'].dt.normalize()\n",
    "    # Calculate time fraction\n",
    "    fraction = (df_sun_all['date'] - day_t).dt.total_seconds() / 86400.0\n",
    "    # Find index\n",
    "    idx_t = df_ephem.index.searchsorted(day_t)\n",
    "    \n",
    "    # Boundary protection\n",
    "    valid_mask = (idx_t < len(df_ephem) - 1)\n",
    "    if not valid_mask.all():\n",
    "        df_sun_all = df_sun_all[valid_mask]\n",
    "        fraction = fraction[valid_mask]\n",
    "        idx_t = idx_t[valid_mask]\n",
    "        \n",
    "    pos_t = ephem_matrix_all[idx_t]\n",
    "    pos_t_plus_1 = ephem_matrix_all[idx_t + 1]\n",
    "    fraction_vals = fraction.values[:, np.newaxis].astype(np.float32)\n",
    "    \n",
    "    # Vectorized interpolation\n",
    "    interpolated_matrix = interpolate_angle(pos_t, pos_t_plus_1, fraction_vals)\n",
    "    \n",
    "    # Construct result DataFrame\n",
    "    df_result_all = df_sun_all.copy()\n",
    "    df_result_all['ephem_idx_daily'] = idx_t # Save index for CTS use\n",
    "    df_result_all['Group'] = df_result_all['area'].apply(categorize_area)\n",
    "    \n",
    "    # Concatenate body data (this step may consume memory, can be chunked if needed)\n",
    "    df_bodies = pd.DataFrame(interpolated_matrix, columns=all_body_cols, index=df_result_all.index)\n",
    "    df_final_all = pd.concat([df_result_all, df_bodies], axis=1)\n",
    "    \n",
    "    # Save 'Ready' data for 'All'\n",
    "    save_path_all = os.path.join(CACHE_DIR, 'ready_All.parquet')\n",
    "    df_final_all.to_parquet(save_path_all)\n",
    "    print(f\"  Saved FULL aligned data to: {save_path_all}\")\n",
    "    \n",
    "    # --- 3. Process other files (lookup method) ---\n",
    "    other_files = {\n",
    "        'sg_1874_2025_daily.csv': 'Daily',\n",
    "        'sg_1874_2025_diss.csv': 'Dissipation',\n",
    "        'sg_1874_2025_dur.csv': 'Duration',\n",
    "        'sg_1874_2025_onset.csv': 'Onset'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nProcessing subset files (Lookup method)...\")\n",
    "    for fname, stage_name in other_files.items():\n",
    "        fpath = os.path.join(BASE_INPUT_DIR, fname)\n",
    "        if not os.path.exists(fpath): continue\n",
    "        \n",
    "        print(f\"  Extracting Stage: {stage_name} from All data...\")\n",
    "        df_sub = pd.read_csv(fpath, usecols=['date', 'hme_lon', 'area'])\n",
    "        df_sub['date'] = pd.to_datetime(df_sub['date'])\n",
    "        \n",
    "        # Inner Join using pre-calculated 'All' data\n",
    "        # Since 'All' is deduplicated, this is a lookup based on unique keys\n",
    "        df_merged = pd.merge(\n",
    "            df_sub, \n",
    "            df_final_all, \n",
    "            on=['date', 'hme_lon', 'area'], \n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        save_path = os.path.join(CACHE_DIR, f'ready_{stage_name}.parquet')\n",
    "        df_merged.to_parquet(save_path)\n",
    "        print(f\"    Saved {len(df_merged)} records to {save_path}\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del df_ephem, ephem_matrix_all, df_final_all, df_bodies\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Step 1 Completed. Total Time: {time.time() - start_time:.1f}s\")\n",
    "\n",
    "# Execute Step 1\n",
    "if __name__ == '__main__':\n",
    "    step1_data_preparation_optimized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ca859-bbd9-4343-8858-c898ab67972e",
   "metadata": {},
   "source": [
    "## Step 2: Algorithm 1 (Total Pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d247b08e-1dcc-4de8-ae08-0b02b7ffb542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 2: Running Algo 1 | Sim: 10000 | Workers: 30\n",
      "============================================================\n",
      "Processing Stage: Dissipation ...\n",
      "  Group: Small <100 (N=27541)\n",
      "  Group: Medium 100-500 (N=318)\n",
      "  Group: Large 500-2000 (N=11)\n",
      "  Group: Total (N=27870)\n",
      "Processing Stage: Onset ...\n",
      "  Group: Small <100 (N=31241)\n",
      "  Group: Medium 100-500 (N=1984)\n",
      "  Group: Large 500-2000 (N=53)\n",
      "  Group: Total (N=33278)\n",
      "Processing Stage: Duration ...\n",
      "  Group: Small <100 (N=69759)\n",
      "  Group: Medium 100-500 (N=5813)\n",
      "  Group: Large 500-2000 (N=106)\n",
      "  Group: Total (N=75678)\n",
      "Processing Stage: All ...\n",
      "  Group: Small <100 (N=151201)\n",
      "  Group: Medium 100-500 (N=86883)\n",
      "  Group: Large 500-2000 (N=18110)\n",
      "  Group: XLarge >2000 (N=630)\n",
      "  Group: Total (N=256824)\n",
      "Processing Stage: Daily ...\n",
      "  Group: Small <100 (N=8268)\n",
      "  Group: Medium 100-500 (N=29)\n",
      "  Group: Large 500-2000 (N=4)\n",
      "  Group: Total (N=8301)\n",
      "Step 2 Completed. Time: 1558.2s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Algorithm 1 (Total Pairs)\n",
    "# =============================================================================\n",
    "\n",
    "def step2_run_algo1():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Step 2: Running Algo 1 | Sim: {N_SIM_ALGO12} | Workers: {N_WORKERS}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # [New] Prevent duplicate runs in Jupyter from causing append pollution\n",
    "    if os.path.exists(OUTPUT_FILE_ALGO1):\n",
    "        os.remove(OUTPUT_FILE_ALGO1)\n",
    "        print(f\"Warning: Removed existing output file {OUTPUT_FILE_ALGO1} to avoid duplicates.\")\n",
    "    \n",
    "    # Load cache\n",
    "    try:\n",
    "        # Load daily position matrix for 8 major planets (for fast CTS lookup)\n",
    "        ephem_matrix_daily = np.load(os.path.join(CACHE_DIR, 'ephem_matrix_8p.npy'))\n",
    "        with open(os.path.join(CACHE_DIR, 'kepler_prob_maps.pkl'), 'rb') as f:\n",
    "            prob_maps = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Cache files not found. Please run Step 1 first.\")\n",
    "        return\n",
    "\n",
    "    results_buffer = []\n",
    "    files = [f for f in os.listdir(CACHE_DIR) if f.startswith('ready_') and f.endswith('.parquet')]\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for f in files:\n",
    "        stage_name = f.replace('ready_', '').replace('.parquet', '')\n",
    "        print(f\"Processing Stage: {stage_name} ...\")\n",
    "        \n",
    "        # Read data (only need to read 8 major planet columns)\n",
    "        cols_to_load = ['hme_lon', 'ephem_idx_daily', 'Group'] + PLANET_COLS\n",
    "        df = pd.read_parquet(os.path.join(CACHE_DIR, f), columns=cols_to_load)\n",
    "        \n",
    "        groups = sorted(df['Group'].unique(), key=lambda x: ('SMLX'.find(x[0]), x))\n",
    "        groups.append('Total')\n",
    "        \n",
    "        for group in groups:\n",
    "            subset = df if group == 'Total' else df[df['Group'] == group]\n",
    "            if subset.empty: continue\n",
    "            \n",
    "            # Prepare core data\n",
    "            sun_lons = subset['hme_lon'].values.astype(np.float64)\n",
    "            obs_planets_interp = subset[PLANET_COLS].values.astype(np.float64) # Interpolated high-precision data\n",
    "            sun_idxs_daily = subset['ephem_idx_daily'].values.astype(int)      # Integer index for CTS\n",
    "            n_recs = len(subset)\n",
    "            \n",
    "            print(f\"  Group: {group} (N={n_recs})\")\n",
    "            \n",
    "            for w in THRESHOLDS:\n",
    "                for etype in ['Opposition', 'Conjunction']:\n",
    "                    \n",
    "                    # --- 1. Calculate observed value (k_obs) ---\n",
    "                    # Call vectorized function in algo_workers\n",
    "                    if algo_workers:\n",
    "                        k_obs = algo_workers.count_events_vectorized(sun_lons, obs_planets_interp, w, etype)\n",
    "                    else:\n",
    "                        # Inline fallback logic\n",
    "                        if etype == 'Conjunction':\n",
    "                            delta = np.abs(sun_lons[:, None] - obs_planets_interp)\n",
    "                            delta = np.where(delta > 180, 360 - delta, delta)\n",
    "                            k_obs = np.sum(delta <= w)\n",
    "                        else:\n",
    "                            delta = np.abs(np.abs(sun_lons[:, None] - obs_planets_interp) - 180)\n",
    "                            k_obs = np.sum(delta <= w)\n",
    "                    \n",
    "                    # --- 2. Prepare CTS simulation ---\n",
    "                    seeds = np.random.randint(0, 1000000000, N_SIM_ALGO12)\n",
    "                    args = [(seed, sun_lons, ephem_matrix_daily, sun_idxs_daily, w, etype) for seed in seeds]\n",
    "                    \n",
    "                    # --- 3. Execute simulation (k_sims) ---\n",
    "                    # Must use algo_workers.cts_worker_algo1 to support multi-core pickle\n",
    "                    if N_WORKERS > 1 and algo_workers:\n",
    "                        with Pool(N_WORKERS) as pool:\n",
    "                            k_sims = pool.starmap(algo_workers.cts_worker_algo1, args)\n",
    "                    else:\n",
    "                        # Use when single-core or no external worker\n",
    "                        if algo_workers:\n",
    "                             k_sims = [algo_workers.cts_worker_algo1(*a) for a in args]\n",
    "                        else:\n",
    "                             # Simple placeholder, cannot perform CTS if algo_workers file is missing\n",
    "                             print(\"Warning: algo_workers.py missing, skipping simulations.\")\n",
    "                             k_sims = [k_obs] * N_SIM_ALGO12\n",
    "\n",
    "                    k_sims = np.array(k_sims)\n",
    "                    \n",
    "                    # --- 4. Statistical calculation ---\n",
    "                    # --- [Modified] P-value calculation (two-tailed + effect direction) ---\n",
    "                    # Calculate left-tail (suppression) probability\n",
    "                    p_left = (np.sum(k_sims <= k_obs) + 1) / (N_SIM_ALGO12 + 1)\n",
    "                    # Calculate right-tail (enhancement) probability\n",
    "                    p_right = (np.sum(k_sims >= k_obs) + 1) / (N_SIM_ALGO12 + 1)\n",
    "                    \n",
    "                    # Two-tailed P-value (twice the minimum, capped at 1.0)\n",
    "                    p_val = 2 * min(p_left, p_right)\n",
    "                    if p_val > 1.0: p_val = 1.0\n",
    "                    \n",
    "                    # Record effect direction\n",
    "                    effect = 'Suppression' if k_obs < k_sims.mean() else 'Enhancement'\n",
    "\n",
    "                    if k_sims.std() == 0: z_score = 0\n",
    "                    else: z_score = (k_obs - k_sims.mean()) / k_sims.std()\n",
    "                    \n",
    "                    # --- 5. Theoretical Baseline (Kepler Baseline) ---\n",
    "                    # Use Kepler Prob Maps to calculate precise expectation\n",
    "                    lon_indices = np.floor(sun_lons).astype(int) % 360\n",
    "                    target_indices = (lon_indices + 180) % 360 if etype == 'Opposition' else lon_indices\n",
    "                    k_exp = 0\n",
    "                    for col in PLANET_COLS:\n",
    "                        k_exp += np.sum(prob_maps[col][target_indices] * (2 * w))\n",
    "                    \n",
    "                    ratio = (k_obs / k_exp * 100) if k_exp > 0 else 0\n",
    "                    \n",
    "                    # --- [Important] Remember to add 'Effect': effect to the dictionary ---\n",
    "                    results_buffer.append({\n",
    "                        'Stage': stage_name, 'Group': group, 'Window': w, 'Type': etype,\n",
    "                        'N_Records': n_recs, 'k_obs': k_obs, 'k_exp': round(k_exp, 2),\n",
    "                        'Ratio': round(ratio, 2), \n",
    "                        'p_val': p_val, \n",
    "                        'Z_score': round(z_score, 2),\n",
    "                        'Effect': effect  # <--- Added this line!\n",
    "                    })\n",
    "        \n",
    "        # Stage save\n",
    "        if results_buffer:\n",
    "            new_df = pd.DataFrame(results_buffer)\n",
    "            write_header = not os.path.exists(OUTPUT_FILE_ALGO1)\n",
    "            new_df.to_csv(OUTPUT_FILE_ALGO1, mode='a', header=write_header, index=False)\n",
    "            results_buffer = []\n",
    "\n",
    "    print(f\"Step 2 Completed. Time: {time.time()-total_start_time:.1f}s\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    step2_run_algo1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929fa6ac-5011-4f48-99e6-659e7c2a5162",
   "metadata": {},
   "source": [
    "## Step 3: Algorithm 2 (Trigger Mechanism - At Least One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23920078-5752-4afe-987f-3fea4087454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 3: Running Algo 2 | Sim: 10000 | Workers: 30\n",
      "============================================================\n",
      "Processing Stage: Dissipation ...\n",
      "  Group: Small <100 (N=27541)\n",
      "  Group: Medium 100-500 (N=318)\n",
      "  Group: Large 500-2000 (N=11)\n",
      "  Group: Total (N=27870)\n",
      "Processing Stage: Onset ...\n",
      "  Group: Small <100 (N=31241)\n",
      "  Group: Medium 100-500 (N=1984)\n",
      "  Group: Large 500-2000 (N=53)\n",
      "  Group: Total (N=33278)\n",
      "Processing Stage: Duration ...\n",
      "  Group: Small <100 (N=69759)\n",
      "  Group: Medium 100-500 (N=5813)\n",
      "  Group: Large 500-2000 (N=106)\n",
      "  Group: Total (N=75678)\n",
      "Processing Stage: All ...\n",
      "  Group: Small <100 (N=151201)\n",
      "  Group: Medium 100-500 (N=86883)\n",
      "  Group: Large 500-2000 (N=18110)\n",
      "  Group: XLarge >2000 (N=630)\n",
      "  Group: Total (N=256824)\n",
      "Processing Stage: Daily ...\n",
      "  Group: Small <100 (N=8268)\n",
      "  Group: Medium 100-500 (N=29)\n",
      "  Group: Large 500-2000 (N=4)\n",
      "  Group: Total (N=8301)\n",
      "Step 3 Completed. Time: 1573.2s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 3: Algorithm 2 (Trigger Mechanism - At Least One)\n",
    "# =============================================================================\n",
    "\n",
    "def step3_run_algo2():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Step 3: Running Algo 2 | Sim: {N_SIM_ALGO12} | Workers: {N_WORKERS}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # [New] Prevent duplicate runs in Jupyter from causing append pollution (for Algo 2 file)\n",
    "    if os.path.exists(OUTPUT_FILE_ALGO2):\n",
    "        os.remove(OUTPUT_FILE_ALGO2)\n",
    "        print(f\"Warning: Removed existing output file {OUTPUT_FILE_ALGO2} to avoid duplicates.\")\n",
    "    \n",
    "    try:\n",
    "        ephem_matrix_daily = np.load(os.path.join(CACHE_DIR, 'ephem_matrix_8p.npy'))\n",
    "        with open(os.path.join(CACHE_DIR, 'kepler_prob_maps.pkl'), 'rb') as f:\n",
    "            prob_maps = pickle.load(f)\n",
    "    except FileNotFoundError: return\n",
    "            \n",
    "    results_buffer = []\n",
    "    files = [f for f in os.listdir(CACHE_DIR) if f.startswith('ready_') and f.endswith('.parquet')]\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for f in files:\n",
    "        stage_name = f.replace('ready_', '').replace('.parquet', '')\n",
    "        print(f\"Processing Stage: {stage_name} ...\")\n",
    "        \n",
    "        cols_to_load = ['hme_lon', 'ephem_idx_daily', 'Group'] + PLANET_COLS\n",
    "        df = pd.read_parquet(os.path.join(CACHE_DIR, f), columns=cols_to_load)\n",
    "        \n",
    "        groups = sorted(df['Group'].unique(), key=lambda x: ('SMLX'.find(x[0]), x))\n",
    "        groups.append('Total')\n",
    "        \n",
    "        for group in groups:\n",
    "            subset = df if group == 'Total' else df[df['Group'] == group]\n",
    "            if subset.empty: continue\n",
    "            \n",
    "            sun_lons = subset['hme_lon'].values.astype(np.float64)\n",
    "            obs_planets_interp = subset[PLANET_COLS].values.astype(np.float64)\n",
    "            sun_idxs_daily = subset['ephem_idx_daily'].values.astype(int)\n",
    "            n_recs = len(subset)\n",
    "            \n",
    "            print(f\"  Group: {group} (N={n_recs})\")\n",
    "            \n",
    "            for w in THRESHOLDS:\n",
    "                for etype in ['Opposition', 'Conjunction']:\n",
    "                    \n",
    "                    # --- 1. Calculate observed value (k_obs) ---\n",
    "                    if algo_workers:\n",
    "                        k_obs = algo_workers.count_events_at_least_once(sun_lons, obs_planets_interp, w, etype)\n",
    "                    else:\n",
    "                        # Inline fallback\n",
    "                        if etype == 'Conjunction':\n",
    "                            delta = np.abs(sun_lons[:, None] - obs_planets_interp)\n",
    "                            delta = np.where(delta > 180, 360 - delta, delta)\n",
    "                            is_event = np.any(delta <= w, axis=1)\n",
    "                        else:\n",
    "                            delta = np.abs(np.abs(sun_lons[:, None] - obs_planets_interp) - 180)\n",
    "                            is_event = np.any(delta <= w, axis=1)\n",
    "                        k_obs = np.sum(is_event)\n",
    "                    \n",
    "                    # --- 2. Simulation ---\n",
    "                    seeds = np.random.randint(0, 1000000000, N_SIM_ALGO12)\n",
    "                    args = [(seed, sun_lons, ephem_matrix_daily, sun_idxs_daily, w, etype) for seed in seeds]\n",
    "                    \n",
    "                    if N_WORKERS > 1 and algo_workers:\n",
    "                        with Pool(N_WORKERS) as pool:\n",
    "                            k_sims = pool.starmap(algo_workers.cts_worker_algo2, args)\n",
    "                    elif algo_workers:\n",
    "                        k_sims = [algo_workers.cts_worker_algo2(*arg) for arg in args]\n",
    "                    else:\n",
    "                        k_sims = [k_obs] * N_SIM_ALGO12\n",
    "                            \n",
    "                    k_sims = np.array(k_sims)\n",
    "                    \n",
    "                    # --- 3. Statistics ---\n",
    "                    # --- [Modified] P-value calculation (two-tailed + effect direction) ---\n",
    "                    # Calculate left-tail (suppression) probability\n",
    "                    p_left = (np.sum(k_sims <= k_obs) + 1) / (N_SIM_ALGO12 + 1)\n",
    "                    # Calculate right-tail (enhancement) probability\n",
    "                    p_right = (np.sum(k_sims >= k_obs) + 1) / (N_SIM_ALGO12 + 1)\n",
    "                    \n",
    "                    # Two-tailed P-value\n",
    "                    p_val = 2 * min(p_left, p_right)\n",
    "                    if p_val > 1.0: p_val = 1.0\n",
    "                    \n",
    "                    # Record effect direction\n",
    "                    effect = 'Suppression' if k_obs < k_sims.mean() else 'Enhancement'\n",
    "\n",
    "                    if k_sims.std() == 0: z_score = 0\n",
    "                    else: z_score = (k_obs - k_sims.mean()) / k_sims.std()\n",
    "                    \n",
    "                    # --- 4. Theoretical Baseline (Prob At Least One) ---\n",
    "                    lon_indices = np.floor(sun_lons).astype(int) % 360\n",
    "                    target_indices = (lon_indices + 180) % 360 if etype == 'Opposition' else lon_indices\n",
    "                    \n",
    "                    p_mat = np.zeros((n_recs, len(PLANET_COLS)))\n",
    "                    for i, col in enumerate(PLANET_COLS):\n",
    "                        p_mat[:, i] = prob_maps[col][target_indices] * (2 * w)\n",
    "                    \n",
    "                    # P(At Least One) = 1 - Prod(1 - P_i)\n",
    "                    p_at_least_one = 1.0 - np.prod(1.0 - p_mat, axis=1)\n",
    "                    k_exp = np.sum(p_at_least_one)\n",
    "                    \n",
    "                    ratio = (k_obs / k_exp * 100) if k_exp > 0 else 0\n",
    "                 \n",
    "                    # --- [Important] Remember to add 'Effect': effect to the dictionary ---\n",
    "                    results_buffer.append({\n",
    "                        'Stage': stage_name, 'Group': group, 'Window': w, 'Type': etype,\n",
    "                        'N_Records': n_recs, 'k_obs': k_obs, 'k_exp': round(k_exp, 2),\n",
    "                        'Ratio': round(ratio, 2), \n",
    "                        'p_val': p_val, \n",
    "                        'Z_score': round(z_score, 2),\n",
    "                        'Effect': effect  # <--- Added this line!\n",
    "                    })\n",
    "        \n",
    "        if results_buffer:\n",
    "            new_df = pd.DataFrame(results_buffer)\n",
    "            write_header = not os.path.exists(OUTPUT_FILE_ALGO2)\n",
    "            new_df.to_csv(OUTPUT_FILE_ALGO2, mode='a', header=write_header, index=False)\n",
    "            results_buffer = []\n",
    "\n",
    "    print(f\"Step 3 Completed. Time: {time.time()-total_start_time:.1f}s\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    step3_run_algo2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cba0d2-0501-4ac1-8065-fc1491407fb9",
   "metadata": {},
   "source": [
    "## Step 4: Algorithm 3 (Single Body Full Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b86919-d781-476a-93f3-3c908065a957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 4: Running Algo 3 (Final) - FORCE SINGLE CORE MODE\n",
      "============================================================\n",
      "\n",
      "Processing Stage: Dissipation ...\n",
      "  Analyzing 781 bodies against 27870 records...\n",
      "  [Mode] Executing in Serial Mode (No Multiprocessing)...\n",
      "  Processed 781/781 bodies. Done.\n",
      "  Saved 7810 rows to ../../results/04_conj_enh_opp_sup/sg/sg_algo3_single_body_781.csv\n",
      "\n",
      "Processing Stage: Onset ...\n",
      "  Analyzing 781 bodies against 33278 records...\n",
      "  [Mode] Executing in Serial Mode (No Multiprocessing)...\n",
      "  Processed 781/781 bodies. Done.\n",
      "  Saved 7810 rows to ../../results/04_conj_enh_opp_sup/sg/sg_algo3_single_body_781.csv\n",
      "\n",
      "Processing Stage: Duration ...\n",
      "  Analyzing 781 bodies against 75678 records...\n",
      "  [Mode] Executing in Serial Mode (No Multiprocessing)...\n",
      "  Processed 781/781 bodies. Done.\n",
      "  Saved 7810 rows to ../../results/04_conj_enh_opp_sup/sg/sg_algo3_single_body_781.csv\n",
      "\n",
      "Processing Stage: All ...\n",
      "  Analyzing 781 bodies against 256824 records...\n",
      "  [Mode] Executing in Serial Mode (No Multiprocessing)...\n",
      "  Processed 781/781 bodies. Done.\n",
      "  Saved 7810 rows to ../../results/04_conj_enh_opp_sup/sg/sg_algo3_single_body_781.csv\n",
      "\n",
      "Processing Stage: Daily ...\n",
      "  Analyzing 781 bodies against 8301 records...\n",
      "  [Mode] Executing in Serial Mode (No Multiprocessing)...\n",
      "  Processed 781/781 bodies. Done.\n",
      "  Saved 7810 rows to ../../results/04_conj_enh_opp_sup/sg/sg_algo3_single_body_781.csv\n",
      "Step 4 Completed. Time: 11.4s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 4: Algorithm 3 (Single Body Full Version) - Frequency + Amplitude + P-value\n",
    "# =============================================================================\n",
    "\n",
    "def worker_algo3_final(args):\n",
    "    \"\"\"\n",
    "    Final Worker (Fixed Version): \n",
    "    1. Dynamically calculate Kepler probability (solve eccentricity issue)\n",
    "    2. Two-tailed P-value test\n",
    "    3. Record Enhancement/Suppression direction\n",
    "    \"\"\"\n",
    "    body_name, body_lons, sun_lons, sun_areas, thresholds, n_sims = args\n",
    "    results = []\n",
    "    n_recs = len(sun_lons)\n",
    "\n",
    "    # Force single-core execution\n",
    "    N_WORKERS_STEP4 = 1\n",
    "    \n",
    "    # Global average area (for calculating amplitude ratio)\n",
    "    global_avg_area = np.mean(sun_areas) if n_recs > 0 else 0\n",
    "    \n",
    "    # --- [Key Fix 1] Calculate the Kepler probability distribution (histogram) of the body on the fly ---\n",
    "    # Count the frequency (density) of the body in each degree from 0-360, reflecting its residence time in various parts of the orbit\n",
    "    # density=True ensures sum * bin_width = 1\n",
    "    hist_prob, _ = np.histogram(body_lons, bins=360, range=(0, 360), density=True)\n",
    "    \n",
    "    # Pre-calculate integer indices of sun positions for fast lookup\n",
    "    sun_idx_conj = np.floor(sun_lons).astype(int) % 360\n",
    "    sun_idx_opp  = (sun_idx_conj + 180) % 360\n",
    "    \n",
    "    for w in thresholds:\n",
    "        # [Removed] Old uniform distribution assumption: p0 = (2 * w) / 360.0 \n",
    "        \n",
    "        for etype in ['Opposition', 'Conjunction']:\n",
    "            # --- A. Basic calculation (remains unchanged) ---\n",
    "            if etype == 'Conjunction':\n",
    "                d = np.abs(sun_lons - body_lons)\n",
    "                d = np.where(d > 180, 360 - d, d)\n",
    "                is_event = (d <= w)\n",
    "                \n",
    "                # [Key Fix 1] Get background probability only for coincidence moments\n",
    "                target_indices = sun_idx_conj\n",
    "                \n",
    "            else: # Opposition\n",
    "                d = np.abs(np.abs(sun_lons - body_lons) - 180)\n",
    "                is_event = (d <= w)\n",
    "                \n",
    "                target_indices = sun_idx_opp\n",
    "            \n",
    "            k_obs = np.sum(is_event)\n",
    "            \n",
    "            # --- B. Calculate precise expectation (k_exp) based on Kepler distribution ---\n",
    "            # Lookup: hist_prob[idx] is the probability density within 1 degree\n",
    "            # Multiply by (2*w) to get the total probability within the window\n",
    "            # Sum over all records to get the expected frequency\n",
    "            daily_probs = hist_prob[target_indices] * (2 * w)\n",
    "            k_exp = np.sum(daily_probs)\n",
    "            \n",
    "            # Calculate frequency ratio\n",
    "            ratio_freq = (k_obs / k_exp * 100) if k_exp > 0 else 0\n",
    "            \n",
    "            # --- C. Amplitude metrics (remains unchanged) ---\n",
    "            if k_obs > 0:\n",
    "                event_avg_area = np.mean(sun_areas[is_event])\n",
    "                ratio_amp = (event_avg_area / global_avg_area * 100) if global_avg_area > 0 else 0\n",
    "            else:\n",
    "                event_avg_area = 0\n",
    "                ratio_amp = 0\n",
    "            \n",
    "            # --- D. Significance P-value (Two-tailed fixed version) ---\n",
    "            # Since the probability of each trial is different, the binomial distribution is strictly not applicable.\n",
    "            # But as a robust approximation, we substitute the average probability p_avg.\n",
    "            p_avg = k_exp / n_recs if n_recs > 0 else 0\n",
    "            \n",
    "            # 1. Calculate two-tailed probability\n",
    "            p_left = stats.binom.cdf(k_obs, n_recs, p_avg)\n",
    "            p_right = stats.binom.sf(k_obs - 1, n_recs, p_avg)\n",
    "            p_val = 2 * min(p_left, p_right)\n",
    "            if p_val > 1.0: p_val = 1.0\n",
    "            \n",
    "            # 2. [Key Fix 2] Record effect direction\n",
    "            effect = 'Suppression' if k_obs < k_exp else 'Enhancement'\n",
    "              \n",
    "            results.append({\n",
    "                'Body': body_name,\n",
    "                'Window': w,\n",
    "                'Type': etype,\n",
    "                'k_obs': k_obs,\n",
    "                'k_exp': round(k_exp, 2),\n",
    "                'Ratio_Freq': round(ratio_freq, 2),\n",
    "                'Avg_Area': round(event_avg_area, 2),\n",
    "                'Ratio_Amp': round(ratio_amp, 2),\n",
    "                'p_val': p_val,\n",
    "                'Effect': effect  # Must include this column\n",
    "            })\n",
    "    return results\n",
    "\n",
    "def step4_run_algo3_final():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Step 4: Running Algo 3 (Final) - FORCE SINGLE CORE MODE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # [CRITICAL CONFIG] Force single-core execution\n",
    "    # We explicitly set this to 1 to bypass global settings.\n",
    "    # This avoids Windows multiprocessing pickling errors and IPC deadlocks.\n",
    "    # Since the calculation is vector-based and fast, serial execution is optimal.\n",
    "    FORCED_WORKERS = 1 \n",
    "    \n",
    "    # Remove existing output file to prevent duplicate appending from previous runs\n",
    "    if os.path.exists(OUTPUT_FILE_ALGO3):\n",
    "        os.remove(OUTPUT_FILE_ALGO3)\n",
    "        print(f\"Warning: Removed existing output file {OUTPUT_FILE_ALGO3} to avoid duplicates.\")\n",
    "    \n",
    "    # Get list of input files from cache directory\n",
    "    files = [f for f in os.listdir(CACHE_DIR) if f.startswith('ready_') and f.endswith('.parquet')]\n",
    "    \n",
    "    total_start = time.time()\n",
    "    \n",
    "    for f in files:\n",
    "        stage_name = f.replace('ready_', '').replace('.parquet', '')\n",
    "        print(f\"\\nProcessing Stage: {stage_name} ...\")\n",
    "        \n",
    "        # Read data\n",
    "        df = pd.read_parquet(os.path.join(CACHE_DIR, f))\n",
    "        \n",
    "        # Identify columns: exclude metadata, keep planetary/body columns\n",
    "        meta_cols = {'date', 'hme_lon', 'area', 'ephem_idx_daily', 'Group', 'lat_lon', 'hg_lon', 'hgc_lon', 'lon', 'lat'}\n",
    "        body_cols = [c for c in df.columns if c not in meta_cols and c.endswith('_lon')]\n",
    "        \n",
    "        print(f\"  Analyzing {len(body_cols)} bodies against {len(df)} records...\")\n",
    "        \n",
    "        # Prepare data arrays\n",
    "        sun_lons = df['hme_lon'].values.astype(np.float32)\n",
    "        sun_areas = df['area'].values.astype(np.float32)\n",
    "        \n",
    "        # Create task list\n",
    "        # Note: Passing large arrays (sun_lons) in single-core mode is efficient \n",
    "        # because Python passes by reference (no memory copying/pickling involved).\n",
    "        tasks = []\n",
    "        for col in body_cols:\n",
    "            body_data = df[col].values.astype(np.float32)\n",
    "            tasks.append((col, body_data, sun_lons, sun_areas, THRESHOLDS, N_SIM_ALGO3))\n",
    "            \n",
    "        results_flat = []\n",
    "        \n",
    "        # [CORE LOGIC] Execute in Serial Mode\n",
    "        # We loop directly in the main process. No overhead, no deadlocks.\n",
    "        print(\"  [Mode] Executing in Serial Mode (No Multiprocessing)...\")\n",
    "        \n",
    "        for i, t in enumerate(tasks):\n",
    "            # Execute the worker function directly\n",
    "            results_flat.extend(worker_algo3_final(t))\n",
    "            \n",
    "        print(f\"  Processed {len(tasks)}/{len(tasks)} bodies. Done.\")\n",
    "\n",
    "        # Save results to CSV\n",
    "        if results_flat:\n",
    "            df_res = pd.DataFrame(results_flat)\n",
    "            df_res['Stage'] = stage_name\n",
    "            \n",
    "            # Write header only if file does not exist\n",
    "            hdr = not os.path.exists(OUTPUT_FILE_ALGO3)\n",
    "            df_res.to_csv(OUTPUT_FILE_ALGO3, mode='a', header=hdr, index=False)\n",
    "            print(f\"  Saved {len(df_res)} rows to {OUTPUT_FILE_ALGO3}\")\n",
    "            \n",
    "    print(f\"Step 4 Completed. Time: {time.time() - total_start:.1f}s\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    step4_run_algo3_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ea205-e92e-476f-ad55-dfc63c2dd26f",
   "metadata": {},
   "source": [
    "## Step 5: physical trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7bf84d9-8d8f-4c4f-b87a-d11698b3a211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[Algo 1: Total Pairs] Physical Trend Verification Report (No FDR Penalty)\n",
      "Core Logic: Windows are nested (N=1), focus on checking the 'Conjunction-Enhancement, Opposition-Suppression' pattern for Large/XLarge.\n",
      "================================================================================\n",
      "Window                                 1             2              3             4              5\n",
      "Group          Type                                                                               \n",
      "Large 500-2000 Conjunction    0% (1.000)    0% (0.735)    68% (1.000)   51% (0.849)    82% (1.000)\n",
      "               Opposition     0% (1.000)    0% (0.768)     0% (0.464)    0% (0.312)     0% (0.187)\n",
      "XLarge >2000   Conjunction  121% (0.416)  132% (0.132)  138% (0.029*)  129% (0.099)  133% (0.035*)\n",
      "               Opposition    71% (0.255)   84% (0.484)    80% (0.270)   76% (0.096)  69% (0.003**)\n",
      "\n",
      "[Interpretation Guide]\n",
      "  1. Look for cells with * or ** (indicating Raw P < 0.05).\n",
      "  2. Check horizontal trends: Is the Ratio on the same side of 100 from 1 to 5 degrees?\n",
      "     - Conjunction: Expected Ratio > 100\n",
      "     - Opposition: Expected Ratio < 100\n",
      "  3. If the above two points are met, it can be judged as physically significant.\n",
      "\n",
      "================================================================================\n",
      "[Algo 2: At Least One] Physical Trend Verification Report (No FDR Penalty)\n",
      "Core Logic: Windows are nested (N=1), focus on checking the 'Conjunction-Enhancement, Opposition-Suppression' pattern for Large/XLarge.\n",
      "================================================================================\n",
      "Window                                 1             2             3             4             5\n",
      "Group          Type                                                                             \n",
      "Large 500-2000 Conjunction    0% (1.000)    0% (0.742)   72% (1.000)   55% (0.864)   45% (0.667)\n",
      "               Opposition     0% (1.000)    0% (0.728)    0% (0.467)    0% (0.299)    0% (0.185)\n",
      "XLarge >2000   Conjunction  124% (0.366)  126% (0.206)  132% (0.074)  122% (0.224)  126% (0.096)\n",
      "               Opposition    69% (0.198)   83% (0.446)   81% (0.280)   79% (0.138)  72% (0.013*)\n",
      "\n",
      "[Interpretation Guide]\n",
      "  1. Look for cells with * or ** (indicating Raw P < 0.05).\n",
      "  2. Check horizontal trends: Is the Ratio on the same side of 100 from 1 to 5 degrees?\n",
      "     - Conjunction: Expected Ratio > 100\n",
      "     - Opposition: Expected Ratio < 100\n",
      "  3. If the above two points are met, it can be judged as physically significant.\n",
      "\n",
      "================================================================================\n",
      "[Algo 3] Performing Fair FDR Correction (for 781 bodies)\n",
      "================================================================================\n",
      "Correction completed. A total of 64 significant body records passing FDR were found across all group scans.\n",
      "Please open the CSV to view rows where 'sig_fdr' is True, focusing on results corresponding to Large/XLarge sunspots.\n",
      "\n",
      ">>> Analysis completed. Please write physical conclusions based on Algo 1 pivot table results. <<<\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration Paths\n",
    "# =============================================================================\n",
    "BASE_DIR = '../../results/04_conj_enh_opp_sup/sg'\n",
    "FILE_ALGO1 = os.path.join(BASE_DIR, 'sg_algo1_total_pairs.csv')\n",
    "FILE_ALGO2 = os.path.join(BASE_DIR, 'sg_algo2_at_least_one.csv')\n",
    "FILE_ALGO3 = os.path.join(BASE_DIR, 'sg_algo3_single_body_781.csv')\n",
    "\n",
    "def analyze_physics_trend(csv_file, algo_name):\n",
    "    \"\"\"\n",
    "    For Algo 1 & 2: \n",
    "    Do not use FDR (treated as N=1 sensitivity analysis), but check for consistency in physical trends.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"File not found: {csv_file}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[{algo_name}] Physical Trend Verification Report (No FDR Penalty)\")\n",
    "    print(f\"Core Logic: Windows are nested (N=1), focus on checking the 'Conjunction-Enhancement, Opposition-Suppression' pattern for Large/XLarge.\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # 1. Physical Filtering: Focus only on Large and XLarge (exclude small sunspot noise)\n",
    "    target_groups = ['Large 500-2000', 'XLarge >2000']\n",
    "    # Check groups actually present in the data\n",
    "    available_groups = [g for g in target_groups if g in df['Group'].unique()]\n",
    "    \n",
    "    if not available_groups:\n",
    "        print(\"No Large/XLarge group data found.\")\n",
    "        return\n",
    "        \n",
    "    df_target = df[df['Group'].isin(available_groups)].copy()\n",
    "\n",
    "    # 2. Construct Pivot Table: Display 1-5 degree windows horizontally to intuitively judge robustness\n",
    "    # Target format: \"Ratio% (P-value)\"\n",
    "    def format_cell(row):\n",
    "        # Markers: ** p<0.01, * p<0.05\n",
    "        p = row['p_val']\n",
    "        mark = \"\"\n",
    "        if p < 0.01: mark = \"**\"\n",
    "        elif p < 0.05: mark = \"*\"\n",
    "        \n",
    "        # Physical direction check\n",
    "        ratio = row['Ratio']\n",
    "        # Conjunction expectation > 100, Opposition expectation < 100\n",
    "        is_consistent = False\n",
    "        if row['Type'] == 'Conjunction' and ratio > 100: is_consistent = True\n",
    "        elif row['Type'] == 'Opposition' and ratio < 100: is_consistent = True\n",
    "        \n",
    "        # If significant but direction is reversed, mark as anomalous\n",
    "        if mark and not is_consistent:\n",
    "            return f\"{ratio:.0f}%(Anomalous{mark})\"\n",
    "        \n",
    "        return f\"{ratio:.0f}% ({p:.3f}{mark})\"\n",
    "\n",
    "    df_target['Result'] = df_target.apply(format_cell, axis=1)\n",
    "    \n",
    "    # Create pivot table\n",
    "    # Rows: Group, Type\n",
    "    # Columns: Window\n",
    "    try:\n",
    "        pivot = df_target.pivot_table(\n",
    "            index=['Group', 'Type'], \n",
    "            columns='Window', \n",
    "            values='Result', \n",
    "            aggfunc='first'\n",
    "        )\n",
    "        \n",
    "        # Adjust column order\n",
    "        cols = sorted(pivot.columns)\n",
    "        pivot = pivot[cols]\n",
    "        \n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.width', 1000)\n",
    "        print(pivot)\n",
    "        \n",
    "        print(\"\\n[Interpretation Guide]\")\n",
    "        print(\"  1. Look for cells with * or ** (indicating Raw P < 0.05).\")\n",
    "        print(\"  2. Check horizontal trends: Is the Ratio on the same side of 100 from 1 to 5 degrees?\")\n",
    "        print(\"     - Conjunction: Expected Ratio > 100\")\n",
    "        print(\"     - Opposition: Expected Ratio < 100\")\n",
    "        print(\"  3. If the above two points are met, it can be judged as physically significant.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate pivot table: {e}\")\n",
    "\n",
    "\n",
    "def process_algo3_fair_fdr(csv_file):\n",
    "    \"\"\"\n",
    "    For Algo 3 (Single Body Scan):\n",
    "    FDR must be retained because 781 bodies were tested (N=781).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_file): return\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[Algo 3] Performing Fair FDR Correction (for 781 bodies)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Must group by environment (Stage + Window + Type)\n",
    "    # Compare 781 bodies under identical conditions\n",
    "    group_cols = ['Stage', 'Window', 'Type']\n",
    "    actual_cols = [c for c in group_cols if c in df.columns]\n",
    "    \n",
    "    df['p_adj_bh'] = np.nan\n",
    "    df['sig_fdr'] = False\n",
    "    \n",
    "    total_sig = 0\n",
    "    \n",
    "    for name, group_data in df.groupby(actual_cols):\n",
    "        # N = 781\n",
    "        p_vals = group_data['p_val'].values\n",
    "        idx = group_data.index\n",
    "        \n",
    "        # BH Correction\n",
    "        reject, p_adj, _, _ = multipletests(p_vals, alpha=0.05, method='fdr_bh')\n",
    "        \n",
    "        df.loc[idx, 'p_adj_bh'] = p_adj\n",
    "        df.loc[idx, 'sig_fdr'] = reject\n",
    "        total_sig += reject.sum()\n",
    "            \n",
    "    # Save results\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Correction completed. A total of {total_sig} significant body records passing FDR were found across all group scans.\")\n",
    "    if total_sig > 0:\n",
    "        print(\"Please open the CSV to view rows where 'sig_fdr' is True, focusing on results corresponding to Large/XLarge sunspots.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. Run physical trend analysis for Algo 1 (Most important)\n",
    "    analyze_physics_trend(FILE_ALGO1, \"Algo 1: Total Pairs\")\n",
    "    \n",
    "    # 2. Run physical trend analysis for Algo 2\n",
    "    analyze_physics_trend(FILE_ALGO2, \"Algo 2: At Least One\")\n",
    "    \n",
    "    # 3. Run FDR correction for Algo 3 (Needle in a haystack mode)\n",
    "    process_algo3_fair_fdr(FILE_ALGO3)\n",
    "    \n",
    "    print(\"\\n>>> Analysis completed. Please write physical conclusions based on Algo 1 pivot table results. <<<\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae81ae-8fd9-4e9a-a969-7684941442bb",
   "metadata": {},
   "source": [
    "## Step 6: Kuiper Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89640c0c-dc30-4f40-aa59-921b95b5b261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 5: Kuiper Test (Full Coverage)\n",
      "============================================================\n",
      "Processing Stage: Dissipation ...\n",
      "Processing Stage: Onset ...\n",
      "Processing Stage: Duration ...\n",
      "Processing Stage: All ...\n",
      "Processing Stage: Daily ...\n",
      "Kuiper Test results saved to ../../results/04_conj_enh_opp_sup/sg/sg_algo_kuiper_test.csv\n",
      "Top 5 Significant Deviations:\n",
      "          Stage           Group   Planet      N  V_statistic  p_value Sig\n",
      "2   Dissipation           Total  399_lon  27870       0.6090      0.0  **\n",
      "18  Dissipation      Small <100  399_lon  27541       0.6096      0.0  **\n",
      "42        Onset      Small <100  399_lon  31241       0.5825      0.0  **\n",
      "34        Onset           Total  399_lon  33278       0.5823      0.0  **\n",
      "50        Onset  Medium 100-500  399_lon   1984       0.6277      0.0  **\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 6: Kuiper Test (Verify Morphology/Bimodal Distribution)\n",
    "# =============================================================================\n",
    "try:\n",
    "    from astropy.stats import kuiper\n",
    "except ImportError:\n",
    "    print(\"Warning: astropy not installed. Skipping Kuiper test.\")\n",
    "    kuiper = None\n",
    "    \n",
    "def step5_run_kuiper_test_full():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Step 5: Kuiper Test (Full Coverage)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if kuiper is None: \n",
    "        print(\"Astropy missing, skipping.\")\n",
    "        return\n",
    "\n",
    "    # Prepare results container\n",
    "    results_kuiper = []\n",
    "    \n",
    "    # Get all stage files in the cache directory\n",
    "    files = [f for f in os.listdir(CACHE_DIR) if f.startswith('ready_') and f.endswith('.parquet')]\n",
    "    \n",
    "    for f in files:\n",
    "        stage_name = f.replace('ready_', '').replace('.parquet', '')\n",
    "        print(f\"Processing Stage: {stage_name} ...\")\n",
    "        \n",
    "        # Read data\n",
    "        df = pd.read_parquet(os.path.join(CACHE_DIR, f))\n",
    "        \n",
    "        # Determine grouping: besides Total, also look at specific Groups\n",
    "        # For convenience, process Total first\n",
    "        groups_dict = {'Total': df}\n",
    "        \n",
    "        # If Group column exists, split and process\n",
    "        if 'Group' in df.columns:\n",
    "            unique_groups = df['Group'].unique()\n",
    "            for g in unique_groups:\n",
    "                groups_dict[g] = df[df['Group'] == g]\n",
    "        \n",
    "        for group_name, subset_df in groups_dict.items():\n",
    "            if len(subset_df) < 10: continue # Skip statistics if sample size is too small\n",
    "            \n",
    "            for planet in PLANET_COLS:\n",
    "                # 1. Calculate phase angle (0-360)\n",
    "                # Assume hme_lon is the longitude of Earth/observation point, planet is the planet's longitude\n",
    "                # Phase = (Planet - Observation Point) % 360\n",
    "                phases = (subset_df[planet] - subset_df['hme_lon']) % 360.0\n",
    "                phases = phases.dropna().values\n",
    "                \n",
    "                if len(phases) == 0: continue\n",
    "\n",
    "                # 2. Normalize to [0, 1]\n",
    "                data = phases / 360.0\n",
    "                \n",
    "                # 3. Execute Kuiper test\n",
    "                try:\n",
    "                    V, p_val = kuiper(data)\n",
    "                    \n",
    "                    results_kuiper.append({\n",
    "                        'Stage': stage_name,\n",
    "                        'Group': group_name,\n",
    "                        'Planet': planet,\n",
    "                        'N': len(phases),\n",
    "                        'V_statistic': round(V, 4),\n",
    "                        'p_value': p_val, # Keep original precision\n",
    "                        'Sig': '**' if p_val < 0.01 else ('*' if p_val < 0.05 else '')\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in Kuiper for {stage_name}-{group_name}-{planet}: {e}\")\n",
    "\n",
    "    # Save\n",
    "    if results_kuiper:\n",
    "        df_k = pd.DataFrame(results_kuiper)\n",
    "        # Sort for easy viewing\n",
    "        df_k.sort_values(by=['p_value'], inplace=True)\n",
    "        df_k.to_csv(OUTPUT_FILE_KUIPER, index=False)\n",
    "        print(f\"Kuiper Test results saved to {OUTPUT_FILE_KUIPER}\")\n",
    "        print(\"Top 5 Significant Deviations:\")\n",
    "        print(df_k.head(5))\n",
    "    else:\n",
    "        print(\"No Kuiper results generated.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    step5_run_kuiper_test_full()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894676d-76d5-4132-aee4-7f84f93070a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
