{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4195d6e1-21f4-4eef-8b63-dc3055e8f745",
   "metadata": {},
   "source": [
    "## Step 0: Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b82405-a84e-4f76-81f0-d8c788c5362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Mode] Production Mode\n",
      "    - Algo 1/2 simulations: 10000\n",
      "    - Algo 3 simulations: 1000\n",
      "    - Workers: 30\n",
      "--- Configuration Loaded ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import importlib\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import scipy.stats as stats  # for fast P-value calculation\n",
    "\n",
    "# Try to import external worker, if not available then ignore (will use inline logic later)\n",
    "try:\n",
    "    import algo_workers\n",
    "    importlib.reload(algo_workers)\n",
    "except ImportError:\n",
    "    algo_workers = None\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Global Configuration & Switches\n",
    "# =============================================================================\n",
    "\n",
    "# [Core Switch] True = Test Mode (fast, 100 simulations); False = Production Mode (rigorous, 1000/10000 simulations)\n",
    "TEST_MODE = False \n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\">>> [Mode] Test Mode\")\n",
    "    print(\"    - Algo 1/2 simulations: 100\")\n",
    "    print(\"    - Algo 3 simulations: 100 (or using binomial distribution)\")\n",
    "    print(\"    - Workers: 1 (avoid debugging deadlocks)\")\n",
    "    N_SIM_ALGO12 = 100\n",
    "    N_SIM_ALGO3 = 100\n",
    "    N_WORKERS = 1\n",
    "else:\n",
    "    print(\">>> [Mode] Production Mode\")\n",
    "    print(\"    - Algo 1/2 simulations: 10000\")\n",
    "    print(\"    - Algo 3 simulations: 1000\")\n",
    "    # Leave 2 cores for system to prevent freezing\n",
    "    N_WORKERS = max(1, cpu_count() - 2)\n",
    "    N_SIM_ALGO12 = 10000\n",
    "    N_SIM_ALGO3 = 1000\n",
    "    print(f\"    - Workers: {N_WORKERS}\")\n",
    "\n",
    "# Path configuration\n",
    "BASE_INPUT_DIR = '../../data/ready'\n",
    "BASE_OUTPUT_DIR = '../../results/04_conj_enh_opp_sup/sf'\n",
    "CACHE_DIR = os.path.join(BASE_OUTPUT_DIR, 'cache_data')\n",
    "\n",
    "# Original large file path (containing 781 bodies)\n",
    "EPHEMERIS_FILE = os.path.join(BASE_INPUT_DIR, '781_planets_dwarfs_asteroids_lonlat.parquet')\n",
    "\n",
    "FILE_FLARE = 'flare_1975_2017.csv'  # Flare file name\n",
    "\n",
    "# Output file paths\n",
    "OUTPUT_FILE_ALGO1 = os.path.join(BASE_OUTPUT_DIR, 'sf_algo1_total_pairs.csv')\n",
    "OUTPUT_FILE_ALGO2 = os.path.join(BASE_OUTPUT_DIR, 'sf_algo2_at_least_one.csv')\n",
    "OUTPUT_FILE_ALGO3 = os.path.join(BASE_OUTPUT_DIR, 'sf_algo3_single_body_781.csv')\n",
    "OUTPUT_FILE_KUIPER = os.path.join(BASE_OUTPUT_DIR, 'sf_algo_kuiper_test.csv')\n",
    "\n",
    "# 8 major planets column names (for Algo 1 & 2)\n",
    "PLANET_COLS = ['199_lon', '299_lon', '399_lon', '499_lon', '599_lon', '699_lon', '799_lon', '899_lon']\n",
    "THRESHOLDS = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists(BASE_OUTPUT_DIR): os.makedirs(BASE_OUTPUT_DIR)\n",
    "if not os.path.exists(CACHE_DIR): os.makedirs(CACHE_DIR)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Common Utility Functions\n",
    "# =============================================================================\n",
    "\n",
    "# Intensity conversion function\n",
    "def convert_to_intensity(row):\n",
    "    \"\"\"Convert GOES flare class to relative intensity proxy\"\"\"\n",
    "    type_multiplier = {'A': 1, 'B': 10, 'C': 100, 'M': 1000, 'X': 10000}\n",
    "    try:\n",
    "        if 'xray_class' in row and not pd.isna(row['xray_class']):\n",
    "            raw = str(row['xray_class']).strip().upper()\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "        if len(raw) < 1: return np.nan\n",
    "        flare_type = raw[0]\n",
    "        \n",
    "        if flare_type not in type_multiplier: return np.nan\n",
    "        \n",
    "        try: \n",
    "            level_val = float(raw[1:]) if len(raw) > 1 else 1.0\n",
    "        except: \n",
    "            level_val = 1.0\n",
    "            \n",
    "        return type_multiplier[flare_type] * level_val\n",
    "    except: return np.nan\n",
    "\n",
    "# Classification function\n",
    "def categorize_flare(row_val):\n",
    "    \"\"\"Group by first letter of xray_class\"\"\"\n",
    "    if not isinstance(row_val, str): return 'Other'\n",
    "    first_char = row_val[0].upper()\n",
    "    if first_char in ['A', 'B', 'C', 'M', 'X']:\n",
    "        return f\"{first_char}-Class\"\n",
    "    return 'Other'\n",
    "\n",
    "# interpolate_angle function remains unchanged...\n",
    "\n",
    "def interpolate_angle(angle1, angle2, fraction):\n",
    "    \"\"\"Vectorized angle interpolation (handling 0-360 wrap)\"\"\"\n",
    "    rad1 = np.deg2rad(angle1)\n",
    "    rad2 = np.deg2rad(angle2)\n",
    "    # Calculate shortest path difference\n",
    "    delta = (rad2 - rad1 + np.pi) % (2 * np.pi) - np.pi\n",
    "    interpolated_rad = rad1 + fraction * delta\n",
    "    return np.degrees(interpolated_rad) % 360.0\n",
    "\n",
    "print(\"--- Configuration Loaded ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e80336-0139-4cc7-99d9-c5bb3b911440",
   "metadata": {},
   "source": [
    "## Step 1: Flare Data Pre-processing (Single File Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8e50a4-8464-482b-93fb-a5e6e4048913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 1: Flare Data Pre-processing\n",
      "============================================================\n",
      "Loading FULL Ephemeris from: ../../data/ready/781_planets_dwarfs_asteroids_lonlat.parquet ...\n",
      "Calculating Kepler probability maps...\n",
      "\n",
      "Processing Flare dataset: ../../data/ready/flare_1975_2017.csv ...\n",
      "  Calculating intensity proxy from xray_class...\n",
      "  Converted 39039/39039 intensity records.\n",
      "  [Data Clean] Removed 5 duplicate records.\n",
      "               (Exact/Logical duplicates dropped from 39039 -> 39034)\n",
      "  Interpolating 39034 records...\n",
      "  Saved 39034 flare records to: ../../results/04_conj_enh_opp_sup/sf/cache_data/ready_Flare_All.parquet\n",
      "Step 1 Completed. Time: 2.8s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Flare Data Pre-processing (Single File Processing)\n",
    "# =============================================================================\n",
    "\n",
    "def step1_data_preparation_optimized():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Step 1: Flare Data Pre-processing\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- 1. Load complete ephemeris (keep original sunspot code logic) ---\n",
    "    print(f\"Loading FULL Ephemeris from: {EPHEMERIS_FILE} ...\")\n",
    "    if not os.path.exists(EPHEMERIS_FILE):\n",
    "        print(\"Error: Ephemeris file not found!\")\n",
    "        return\n",
    "\n",
    "    df_ephem = pd.read_parquet(EPHEMERIS_FILE)\n",
    "    if 'date' in df_ephem.columns:\n",
    "        df_ephem['date'] = pd.to_datetime(df_ephem['date'])\n",
    "        df_ephem.set_index('date', inplace=True)\n",
    "    \n",
    "    df_ephem.index = df_ephem.index.normalize()\n",
    "    df_ephem = df_ephem[~df_ephem.index.duplicated(keep='first')]\n",
    "    df_ephem.sort_index(inplace=True)\n",
    "    \n",
    "    all_body_cols = [c for c in df_ephem.columns if str(c).endswith('_lon')]\n",
    "    ephem_matrix_all = df_ephem[all_body_cols].values.astype(np.float32)\n",
    "    \n",
    "    # Save cache (needed for Algo 1/2)\n",
    "    path_matrix_8p = os.path.join(CACHE_DIR, 'ephem_matrix_8p.npy')\n",
    "    np.save(path_matrix_8p, df_ephem[PLANET_COLS].values.astype(np.float64))\n",
    "    \n",
    "    print(\"Calculating Kepler probability maps...\")\n",
    "    prob_maps = {}\n",
    "    for col in PLANET_COLS:\n",
    "        hist, _ = np.histogram(df_ephem[col], bins=360, range=(0, 360), density=True)\n",
    "        prob_maps[col] = hist\n",
    "    with open(os.path.join(CACHE_DIR, 'kepler_prob_maps.pkl'), 'wb') as f:\n",
    "        pickle.dump(prob_maps, f)\n",
    "\n",
    "    # 2. Process flare file\n",
    "    path_flare = os.path.join(BASE_INPUT_DIR, FILE_FLARE) # Ensure Cell 1 defined FILE_FLARE\n",
    "    print(f\"\\nProcessing Flare dataset: {path_flare} ...\")\n",
    "    \n",
    "    if not os.path.exists(path_flare):\n",
    "        print(f\"Error: file not found!\")\n",
    "        return\n",
    "\n",
    "    # Read data\n",
    "    df_flare = pd.read_csv(path_flare)\n",
    "    \n",
    "    # [Key Mapping 1] Time\n",
    "    df_flare['date'] = pd.to_datetime(df_flare['datetime_start'])\n",
    "    \n",
    "    # [Key Mapping 2] Calculate intensity proxy (assign to area)\n",
    "    print(\"  Calculating intensity proxy from xray_class...\")\n",
    "    # Use the function just defined in Cell 1\n",
    "    df_flare['area'] = df_flare.apply(convert_to_intensity, axis=1)\n",
    "    \n",
    "    # Check conversion results\n",
    "    valid_area_count = df_flare['area'].notna().sum()\n",
    "    print(f\"  Converted {valid_area_count}/{len(df_flare)} intensity records.\")\n",
    "    \n",
    "    # Remove rows with failed conversion (i.e., rows without valid xray_class)\n",
    "    df_flare.dropna(subset=['area', 'hme_lon', 'date'], inplace=True)\n",
    "\n",
    "    # Filter time range\n",
    "    min_date, max_date = df_ephem.index.min(), df_ephem.index.max()\n",
    "    df_flare = df_flare[(df_flare['date'] >= min_date) & (df_flare['date'] <= max_date)].copy()\n",
    "    \n",
    "    # Generate Group (using xray_class)\n",
    "    df_flare['Group'] = df_flare['xray_class'].apply(categorize_flare)\n",
    "    df_flare = df_flare[df_flare['Group'] != 'Other']\n",
    "\n",
    "    # Filter invalid data\n",
    "    df_flare.dropna(subset=['hme_lon', 'date'], inplace=True)\n",
    "\n",
    "    # [New] Critical deduplication step\n",
    "    len_before = len(df_flare)\n",
    "    \n",
    "    # 1. Physical deduplication: if time, class, and longitude are all the same, treat as same event\n",
    "    # keep='first' means keep the first occurrence and delete subsequent duplicates\n",
    "    df_flare.drop_duplicates(subset=['date', 'xray_class', 'hme_lon'], inplace=True)\n",
    "    \n",
    "    len_after = len(df_flare)\n",
    "    n_dropped = len_before - len_after\n",
    "    \n",
    "    print(f\"  [Data Clean] Removed {n_dropped} duplicate records.\")\n",
    "    if n_dropped > 0:\n",
    "        print(f\"               (Exact/Logical duplicates dropped from {len_before} -> {len_after})\")\n",
    "    \n",
    "    # Filter time range\n",
    "    min_date, max_date = df_ephem.index.min(), df_ephem.index.max()\n",
    "    df_flare = df_flare[(df_flare['date'] >= min_date) & (df_flare['date'] <= max_date)].copy()\n",
    "    \n",
    "    # Generate Group (for Algo 1/2)\n",
    "    df_flare['Group'] = df_flare['xray_class'].apply(categorize_flare)\n",
    "    df_flare = df_flare[df_flare['Group'] != 'Other'] # Filter miscellaneous\n",
    "\n",
    "    # --- Interpolation calculation (keep unchanged) ---\n",
    "    print(f\"  Interpolating {len(df_flare)} records...\")\n",
    "    \n",
    "    day_t = df_flare['date'].dt.normalize()\n",
    "    fraction = (df_flare['date'] - day_t).dt.total_seconds() / 86400.0\n",
    "    idx_t = df_ephem.index.searchsorted(day_t)\n",
    "    \n",
    "    valid_mask = (idx_t < len(df_ephem) - 1)\n",
    "    if not valid_mask.all():\n",
    "        df_flare = df_flare[valid_mask]\n",
    "        idx_t = idx_t[valid_mask]\n",
    "        fraction = fraction[valid_mask]\n",
    "        \n",
    "    pos_t = ephem_matrix_all[idx_t]\n",
    "    pos_t_plus_1 = ephem_matrix_all[idx_t + 1]\n",
    "    fraction_vals = fraction.values[:, np.newaxis].astype(np.float32)\n",
    "    \n",
    "    interpolated_matrix = interpolate_angle(pos_t, pos_t_plus_1, fraction_vals)\n",
    "    \n",
    "    # Construct result (including calculated area)\n",
    "    df_result = df_flare.copy()\n",
    "    df_result['ephem_idx_daily'] = idx_t\n",
    "    \n",
    "    df_bodies = pd.DataFrame(interpolated_matrix, columns=all_body_cols, index=df_result.index)\n",
    "    df_final = pd.concat([df_result, df_bodies], axis=1)\n",
    "    \n",
    "    # Save\n",
    "    save_path = os.path.join(CACHE_DIR, 'ready_Flare_All.parquet')\n",
    "    df_final.to_parquet(save_path)\n",
    "    print(f\"  Saved {len(df_final)} flare records to: {save_path}\")\n",
    "    print(f\"Step 1 Completed. Time: {time.time() - start_time:.1f}s\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    step1_data_preparation_optimized()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddacbc1-06e4-41cd-9a9d-dc2a0bf85502",
   "metadata": {},
   "source": [
    "## Step 2: Algorithm 1 (Total Pairs Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc584bd-3747-4918-a5dd-dbdc59074ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 2: Running Algo 1 | Sim: 10000 | Workers: 30\n",
      "============================================================\n",
      "Processing Stage: Flare_All ...\n",
      "  Group: B-Class (N=7127)\n",
      "  Group: C-Class (N=26654)\n",
      "  Group: M-Class (N=4824)\n",
      "  Group: X-Class (N=429)\n",
      "  Group: Total (N=39034)\n",
      "Step 2 Completed. Time: 106.4s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Algorithm 1 (Total Pairs Method)\n",
    "# =============================================================================\n",
    "\n",
    "def step2_run_algo1():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Step 2: Running Algo 1 | Sim: {N_SIM_ALGO12} | Workers: {N_WORKERS}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # [New] Prevent duplicate appends from repeated Jupyter runs\n",
    "    if os.path.exists(OUTPUT_FILE_ALGO1):\n",
    "        os.remove(OUTPUT_FILE_ALGO1)\n",
    "        print(f\"Warning: Removed existing output file {OUTPUT_FILE_ALGO1} to avoid duplicates.\")\n",
    "    \n",
    "    # Load cache\n",
    "    try:\n",
    "        # Load daily position matrix for 8 major planets (for CTS fast lookup)\n",
    "        ephem_matrix_daily = np.load(os.path.join(CACHE_DIR, 'ephem_matrix_8p.npy'))\n",
    "        with open(os.path.join(CACHE_DIR, 'kepler_prob_maps.pkl'), 'rb') as f:\n",
    "            prob_maps = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Cache files not found. Please run Step 1 first.\")\n",
    "        return\n",
    "\n",
    "    results_buffer = []\n",
    "    files = [f for f in os.listdir(CACHE_DIR) if f.startswith('ready_') and f.endswith('.parquet')]\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for f in files:\n",
    "        stage_name = f.replace('ready_', '').replace('.parquet', '')\n",
    "        print(f\"Processing Stage: {stage_name} ...\")\n",
    "        \n",
    "        # Read data (only need 8 major planet columns)\n",
    "        cols_to_load = ['hme_lon', 'ephem_idx_daily', 'Group'] + PLANET_COLS\n",
    "        df = pd.read_parquet(os.path.join(CACHE_DIR, f), columns=cols_to_load)\n",
    "        \n",
    "        groups = sorted(df['Group'].unique(), key=lambda x: ('BCMX'.find(x[0]) if x[0] in 'BCMX' else 99, x))\n",
    "        groups.append('Total')\n",
    "        \n",
    "        for group in groups:\n",
    "            subset = df if group == 'Total' else df[df['Group'] == group]\n",
    "            if subset.empty: continue\n",
    "            \n",
    "            # Prepare core data\n",
    "            sun_lons = subset['hme_lon'].values.astype(np.float64)\n",
    "            obs_planets_interp = subset[PLANET_COLS].values.astype(np.float64) # Already interpolated high-precision data\n",
    "            sun_idxs_daily = subset['ephem_idx_daily'].values.astype(int)      # CTS uses integer indices\n",
    "            n_recs = len(subset)\n",
    "            \n",
    "            print(f\"  Group: {group} (N={n_recs})\")\n",
    "            \n",
    "            for w in THRESHOLDS:\n",
    "                for etype in ['Opposition', 'Conjunction']:\n",
    "                    \n",
    "                    # --- 1. Calculate observed value (k_obs) ---\n",
    "                    # Call vectorized function in algo_workers\n",
    "                    if algo_workers:\n",
    "                        k_obs = algo_workers.count_events_vectorized(sun_lons, obs_planets_interp, w, etype)\n",
    "                    else:\n",
    "                        # Inline fallback logic\n",
    "                        if etype == 'Conjunction':\n",
    "                            delta = np.abs(sun_lons[:, None] - obs_planets_interp)\n",
    "                            delta = np.where(delta > 180, 360 - delta, delta)\n",
    "                            k_obs = np.sum(delta <= w)\n",
    "                        else:\n",
    "                            delta = np.abs(np.abs(sun_lons[:, None] - obs_planets_interp) - 180)\n",
    "                            k_obs = np.sum(delta <= w)\n",
    "                    \n",
    "                    # --- 2. Prepare CTS simulation ---\n",
    "                    seeds = np.random.randint(0, 1000000000, N_SIM_ALGO12)\n",
    "                    args = [(seed, sun_lons, ephem_matrix_daily, sun_idxs_daily, w, etype) for seed in seeds]\n",
    "                    \n",
    "                    # --- 3. Execute simulation (k_sims) ---\n",
    "                    # Must use algo_workers.cts_worker_algo1 for multi-core pickle support\n",
    "                    if N_WORKERS > 1 and algo_workers:\n",
    "                        with Pool(N_WORKERS) as pool:\n",
    "                            k_sims = pool.starmap(algo_workers.cts_worker_algo1, args)\n",
    "                    else:\n",
    "                        # Single core or no external worker\n",
    "                        if algo_workers:\n",
    "                             k_sims = [algo_workers.cts_worker_algo1(*a) for a in args]\n",
    "                        else:\n",
    "                             # Simple placeholder, if no algo_workers file cannot perform CTS\n",
    "                             print(\"Warning: algo_workers.py missing, skipping simulations.\")\n",
    "                             k_sims = [k_obs] * N_SIM_ALGO12\n",
    "\n",
    "                    k_sims = np.array(k_sims)\n",
    "                    \n",
    "                    # --- 4. Statistical calculation ---\n",
    "                    # --- [Modified] P-value calculation (two-tailed + effect direction) ---\n",
    "                    # Calculate left tail (suppression) probability\n",
    "                    p_left = (np.sum(k_sims <= k_obs) + 1) / (N_SIM_ALGO12 + 1)\n",
    "                    # Calculate right tail (enhancement) probability\n",
    "                    p_right = (np.sum(k_sims >= k_obs) + 1) / (N_SIM_ALGO12 + 1)\n",
    "                    \n",
    "                    # Two-tailed P-value (double the minimum, capped at 1.0)\n",
    "                    p_val = 2 * min(p_left, p_right)\n",
    "                    if p_val > 1.0: p_val = 1.0\n",
    "                    \n",
    "                    # Record effect direction\n",
    "                    effect = 'Suppression' if k_obs < k_sims.mean() else 'Enhancement'\n",
    "\n",
    "                    if k_sims.std() == 0: z_score = 0\n",
    "                    else: z_score = (k_obs - k_sims.mean()) / k_sims.std()\n",
    "                    \n",
    "                    # --- 5. Theoretical baseline (Kepler Baseline) ---\n",
    "                    # Calculate precise expectation using Kepler Prob Maps\n",
    "                    lon_indices = np.floor(sun_lons).astype(int) % 360\n",
    "                    target_indices = (lon_indices + 180) % 360 if etype == 'Opposition' else lon_indices\n",
    "                    k_exp = 0\n",
    "                    for col in PLANET_COLS:\n",
    "                        k_exp += np.sum(prob_maps[col][target_indices] * (2 * w))\n",
    "                    \n",
    "                    ratio = (k_obs / k_exp * 100) if k_exp > 0 else 0\n",
    "                    \n",
    "                    # --- [Important] Remember to add 'Effect': effect to the dict ---\n",
    "                    results_buffer.append({\n",
    "                        'Stage': stage_name, 'Group': group, 'Window': w, 'Type': etype,\n",
    "                        'N_Records': n_recs, 'k_obs': k_obs, 'k_exp': round(k_exp, 2),\n",
    "                        'Ratio': round(ratio, 2), \n",
    "                        'p_val': p_val, \n",
    "                        'Z_score': round(z_score, 2),\n",
    "                        'Effect': effect  # <--- Add this line!\n",
    "                    })\n",
    "        \n",
    "        # Stage save\n",
    "        if results_buffer:\n",
    "            new_df = pd.DataFrame(results_buffer)\n",
    "            write_header = not os.path.exists(OUTPUT_FILE_ALGO1)\n",
    "            new_df.to_csv(OUTPUT_FILE_ALGO1, mode='a', header=write_header, index=False)\n",
    "            results_buffer = []\n",
    "\n",
    "    print(f\"Step 2 Completed. Time: {time.time()-total_start_time:.1f}s\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    step2_run_algo1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77cd242-cb6b-4184-9305-8941162aa1ca",
   "metadata": {},
   "source": [
    "## Step 3: Algorithm 2 (Trigger Mechanism - At Least One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954168c1-0f46-4abc-a2fc-363ebff31e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 3: Running Algo 2 | Sim: 10000 | Workers: 30\n",
      "============================================================\n",
      "Processing Stage: Flare_All ...\n",
      "  Group: B-Class (N=7127)\n",
      "  Group: C-Class (N=26654)\n",
      "  Group: M-Class (N=4824)\n",
      "  Group: X-Class (N=429)\n",
      "  Group: Total (N=39034)\n",
      "Step 3 Completed. Time: 109.8s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 3: Algorithm 2 (Trigger Mechanism - At Least One)\n",
    "# =============================================================================\n",
    "\n",
    "def step3_run_algo2():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Step 3: Running Algo 2 | Sim: {N_SIM_ALGO12} | Workers: {N_WORKERS}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # [New] Prevent duplicate appends from repeated Jupyter runs (for Algo 2 file)\n",
    "    if os.path.exists(OUTPUT_FILE_ALGO2):\n",
    "        os.remove(OUTPUT_FILE_ALGO2)\n",
    "        print(f\"Warning: Removed existing output file {OUTPUT_FILE_ALGO2} to avoid duplicates.\")\n",
    "    \n",
    "    try:\n",
    "        ephem_matrix_daily = np.load(os.path.join(CACHE_DIR, 'ephem_matrix_8p.npy'))\n",
    "        with open(os.path.join(CACHE_DIR, 'kepler_prob_maps.pkl'), 'rb') as f:\n",
    "            prob_maps = pickle.load(f)\n",
    "    except FileNotFoundError: return\n",
    "            \n",
    "    results_buffer = []\n",
    "    files = [f for f in os.listdir(CACHE_DIR) if f.startswith('ready_') and f.endswith('.parquet')]\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for f in files:\n",
    "        stage_name = f.replace('ready_', '').replace('.parquet', '')\n",
    "        print(f\"Processing Stage: {stage_name} ...\")\n",
    "        \n",
    "        cols_to_load = ['hme_lon', 'ephem_idx_daily', 'Group'] + PLANET_COLS\n",
    "        df = pd.read_parquet(os.path.join(CACHE_DIR, f), columns=cols_to_load)\n",
    "        \n",
    "        groups = sorted(df['Group'].unique(), key=lambda x: ('BCMX'.find(x[0]) if x[0] in 'BCMX' else 99, x))\n",
    "        groups.append('Total')\n",
    "        \n",
    "        for group in groups:\n",
    "            subset = df if group == 'Total' else df[df['Group'] == group]\n",
    "            if subset.empty: continue\n",
    "            \n",
    "            sun_lons = subset['hme_lon'].values.astype(np.float64)\n",
    "            obs_planets_interp = subset[PLANET_COLS].values.astype(np.float64)\n",
    "            sun_idxs_daily = subset['ephem_idx_daily'].values.astype(int)\n",
    "            n_recs = len(subset)\n",
    "            \n",
    "            print(f\"  Group: {group} (N={n_recs})\")\n",
    "            \n",
    "            for w in THRESHOLDS:\n",
    "                for etype in ['Opposition', 'Conjunction']:\n",
    "                    \n",
    "                    # --- 1. Calculate observed value (k_obs) ---\n",
    "                    if algo_workers:\n",
    "                        k_obs = algo_workers.count_events_at_least_once(sun_lons, obs_planets_interp, w, etype)\n",
    "                    else:\n",
    "                        # Inline fallback\n",
    "                        if etype == 'Conjunction':\n",
    "                            delta = np.abs(sun_lons[:, None] - obs_planets_interp)\n",
    "                            delta = np.where(delta > 180, 360 - delta, delta)\n",
    "                            is_event = np.any(delta <= w, axis=1)\n",
    "                        else:\n",
    "                            delta = np.abs(np.abs(sun_lons[:, None] - obs_planets_interp) - 180)\n",
    "                            is_event = np.any(delta <= w, axis=1)\n",
    "                        k_obs = np.sum(is_event)\n",
    "                    \n",
    "                    # --- 2. Simulation ---\n",
    "                    seeds = np.random.randint(0, 1000000000, N_SIM_ALGO12)\n",
    "                    args = [(seed, sun_lons, ephem_matrix_daily, sun_idxs_daily, w, etype) for seed in seeds]\n",
    "                    \n",
    "                    if N_WORKERS > 1 and algo_workers:\n",
    "                        with Pool(N_WORKERS) as pool:\n",
    "                            k_sims = pool.starmap(algo_workers.cts_worker_algo2, args)\n",
    "                    elif algo_workers:\n",
    "                        k_sims = [algo_workers.cts_worker_algo2(*arg) for arg in args]\n",
    "                    else:\n",
    "                        k_sims = [k_obs] * N_SIM_ALGO12\n",
    "                            \n",
    "                    k_sims = np.array(k_sims)\n",
    "                    \n",
    "                    # --- 3. Statistics ---\n",
    "                    # --- [Modified] P-value calculation (two-tailed + effect direction) ---\n",
    "                    # Calculate left tail (suppression) probability\n",
    "                    p_left = (np.sum(k_sims <= k_obs) + 1) / (N_SIM_ALGO12 + 1)\n",
    "                    # Calculate right tail (enhancement) probability\n",
    "                    p_right = (np.sum(k_sims >= k_obs) + 1) / (N_SIM_ALGO12 + 1)\n",
    "                    \n",
    "                    # Two-tailed P-value\n",
    "                    p_val = 2 * min(p_left, p_right)\n",
    "                    if p_val > 1.0: p_val = 1.0\n",
    "                    \n",
    "                    # Record effect direction\n",
    "                    effect = 'Suppression' if k_obs < k_sims.mean() else 'Enhancement'\n",
    "\n",
    "                    if k_sims.std() == 0: z_score = 0\n",
    "                    else: z_score = (k_obs - k_sims.mean()) / k_sims.std()\n",
    "                    \n",
    "                    # --- 4. Theoretical baseline (Prob At Least One) ---\n",
    "                    lon_indices = np.floor(sun_lons).astype(int) % 360\n",
    "                    target_indices = (lon_indices + 180) % 360 if etype == 'Opposition' else lon_indices\n",
    "                    \n",
    "                    p_mat = np.zeros((n_recs, len(PLANET_COLS)))\n",
    "                    for i, col in enumerate(PLANET_COLS):\n",
    "                        p_mat[:, i] = prob_maps[col][target_indices] * (2 * w)\n",
    "                    \n",
    "                    # P(At Least One) = 1 - Prod(1 - P_i)\n",
    "                    p_at_least_one = 1.0 - np.prod(1.0 - p_mat, axis=1)\n",
    "                    k_exp = np.sum(p_at_least_one)\n",
    "                    \n",
    "                    ratio = (k_obs / k_exp * 100) if k_exp > 0 else 0\n",
    "                 \n",
    "                    # --- [Important] Remember to add 'Effect': effect to the dict ---\n",
    "                    results_buffer.append({\n",
    "                        'Stage': stage_name, 'Group': group, 'Window': w, 'Type': etype,\n",
    "                        'N_Records': n_recs, 'k_obs': k_obs, 'k_exp': round(k_exp, 2),\n",
    "                        'Ratio': round(ratio, 2), \n",
    "                        'p_val': p_val, \n",
    "                        'Z_score': round(z_score, 2),\n",
    "                        'Effect': effect  # <--- Add this line!\n",
    "                    })\n",
    "        \n",
    "        if results_buffer:\n",
    "            new_df = pd.DataFrame(results_buffer)\n",
    "            write_header = not os.path.exists(OUTPUT_FILE_ALGO2)\n",
    "            new_df.to_csv(OUTPUT_FILE_ALGO2, mode='a', header=write_header, index=False)\n",
    "            results_buffer = []\n",
    "\n",
    "    print(f\"Step 3 Completed. Time: {time.time()-total_start_time:.1f}s\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    step3_run_algo2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c7729-b762-4f7e-a0e5-84a2f54e1cca",
   "metadata": {},
   "source": [
    "## Step 4: Algorithm 3 (Single Body Complete Version) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd534e6-562e-4b02-ace6-dcdacf160810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 4: Running Algo 3 (Final) - FORCE SINGLE CORE MODE\n",
      "============================================================\n",
      "\n",
      "Processing Stage: Flare_All ...\n",
      "  Analyzing 781 bodies against 39034 records...\n",
      "  [Mode] Executing in Serial Mode (No Multiprocessing)...\n",
      "  Processed 781/781 bodies. Done.\n",
      "  Saved 7810 rows to ../../results/04_conj_enh_opp_sup/sf/sf_algo3_single_body_781.csv\n",
      "Step 4 Completed. Time: 1.3s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 4: Algorithm 3 (Single Body Complete Version) - Frequency + Amplitude + P-value\n",
    "# =============================================================================\n",
    "\n",
    "def worker_algo3_final(args):\n",
    "    \"\"\"\n",
    "    Final Version Worker (Fixed): \n",
    "    1. Dynamically calculate Kepler probability (solve eccentricity problem)\n",
    "    2. Two-tailed P-value test\n",
    "    3. Record Enhancement/Suppression direction\n",
    "    \"\"\"\n",
    "    body_name, body_lons, sun_lons, sun_areas, thresholds, n_sims = args\n",
    "    results = []\n",
    "    n_recs = len(sun_lons)\n",
    "    \n",
    "    # Global average area (for calculating amplitude ratio)\n",
    "    global_avg_area = np.mean(sun_areas) if n_recs > 0 else 0\n",
    "    \n",
    "    # --- [Key Fix 1] Calculate Kepler probability distribution (histogram) on-site for this body ---\n",
    "    # Count the occurrence frequency (density) of this body at each degree 0-360, reflecting its dwell time at each orbit position\n",
    "    # density=True ensures sum * bin_width = 1\n",
    "    hist_prob, _ = np.histogram(body_lons, bins=360, range=(0, 360), density=True)\n",
    "    \n",
    "    # Pre-calculate integer indices of sun positions for fast table lookup\n",
    "    sun_idx_conj = np.floor(sun_lons).astype(int) % 360\n",
    "    sun_idx_opp  = (sun_idx_conj + 180) % 360\n",
    "    \n",
    "    for w in thresholds:\n",
    "        # [Removed] Old uniform distribution assumption: p0 = (2 * w) / 360.0 \n",
    "        \n",
    "        for etype in ['Opposition', 'Conjunction']:\n",
    "            # --- A. Basic calculation (keep unchanged) ---\n",
    "            if etype == 'Conjunction':\n",
    "                d = np.abs(sun_lons - body_lons)\n",
    "                d = np.where(d > 180, 360 - d, d)\n",
    "                is_event = (d <= w)\n",
    "                \n",
    "                # [Key Fix 1] Get background probability only at conjunction moments\n",
    "                target_indices = sun_idx_conj\n",
    "                \n",
    "            else: # Opposition\n",
    "                d = np.abs(np.abs(sun_lons - body_lons) - 180)\n",
    "                is_event = (d <= w)\n",
    "                \n",
    "                target_indices = sun_idx_opp\n",
    "            \n",
    "            k_obs = np.sum(is_event)\n",
    "            \n",
    "            # --- B. Calculate precise expectation (k_exp) based on Kepler distribution ---\n",
    "            # Table lookup: hist_prob[idx] is probability density within 1 degree\n",
    "            # Multiply by (2*w) to get total probability within window\n",
    "            # Sum over all records to get expected frequency\n",
    "            daily_probs = hist_prob[target_indices] * (2 * w)\n",
    "            k_exp = np.sum(daily_probs)\n",
    "            \n",
    "            # Calculate frequency ratio\n",
    "            ratio_freq = (k_obs / k_exp * 100) if k_exp > 0 else 0\n",
    "            \n",
    "            # --- C. Amplitude indicator (keep unchanged) ---\n",
    "            if k_obs > 0:\n",
    "                event_avg_area = np.mean(sun_areas[is_event])\n",
    "                ratio_amp = (event_avg_area / global_avg_area * 100) if global_avg_area > 0 else 0\n",
    "            else:\n",
    "                event_avg_area = 0\n",
    "                ratio_amp = 0\n",
    "            \n",
    "            # --- D. Significance P-value (Two-tailed fixed version) ---\n",
    "            # Since probability varies for each trial, binomial distribution is strictly not applicable.\n",
    "            # But as a robust approximation, we use average probability p_avg.\n",
    "            p_avg = k_exp / n_recs if n_recs > 0 else 0\n",
    "            \n",
    "            # 1. Calculate two-tailed probability\n",
    "            p_left = stats.binom.cdf(k_obs, n_recs, p_avg)\n",
    "            p_right = stats.binom.sf(k_obs - 1, n_recs, p_avg)\n",
    "            p_val = 2 * min(p_left, p_right)\n",
    "            if p_val > 1.0: p_val = 1.0\n",
    "            \n",
    "            # 2. [Key Fix 2] Record effect direction\n",
    "            effect = 'Suppression' if k_obs < k_exp else 'Enhancement'\n",
    "              \n",
    "            results.append({\n",
    "                'Body': body_name,\n",
    "                'Window': w,\n",
    "                'Type': etype,\n",
    "                'k_obs': k_obs,\n",
    "                'k_exp': round(k_exp, 2),\n",
    "                'Ratio_Freq': round(ratio_freq, 2),\n",
    "                'Avg_Area': round(event_avg_area, 2),\n",
    "                'Ratio_Amp': round(ratio_amp, 2),\n",
    "                'p_val': p_val,\n",
    "                'Effect': effect  # Must include this column\n",
    "            })\n",
    "    return results\n",
    "\n",
    "def step4_run_algo3_final():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Step 4: Running Algo 3 (Final) - FORCE SINGLE CORE MODE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # [CRITICAL CONFIG] Force single-core execution\n",
    "    # We explicitly set this to 1 to bypass global settings.\n",
    "    # This avoids Windows multiprocessing pickling errors and IPC deadlocks.\n",
    "    # Since the calculation is vector-based and fast, serial execution is optimal.\n",
    "    FORCED_WORKERS = 1 \n",
    "    \n",
    "    # Remove existing output file to prevent duplicate appending from previous runs\n",
    "    if os.path.exists(OUTPUT_FILE_ALGO3):\n",
    "        os.remove(OUTPUT_FILE_ALGO3)\n",
    "        print(f\"Warning: Removed existing output file {OUTPUT_FILE_ALGO3} to avoid duplicates.\")\n",
    "    \n",
    "    # Get list of input files from cache directory\n",
    "    files = [f for f in os.listdir(CACHE_DIR) if f.startswith('ready_') and f.endswith('.parquet')]\n",
    "    \n",
    "    total_start = time.time()\n",
    "    \n",
    "    for f in files:\n",
    "        stage_name = f.replace('ready_', '').replace('.parquet', '')\n",
    "        print(f\"\\nProcessing Stage: {stage_name} ...\")\n",
    "        \n",
    "        # Read data\n",
    "        df = pd.read_parquet(os.path.join(CACHE_DIR, f))\n",
    "        \n",
    "        # Identify columns: exclude metadata, keep planetary/body columns\n",
    "        meta_cols = {'date', 'hme_lon', 'area', 'ephem_idx_daily', 'Group', 'lat_lon', 'hg_lon', 'hgc_lon', 'lon', 'lat'}\n",
    "        body_cols = [c for c in df.columns if c not in meta_cols and c.endswith('_lon')]\n",
    "        \n",
    "        print(f\"  Analyzing {len(body_cols)} bodies against {len(df)} records...\")\n",
    "        \n",
    "        # Prepare data arrays\n",
    "        sun_lons = df['hme_lon'].values.astype(np.float32)\n",
    "        sun_areas = df['area'].values.astype(np.float32)\n",
    "        \n",
    "        # Create task list\n",
    "        # Note: Passing large arrays (sun_lons) in single-core mode is efficient \n",
    "        # because Python passes by reference (no memory copying/pickling involved).\n",
    "        tasks = []\n",
    "        for col in body_cols:\n",
    "            body_data = df[col].values.astype(np.float32)\n",
    "            tasks.append((col, body_data, sun_lons, sun_areas, THRESHOLDS, N_SIM_ALGO3))\n",
    "            \n",
    "        results_flat = []\n",
    "        \n",
    "        # [CORE LOGIC] Execute in Serial Mode\n",
    "        # We loop directly in the main process. No overhead, no deadlocks.\n",
    "        print(\"  [Mode] Executing in Serial Mode (No Multiprocessing)...\")\n",
    "        \n",
    "        for i, t in enumerate(tasks):\n",
    "            # Execute the worker function directly\n",
    "            results_flat.extend(worker_algo3_final(t))\n",
    "            \n",
    "        print(f\"  Processed {len(tasks)}/{len(tasks)} bodies. Done.\")\n",
    "\n",
    "        # Save results to CSV\n",
    "        if results_flat:\n",
    "            df_res = pd.DataFrame(results_flat)\n",
    "            df_res['Stage'] = stage_name\n",
    "            \n",
    "            # Write header only if file does not exist\n",
    "            hdr = not os.path.exists(OUTPUT_FILE_ALGO3)\n",
    "            df_res.to_csv(OUTPUT_FILE_ALGO3, mode='a', header=hdr, index=False)\n",
    "            print(f\"  Saved {len(df_res)} rows to {OUTPUT_FILE_ALGO3}\")\n",
    "            \n",
    "    print(f\"Step 4 Completed. Time: {time.time() - total_start:.1f}s\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    step4_run_algo3_final()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6999de3-62ff-4248-838e-27116cd222ed",
   "metadata": {},
   "source": [
    "# Step 5: physical trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d6ec282-8090-43db-967a-bb7163620a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[Algo 1: Total Pairs] Physical Trend Verification Report (No FDR Penalty)\n",
      "Core Logic: Check ratio trends for different flare classes (C/M/X) within 1-5 degree windows.\n",
      "================================================================================\n",
      "Window                           1               2               3               4               5\n",
      "Group   Type                                                                                      \n",
      "B-Class Conjunction   112% (0.557)    114% (0.435)    115% (0.366)    117% (0.246)    118% (0.188)\n",
      "        Opposition     78% (0.063)     83% (0.160)     83% (0.128)     83% (0.120)     84% (0.130)\n",
      "C-Class Conjunction  124% (0.015*)  127% (0.000**)  124% (0.003**)  123% (0.006**)  123% (0.002**)\n",
      "        Opposition     90% (0.821)     88% (0.513)     88% (0.473)     88% (0.553)     89% (0.733)\n",
      "M-Class Conjunction   116% (0.334)    117% (0.249)   122% (0.043*)    120% (0.073)    116% (0.236)\n",
      "        Opposition    103% (0.883)     97% (0.967)     92% (0.845)     92% (0.833)     92% (0.883)\n",
      "Total   Conjunction   121% (0.060)  123% (0.003**)  122% (0.003**)  121% (0.008**)  121% (0.004**)\n",
      "        Opposition     90% (0.778)     88% (0.506)     87% (0.412)     88% (0.477)     88% (0.667)\n",
      "X-Class Conjunction   110% (0.722)    110% (0.666)    110% (0.643)    106% (0.778)    105% (0.814)\n",
      "        Opposition     89% (0.789)     66% (0.081)     84% (0.424)     89% (0.600)     87% (0.514)\n",
      "\n",
      "[Interpretation Guide]\n",
      "  1. Look for cells with * or ** in M-Class or X-Class rows.\n",
      "  2. Verify trends: Is Conjunction Ratio > 100%? Is Opposition Ratio < 100%?\n",
      "  3. If the effect for large flares (M/X) is stronger than for small flares (C), the physical link is established.\n",
      "\n",
      "================================================================================\n",
      "[Algo 2: At Least One] Physical Trend Verification Report (No FDR Penalty)\n",
      "Core Logic: Check ratio trends for different flare classes (C/M/X) within 1-5 degree windows.\n",
      "================================================================================\n",
      "Window                           1               2              3             4             5\n",
      "Group   Type                                                                                 \n",
      "B-Class Conjunction   111% (0.577)    112% (0.496)   111% (0.543)  112% (0.493)  114% (0.342)\n",
      "        Opposition     79% (0.062)     83% (0.119)    83% (0.100)   83% (0.067)   84% (0.082)\n",
      "C-Class Conjunction  123% (0.025*)  124% (0.003**)  120% (0.030*)  119% (0.074)  118% (0.074)\n",
      "        Opposition     90% (0.776)     88% (0.414)    87% (0.296)   88% (0.312)   88% (0.313)\n",
      "M-Class Conjunction   113% (0.500)    114% (0.403)   118% (0.104)  115% (0.207)  111% (0.576)\n",
      "        Opposition    106% (0.792)     97% (0.970)    92% (0.787)   90% (0.660)   90% (0.651)\n",
      "Total   Conjunction   119% (0.086)   121% (0.018*)   118% (0.065)  117% (0.124)  116% (0.144)\n",
      "        Opposition     90% (0.765)     88% (0.411)    87% (0.218)   87% (0.192)   88% (0.203)\n",
      "X-Class Conjunction   112% (0.657)    109% (0.696)   111% (0.572)  109% (0.634)  108% (0.668)\n",
      "        Opposition     91% (0.847)     68% (0.106)    76% (0.173)   82% (0.280)   83% (0.294)\n",
      "\n",
      "[Interpretation Guide]\n",
      "  1. Look for cells with * or ** in M-Class or X-Class rows.\n",
      "  2. Verify trends: Is Conjunction Ratio > 100%? Is Opposition Ratio < 100%?\n",
      "  3. If the effect for large flares (M/X) is stronger than for small flares (C), the physical link is established.\n",
      "\n",
      "================================================================================\n",
      "[Algo 3] Performing Fair FDR Correction (for 781 bodies)\n",
      "================================================================================\n",
      "Correction completed. A total of 335 significant body records passing FDR were found across all scans.\n",
      "Please open the CSV to view rows where 'sig_fdr' is True, focusing on non-Earth (399) asteroids.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Configuration Paths (Automatically read BASE_OUTPUT_DIR from above)\n",
    "# =============================================================================\n",
    "# Use variables defined at the beginning of the script to ensure path consistency\n",
    "BASE_DIR = BASE_OUTPUT_DIR \n",
    "\n",
    "# Note: Filenames may need modification based on your flare code (e.g., prefix 'sf_' or 'sg_')\n",
    "# Please check the OUTPUT_FILE_ALGO1 variable names defined in Step 2/3/4\n",
    "FILE_ALGO1 = OUTPUT_FILE_ALGO1\n",
    "FILE_ALGO2 = OUTPUT_FILE_ALGO2\n",
    "FILE_ALGO3 = OUTPUT_FILE_ALGO3\n",
    "\n",
    "def analyze_physics_trend(csv_file, algo_name):\n",
    "    \"\"\"\n",
    "    For Algo 1 & 2: \n",
    "    Do not use FDR (treated as N=1 sensitivity analysis), but check for consistency in physical trends (Conjunction-Enhancement, Opposition-Suppression).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"File not found: {csv_file}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[{algo_name}] Physical Trend Verification Report (No FDR Penalty)\")\n",
    "    print(f\"Core Logic: Check ratio trends for different flare classes (C/M/X) within 1-5 degree windows.\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # 1. Physical Filtering: Ensure only existing groups are analyzed (usually C, M, X Class)\n",
    "    # Automatically detect groups present in the data\n",
    "    available_groups = sorted(df['Group'].unique())\n",
    "    \n",
    "    if not available_groups:\n",
    "        print(\"No group data found.\")\n",
    "        return\n",
    "        \n",
    "    df_target = df.copy()\n",
    "\n",
    "    # 2. Construct pivot table display format: \"Ratio% (P-value)\"\n",
    "    def format_cell(row):\n",
    "        # Markers: ** p<0.01, * p<0.05 (Raw P-value)\n",
    "        p = row['p_val']\n",
    "        mark = \"\"\n",
    "        if p < 0.01: mark = \"**\"\n",
    "        elif p < 0.05: mark = \"*\"\n",
    "        \n",
    "        # Physical direction consistency check\n",
    "        ratio = row['Ratio']\n",
    "        # Expected: Conjunction > 100, Opposition < 100\n",
    "        is_consistent = False\n",
    "        if row['Type'] == 'Conjunction' and ratio > 100: is_consistent = True\n",
    "        elif row['Type'] == 'Opposition' and ratio < 100: is_consistent = True\n",
    "        \n",
    "        # If significant but direction is reversed (e.g., conjunction actually decreases), mark as anomalous\n",
    "        if mark and not is_consistent:\n",
    "            return f\"{ratio:.0f}%(Anomalous{mark})\"\n",
    "        \n",
    "        return f\"{ratio:.0f}% ({p:.3f}{mark})\"\n",
    "\n",
    "    df_target['Result'] = df_target.apply(format_cell, axis=1)\n",
    "    \n",
    "    # Create pivot table\n",
    "    # Rows: Group (Flare Class), Type (Phase Type)\n",
    "    # Columns: Window (Window Size)\n",
    "    try:\n",
    "        pivot = df_target.pivot_table(\n",
    "            index=['Group', 'Type'], \n",
    "            columns='Window', \n",
    "            values='Result', \n",
    "            aggfunc='first'\n",
    "        )\n",
    "        \n",
    "        # Adjust column order\n",
    "        cols = sorted(pivot.columns)\n",
    "        pivot = pivot[cols]\n",
    "        \n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.width', 1000)\n",
    "        print(pivot)\n",
    "        \n",
    "        print(\"\\n[Interpretation Guide]\")\n",
    "        print(\"  1. Look for cells with * or ** in M-Class or X-Class rows.\")\n",
    "        print(\"  2. Verify trends: Is Conjunction Ratio > 100%? Is Opposition Ratio < 100%?\")\n",
    "        print(\"  3. If the effect for large flares (M/X) is stronger than for small flares (C), the physical link is established.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate pivot table: {e}\")\n",
    "\n",
    "\n",
    "def process_algo3_fair_fdr(csv_file):\n",
    "    \"\"\"\n",
    "    For Algo 3 (Single Body Scan):\n",
    "    FDR must be retained because 781 bodies were tested (N=781).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_file): \n",
    "        print(f\"File not found: {csv_file}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[Algo 3] Performing Fair FDR Correction (for 781 bodies)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Must group by environment (Stage + Window + Type)\n",
    "    # Compare 781 bodies under identical conditions\n",
    "    # Note: Flare data may not have a 'Stage' concept (only All); adapt automatically if present\n",
    "    group_cols = ['Stage', 'Window', 'Type']\n",
    "    actual_cols = [c for c in group_cols if c in df.columns]\n",
    "    \n",
    "    df['p_adj_bh'] = np.nan\n",
    "    df['sig_fdr'] = False\n",
    "    \n",
    "    total_sig = 0\n",
    "    \n",
    "    for name, group_data in df.groupby(actual_cols):\n",
    "        # N = Number of bodies in this group (usually 781)\n",
    "        p_vals = group_data['p_val'].values\n",
    "        idx = group_data.index\n",
    "        \n",
    "        # BH (Benjamini-Hochberg) Correction\n",
    "        reject, p_adj, _, _ = multipletests(p_vals, alpha=0.05, method='fdr_bh')\n",
    "        \n",
    "        df.loc[idx, 'p_adj_bh'] = p_adj\n",
    "        df.loc[idx, 'sig_fdr'] = reject\n",
    "        total_sig += reject.sum()\n",
    "            \n",
    "    # Save results\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Correction completed. A total of {total_sig} significant body records passing FDR were found across all scans.\")\n",
    "    if total_sig > 0:\n",
    "        print(\"Please open the CSV to view rows where 'sig_fdr' is True, focusing on non-Earth (399) asteroids.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. Run physical trend analysis for Algo 1 (Most important)\n",
    "    analyze_physics_trend(FILE_ALGO1, \"Algo 1: Total Pairs\")\n",
    "    \n",
    "    # 2. Run physical trend analysis for Algo 2\n",
    "    analyze_physics_trend(FILE_ALGO2, \"Algo 2: At Least One\")\n",
    "    \n",
    "    # 3. Run FDR correction for Algo 3 (Needle in a haystack mode)\n",
    "    process_algo3_fair_fdr(FILE_ALGO3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb885cf-9dc1-4f4f-8d4b-040c8bd06587",
   "metadata": {},
   "source": [
    "## Step 6: Kuiper Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a867c49b-6d21-4894-acc9-604b402d8bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 5: Kuiper Test for Distribution Morphology (Full Group Scan)\n",
      "============================================================\n",
      "Processing Stage: Flare_All ...\n",
      "Kuiper Test completed. Results saved to: ../../results/04_conj_enh_opp_sup/sf/sf_algo_kuiper_test.csv\n",
      "\n",
      "Top 10 Most Significant Distribution Anomalies (Focus on M/X Class):\n",
      "        Stage    Group   Planet  N_Count  V_statistic       p_value Sig\n",
      "2   Flare_All    Total  399_lon    39034       0.4996  0.000000e+00  **\n",
      "10  Flare_All  C-Class  399_lon    26654       0.5126  0.000000e+00  **\n",
      "18  Flare_All  M-Class  399_lon     4824       0.4893  0.000000e+00  **\n",
      "34  Flare_All  B-Class  399_lon     7127       0.4933  0.000000e+00  **\n",
      "26  Flare_All  X-Class  399_lon      429       0.4917  2.368558e-88  **\n",
      "12  Flare_All  C-Class  599_lon    26654       0.0301  1.974810e-19  **\n",
      "3   Flare_All    Total  499_lon    39034       0.0207  4.072441e-13  **\n",
      "4   Flare_All    Total  599_lon    39034       0.0186  2.048680e-10  **\n",
      "33  Flare_All  B-Class  299_lon     7127       0.0431  3.170678e-10  **\n",
      "11  Flare_All  C-Class  499_lon    26654       0.0220  6.591044e-10  **\n",
      "\n",
      ">>> ALL STEPS COMPLETED SUCCESSFULLY. <<<\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Step 6: Kuiper Test (Verify Morphology/Bimodal Distribution) - Full Coverage\n",
    "# =============================================================================\n",
    "try:\n",
    "    from astropy.stats import kuiper\n",
    "except ImportError:\n",
    "    print(\"Warning: astropy not installed. Skipping Kuiper test.\")\n",
    "    kuiper = None\n",
    "\n",
    "# Output path (ensure consistency with above definitions)\n",
    "OUTPUT_FILE_KUIPER = os.path.join(BASE_OUTPUT_DIR, 'sf_algo_kuiper_test.csv')\n",
    "\n",
    "def step5_run_kuiper_test_full():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Step 5: Kuiper Test for Distribution Morphology (Full Group Scan)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if kuiper is None: return\n",
    "\n",
    "    # Get all ready files in the cache directory\n",
    "    files = [f for f in os.listdir(CACHE_DIR) if f.startswith('ready_') and f.endswith('.parquet')]\n",
    "    \n",
    "    results_kuiper = []\n",
    "    \n",
    "    for f in files:\n",
    "        stage_name = f.replace('ready_', '').replace('.parquet', '')\n",
    "        print(f\"Processing Stage: {stage_name} ...\")\n",
    "        \n",
    "        # Read data\n",
    "        fpath = os.path.join(CACHE_DIR, f)\n",
    "        df = pd.read_parquet(fpath)\n",
    "        \n",
    "        # 1. Determine groups to iterate over\n",
    "        # Always include 'Total'\n",
    "        groups_dict = {'Total': df}\n",
    "        \n",
    "        # If Group column exists (e.g., C, M, X Class), split and process\n",
    "        if 'Group' in df.columns:\n",
    "            for g in df['Group'].unique():\n",
    "                # Filter out NaN or empty strings\n",
    "                if pd.isna(g) or str(g).strip() == '': continue\n",
    "                groups_dict[str(g)] = df[df['Group'] == g]\n",
    "        \n",
    "        # 2. Iterate over each group for testing\n",
    "        for group_name, subset_df in groups_dict.items():\n",
    "            # Skip if sample size is too small (avoid mathematical errors)\n",
    "            if len(subset_df) < 10: \n",
    "                continue\n",
    "                \n",
    "            # Automatically determine reference longitude column name ('hgs_lon' or 'hme_lon')\n",
    "            # Prioritize 'hme_lon' (used in sunspot code), then 'hgc_lon' (sometimes used in flare code)\n",
    "            ref_col = 'hme_lon' if 'hme_lon' in subset_df.columns else ('hgc_lon' if 'hgc_lon' in subset_df.columns else None)\n",
    "            \n",
    "            if ref_col is None:\n",
    "                # Skip if reference column not found\n",
    "                continue\n",
    "                \n",
    "            # Test for 8 major planets\n",
    "            for planet in PLANET_COLS:\n",
    "                # 1. Calculate phase angle (0-360)\n",
    "                # Phase = (Planet_Lon - Sun_Event_Lon) % 360\n",
    "                phases = (subset_df[planet] - subset_df[ref_col]) % 360.0\n",
    "                phases = phases.dropna().values\n",
    "                \n",
    "                if len(phases) == 0: continue\n",
    "                \n",
    "                # 2. Normalize to [0, 1]\n",
    "                data = phases / 360.0\n",
    "                \n",
    "                # 3. Execute Kuiper test (check for deviation from uniform distribution)\n",
    "                try:\n",
    "                    V, p_val = kuiper(data)\n",
    "                    \n",
    "                    results_kuiper.append({\n",
    "                        'Stage': stage_name,\n",
    "                        'Group': group_name,\n",
    "                        'Planet': planet,\n",
    "                        'N_Count': len(phases),\n",
    "                        'V_statistic': round(V, 4),\n",
    "                        'p_value': p_val,\n",
    "                        # Simple significance marking\n",
    "                        'Sig': '**' if p_val < 0.01 else ('*' if p_val < 0.05 else '')\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "    \n",
    "    # Save results\n",
    "    if results_kuiper:\n",
    "        df_k = pd.DataFrame(results_kuiper)\n",
    "        # Sort by significance, most significant results first\n",
    "        df_k.sort_values(by='p_value', inplace=True)\n",
    "        \n",
    "        df_k.to_csv(OUTPUT_FILE_KUIPER, index=False)\n",
    "        print(f\"Kuiper Test completed. Results saved to: {OUTPUT_FILE_KUIPER}\")\n",
    "        print(\"\\nTop 10 Most Significant Distribution Anomalies (Focus on M/X Class):\")\n",
    "        print(df_k.head(10))\n",
    "    else:\n",
    "        print(\"No Kuiper test results generated (possibly due to mismatched column names or insufficient sample size).\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    step5_run_kuiper_test_full()\n",
    "    print(\"\\n>>> ALL STEPS COMPLETED SUCCESSFULLY. <<<\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60dd02-bdff-48b9-92f3-f1ea82cbc57c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
