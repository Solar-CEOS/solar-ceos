{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5b3079-3780-4cc7-a644-07a337b40c3b",
   "metadata": {},
   "source": [
    "# Classification Extraction: Start, End, Full Lifecycle, Single Day\n",
    "\n",
    "- **Onset**: The position (Carrington longitude/latitude) on the first day of appearance was visible on the solar disk the previous day but had no record.\n",
    "- **Diss**: The position on the last day of appearance was visible on the solar disk the following day but had no record.\n",
    "- **Dur**: Sunspots where both start and end were observed, meaning their entire lifespan was recorded.\n",
    "- **Daily**: Full lifecycle sunspots with a lifespan of only one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f5bf0b-f574-4358-ba8b-4d660db6ba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Found 3656 duplicate records (same date and group_id)\n",
      "Task completed\n"
     ]
    }
   ],
   "source": [
    "# Mark Duplicates, Continuity, and Lifespan\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define data directory\n",
    "data_dir = '../../data/interm'\n",
    "\n",
    "# Read CSV file (output from 06)\n",
    "input_file = os.path.join(data_dir, 'sg_base_data.csv')\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Check and report duplicate records\n",
    "# Note: 06 output uses 'date' and 'group_id'\n",
    "date_col = 'date'\n",
    "id_col = 'group_id'\n",
    "\n",
    "duplicates = df[df.duplicated(subset=[date_col, id_col], keep=False)]\n",
    "if not duplicates.empty:\n",
    "    duplicate_count = len(duplicates)\n",
    "    print(f\"Warning: Found {duplicate_count} duplicate records (same {date_col} and {id_col})\")\n",
    "    duplicates.to_csv(os.path.join(data_dir, 'duplicate_records.csv'), index=False)\n",
    "\n",
    "# Save original index\n",
    "df['original_index'] = df.index\n",
    "# Mark duplicate dates and IDs\n",
    "df['is_duplicate'] = df.duplicated(subset=[date_col, id_col], keep=False).astype(int)\n",
    "\n",
    "# Save original date string\n",
    "df['original_date_format'] = df[date_col].copy()\n",
    "\n",
    "# Convert date column to datetime type\n",
    "df[date_col] = pd.to_datetime(df[date_col], format='mixed')\n",
    "# Create a date-only column for calculations\n",
    "df['date_only'] = df[date_col].dt.normalize()\n",
    "\n",
    "# Sort by group_id and date\n",
    "df = df.sort_values(by=[id_col, date_col])\n",
    "# Calculate date difference within each group_id to check if it exceeds 2 Carrington cycles (28 days * 2)\n",
    "df['date_diff'] = df.groupby(id_col)['date_only'].diff().gt(pd.Timedelta(days=56))\n",
    "# Apply cumulative sum to date differences within each group_id to form continuity markers\n",
    "df['temp_continuous'] = df.groupby(id_col)['date_diff'].cumsum()\n",
    "# Calculate number of rows for each group_id\n",
    "counts = df.groupby(id_col)['temp_continuous'].transform('size')\n",
    "# If a group_id has only one row, mark as 101, otherwise use \"temp_continuous\"\n",
    "df['continuity_id'] = df['temp_continuous']\n",
    "df.loc[counts == 1, 'continuity_id'] = 101\n",
    "\n",
    "# Calculate lifespan based on dates (ignoring time), add 1 because both start and end dates are inclusive\n",
    "df['lifespan'] = df.groupby([id_col, 'continuity_id'])['date_only'].transform(lambda x: (x.max() - x.min()).days + 1)\n",
    "\n",
    "# Re-sort by original index\n",
    "df.sort_values('original_index', inplace=True)\n",
    "\n",
    "# Restore original date string format\n",
    "df[date_col] = df['original_date_format']\n",
    "\n",
    "# Drop temporary columns\n",
    "df.drop(columns=['temp_continuous', 'date_diff', 'original_index', 'original_date_format', 'date_only'], inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "output_file = os.path.join(data_dir, 'sg_base_data_marked.csv')\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print('Task completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df565b23-76ae-4371-81f9-6c74880a69c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all sunspot groups...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 42941/42941 [13:50<00:00, 51.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task completed\n"
     ]
    }
   ],
   "source": [
    "# Calculate Pre-visibility and Post-visibility\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from sunpy.coordinates import frames\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.time import Time\n",
    "import warnings\n",
    "from erfa import ErfaWarning\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Ignore specific types of warnings\n",
    "warnings.filterwarnings('ignore', category=ErfaWarning)\n",
    "\n",
    "def is_sun_disk(date, L, B):\n",
    "    \"\"\"Check if a Carrington coordinate (L, B) is visible on the solar disk from Earth at a given date\"\"\"\n",
    "    # Create time object\n",
    "    time = Time(date, scale='utc')\n",
    "    # Create Carrington coordinate object\n",
    "    hgc_coord = SkyCoord(L * u.deg, B * u.deg, radius=1 * u.solRad, obstime=time, observer='earth', frame=frames.HeliographicCarrington)\n",
    "    # Transform to Helioprojective coordinates\n",
    "    hpc_coord = hgc_coord.transform_to(frames.Helioprojective)\n",
    "    return hpc_coord.is_visible()\n",
    "\n",
    "# Define data directory\n",
    "data_dir = '../../data/interm'\n",
    "\n",
    "# Read CSV file\n",
    "input_file = os.path.join(data_dir, 'sg_base_data_marked.csv')\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Define column names\n",
    "date_col = 'date'\n",
    "id_col = 'group_id'\n",
    "lon_col = 'hcc_lon' if 'hcc_lon' in df.columns else 'hcc_lon_calc'\n",
    "lat_col = 'hg_lat'\n",
    "\n",
    "# Save original date string\n",
    "original_date = df[date_col].copy()\n",
    "# Convert date column format for calculation\n",
    "df[date_col] = pd.to_datetime(df[date_col], format='mixed')\n",
    "\n",
    "# Initialize 'pre_visible' and 'post_visible' columns to 0\n",
    "df['pre_visible'] = 0\n",
    "df['post_visible'] = 0\n",
    "\n",
    "# Process by group_id and continuity_id\n",
    "print(\"Processing all sunspot groups...\")\n",
    "for (number, continuous), group in tqdm(df.groupby([id_col, 'continuity_id'])):\n",
    "    # Get all rows for the minimum date\n",
    "    min_date = group[date_col].min()\n",
    "    min_indices = group[group[date_col] == min_date].index\n",
    "    \n",
    "    # Calculate pre-visibility for all rows of the minimum date\n",
    "    for min_idx in min_indices:\n",
    "        try:\n",
    "            result = is_sun_disk(\n",
    "                (group.loc[min_idx, date_col] - pd.DateOffset(days=1)).replace(hour=0, minute=0, second=0),\n",
    "                group.loc[min_idx, lon_col], group.loc[min_idx, lat_col]\n",
    "            )\n",
    "            df.loc[min_idx, 'pre_visible'] = 1 if result else -1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing pre-visibility for ID {number} Continuity {continuous} Date {group.loc[min_idx, date_col]}: {e}\")\n",
    "            df.loc[min_idx, 'pre_visible'] = 99\n",
    "\n",
    "    # Get all rows for the maximum date\n",
    "    max_date = group[date_col].max()\n",
    "    max_indices = group[group[date_col] == max_date].index\n",
    "    \n",
    "    # Calculate post-visibility for all rows of the maximum date\n",
    "    for max_idx in max_indices:\n",
    "        try:\n",
    "            result = is_sun_disk(\n",
    "                (group.loc[max_idx, date_col] + pd.DateOffset(days=2)).replace(hour=0, minute=0, second=0),\n",
    "                group.loc[max_idx, lon_col], group.loc[max_idx, lat_col]\n",
    "            )\n",
    "            df.loc[max_idx, 'post_visible'] = 1 if result else -1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing post-visibility for ID {number} Continuity {continuous} Date {group.loc[max_idx, date_col]}: {e}\")\n",
    "            df.loc[max_idx, 'post_visible'] = 99\n",
    "\n",
    "# Restore original date string\n",
    "df[date_col] = original_date\n",
    "\n",
    "# Save to CSV\n",
    "output_file = os.path.join(data_dir, 'sg_1874_2025_all_raw.csv')\n",
    "df.to_csv(output_file, index=False)\n",
    "print('Task completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f18536-dd8f-4321-8c81-f94237aaaf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data file: ../../data/interm/sg_1874_2025_all_raw.csv\n",
      "Successfully read data, total 256861 rows\n",
      "Processing onset data...\n",
      "Onset data processing complete, total 33295 rows\n",
      "Processing diss data...\n",
      "Diss data processing complete, total 27888 rows\n",
      "Processing dur data...\n",
      "Dur data processing complete, contains 22976 sunspot groups, total 75679 rows\n",
      "Processing daily data...\n",
      "Daily data processing complete, total 8305 rows\n",
      "All data processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Extraction: Onset, Diss, Dur, Daily\n",
    "# Discard: Earliest ID of onset, latest ID of diss, and the year 1982 when ID rules changed. These are handled separately.\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from erfa import ErfaWarning\n",
    "import os\n",
    "\n",
    "# Ignore specific types of warnings\n",
    "warnings.filterwarnings('ignore', category=ErfaWarning)\n",
    "\n",
    "# Define data directory\n",
    "data_dir = '../../data/interm'\n",
    "\n",
    "# Configuration parameters\n",
    "INPUT_FILE = os.path.join(data_dir, 'sg_1874_2025_all_raw.csv')\n",
    "OUTPUT_FILES = {\n",
    "    'onset': os.path.join(data_dir, 'sg_1874_2025_onset_raw.csv'),\n",
    "    'diss': os.path.join(data_dir, 'sg_1874_2025_diss_raw.csv'),\n",
    "    'dur': os.path.join(data_dir, 'sg_1874_2025_dur_raw.csv'),\n",
    "    'daily': os.path.join(data_dir, 'sg_1874_2025_daily_raw.csv')\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Read INPUT_FILE\n",
    "    print(f\"Reading data file: {INPUT_FILE}\")\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"Successfully read data, total {len(df)} rows\")\n",
    "    \n",
    "    # Define column names\n",
    "    id_col = 'group_id'\n",
    "    \n",
    "    # Check if necessary columns exist\n",
    "    required_columns = ['pre_visible', 'post_visible', id_col, 'continuity_id', 'lifespan']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        missing = [col for col in required_columns if col not in df.columns]\n",
    "        raise ValueError(f\"Missing required columns in data file: {missing}\")\n",
    "\n",
    "    # Process onset data\n",
    "    print(\"Processing onset data...\")\n",
    "    onset_data = df[df['pre_visible'] == 1].copy()\n",
    "    onset_data.to_csv(OUTPUT_FILES['onset'], index=False)\n",
    "    print(f\"Onset data processing complete, total {len(onset_data)} rows\")\n",
    "\n",
    "    # Process diss data\n",
    "    print(\"Processing diss data...\")\n",
    "    diss_data = df[df['post_visible'] == 1].copy()\n",
    "    diss_data.to_csv(OUTPUT_FILES['diss'], index=False)\n",
    "    print(f\"Diss data processing complete, total {len(diss_data)} rows\")\n",
    "\n",
    "    # Process dur data: find sunspot groups with both pre_visible and post_visible markers\n",
    "    print(\"Processing dur data...\")\n",
    "    # Get pre_visible and post_visible status for each sunspot group\n",
    "    group_status = df.groupby([id_col, 'continuity_id']).agg({\n",
    "        'pre_visible': 'max',  # 1 if any row in group has 1\n",
    "        'post_visible': 'max'   # 1 if any row in group has 1\n",
    "    }).reset_index()\n",
    "\n",
    "    # Find groups with both pre_visible and post_visible\n",
    "    valid_groups = group_status.query('pre_visible == 1 and post_visible == 1')\n",
    "    \n",
    "    # Efficiently get all data for these groups\n",
    "    mask = df.set_index([id_col, 'continuity_id']).index.isin(\n",
    "        valid_groups.set_index([id_col, 'continuity_id']).index)\n",
    "    dur_data = df[mask].copy()\n",
    "\n",
    "    dur_data.to_csv(OUTPUT_FILES['dur'], index=False)\n",
    "    print(f\"Dur data processing complete, contains {len(valid_groups)} sunspot groups, total {len(dur_data)} rows\")\n",
    "\n",
    "    # Process daily data\n",
    "    print(\"Processing daily data...\")\n",
    "    daily_data = dur_data[dur_data['lifespan'] == 1].copy()\n",
    "    daily_data.to_csv(OUTPUT_FILES['daily'], index=False)\n",
    "    print(f\"Daily data processing complete, total {len(daily_data)} rows\")\n",
    "\n",
    "    print(\"All data processing complete!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File {INPUT_FILE} not found, please check path and filename\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25720c29-15fe-4c36-a746-ee623ec6b912",
   "metadata": {},
   "source": [
    "## Remove Sunspot Groups Mixed into Onset or Diss Boundaries\n",
    "\n",
    "- **Daily**: Remove duplicate rows with -1.\n",
    "- **Database Start**: 1874-05-09, 1874-05-10 (4 groups)\n",
    "- **Database End**: 2025-04-02 (7 groups)\n",
    "- **1977-01-01**: 1 group before and 1 after\n",
    "- **1982-01-01**: 9 groups before, 12 after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145dfcf8-4ec3-4c98-8982-7694aaa10742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing stage: onset\n",
      "Successfully processed: deleted 16 records, 33279 records remaining\n",
      "\n",
      "Processing stage: diss\n",
      "Successfully processed: deleted 18 records, 27870 records remaining\n",
      "\n",
      "Processing stage: daily\n",
      "Successfully processed: deleted 3 records, 8301 records remaining\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define data directory\n",
    "data_dir = '../../data/interm'\n",
    "\n",
    "# Mapping of stages to their English names\n",
    "stages = ['onset', 'diss', 'daily']\n",
    "\n",
    "for stage in stages:\n",
    "    print(f\"\\nProcessing stage: {stage}\")\n",
    "    try:\n",
    "        # Read boundary processing data\n",
    "        excel_path = os.path.join(data_dir, 'sg_boundary_processing.xlsx')\n",
    "        df_boundary = pd.read_excel(excel_path, sheet_name=stage)\n",
    "        \n",
    "        # Read raw lifecycle data\n",
    "        input_csv = os.path.join(data_dir, f'sg_1874_2025_{stage}_raw.csv')\n",
    "        df_raw = pd.read_csv(input_csv)\n",
    "        \n",
    "        # Define column names\n",
    "        date_col = 'date'\n",
    "        id_col = 'group_id'\n",
    "        \n",
    "        # Check if key columns exist\n",
    "        if date_col not in df_boundary.columns or id_col not in df_boundary.columns:\n",
    "            raise ValueError(f\"Sheet '{stage}' is missing '{date_col}' or '{id_col}' column\")\n",
    "        \n",
    "        # Standardize date format (convert to datetime)\n",
    "        df_boundary[date_col] = pd.to_datetime(df_boundary[date_col], format='mixed', errors='coerce')\n",
    "        df_raw[date_col] = pd.to_datetime(df_raw[date_col], format='mixed', errors='coerce')\n",
    "        \n",
    "        # Check for invalid dates\n",
    "        if df_boundary[date_col].isna().any() or df_raw[date_col].isna().any():\n",
    "            print(f\"Warning: Malformed dates found in {stage}, please check source data!\")\n",
    "        \n",
    "        # Create merge keys (standardized as strings for comparison)\n",
    "        df_boundary['merge_key'] = df_boundary[date_col].dt.strftime('%Y%m%d') + '_' + df_boundary[id_col].astype(str)\n",
    "        df_raw['merge_key'] = df_raw[date_col].dt.strftime('%Y%m%d') + '_' + df_raw[id_col].astype(str)\n",
    "        \n",
    "        # Filter and save\n",
    "        df_filtered = df_raw[~df_raw['merge_key'].isin(df_boundary['merge_key'])].drop(columns=['merge_key'])\n",
    "        output_csv = os.path.join(data_dir, f'sg_1874_2025_{stage}.csv')\n",
    "        df_filtered.to_csv(output_csv, index=False)\n",
    "        \n",
    "        print(f\"Successfully processed: deleted {len(df_boundary)} records, {len(df_filtered)} records remaining\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Processing failed for {stage}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa75a36f-30d7-4c45-a350-0afaeb93d770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onset formatting and cleanup complete, saved to ../../data/ready/sg_1874_2025_onset.csv\n",
      "diss formatting and cleanup complete, saved to ../../data/ready/sg_1874_2025_diss.csv\n",
      "dur formatting and cleanup complete, saved to ../../data/ready/sg_1874_2025_dur.csv\n",
      "daily formatting and cleanup complete, saved to ../../data/ready/sg_1874_2025_daily.csv\n",
      "all formatting and cleanup complete, saved to ../../data/ready/sg_1874_2025_all.csv\n"
     ]
    }
   ],
   "source": [
    "# Date Formatting and Cleanup\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define directories\n",
    "data_dir = '../../data/interm'\n",
    "final_dir = '../../data/ready'\n",
    "\n",
    "# Ensure final directory exists\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "# Stages to format and move to final\n",
    "stages = ['onset', 'diss', 'dur', 'daily', 'all']\n",
    "\n",
    "for stage in stages:\n",
    "    # Determine input filename\n",
    "    if stage in ['dur', 'all']:\n",
    "        input_filename = f'sg_1874_2025_{stage}_raw.csv'\n",
    "    else:\n",
    "        input_filename = f'sg_1874_2025_{stage}.csv'\n",
    "        \n",
    "    input_path = os.path.join(data_dir, input_filename)\n",
    "    \n",
    "    if os.path.exists(input_path):\n",
    "        # Read CSV file\n",
    "        df = pd.read_csv(input_path)\n",
    "        \n",
    "        # 1. Date formatting\n",
    "        date_col = 'date'\n",
    "        df[date_col] = pd.to_datetime(df[date_col], format='mixed').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # 2. Round numeric columns (angles, etc.) to 4 decimal places\n",
    "        float_cols = df.select_dtypes(include=['float']).columns\n",
    "        df[float_cols] = df[float_cols].round(4)\n",
    "        \n",
    "        # 3. Drop internal marker columns after hme_lat\n",
    "        cols_to_drop = ['is_duplicate', 'continuity_id', 'lifespan', 'pre_visible', 'post_visible']\n",
    "        df.drop(columns=[c for c in cols_to_drop if c in df.columns], inplace=True, errors='ignore')\n",
    "        \n",
    "        # Save to final directory with clean name\n",
    "        output_filename = f'sg_1874_2025_{stage}.csv'\n",
    "        output_path = os.path.join(final_dir, output_filename)\n",
    "        \n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f'{stage} formatting and cleanup complete, saved to {output_path}')\n",
    "    else:\n",
    "        print(f'Input file not found: {input_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb307a4c-8382-4100-83c5-ed6a65ff91e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
