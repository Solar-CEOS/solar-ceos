{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b780e9ff-bd8b-4265-8a03-b8ae8b7d5a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data from ready directory...\n",
      "Processed: Onset SG (Groups: 33166)\n",
      "Processed: Diss. SG (Groups: 27755)\n",
      "Processed: Dur. SG (Groups: 22976)\n",
      "Processed: Daily SG (Groups: 8300)\n",
      "Processed: Full DB (Groups: 42941)\n",
      "\n",
      "================================================================================\n",
      "Table: Statistics of SG Lifecycles (1874–2025)\n",
      "================================================================================\n",
      "Category N Groups Prop. (%) N Records A_max (MH) < A > (MH)\n",
      "Onset SG   33,166     77.24     33278       1855      30.65\n",
      "Diss. SG   27,755     64.64     27870        919      15.04\n",
      " Dur. SG   22,976     53.51     75678       1650      34.75\n",
      "Daily SG    8,300     19.33      8301        919      11.08\n",
      " Full DB   42,941    100.00    256859       6132     157.92\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Table Notes Calculation]\n",
      "Note 1: MH: Millionths of a Solar Hemisphere.\n",
      "Note 2: The average annual count of Daily SGs was 73.13 before 1982,\n",
      "        dropping to 11.31 thereafter.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Configuration\n",
    "# Suffixes match the filenames in data/ready/sg_1874_2025_{suffix}.csv\n",
    "stages_config = [\n",
    "    {'name': 'Onset SG', 'suffix': 'onset'},\n",
    "    {'name': 'Diss. SG', 'suffix': 'diss'},\n",
    "    {'name': 'Dur. SG', 'suffix': 'dur'},\n",
    "    {'name': 'Daily SG', 'suffix': 'daily'},\n",
    "    {'name': 'Full DB', 'suffix': 'all'}\n",
    "]\n",
    "\n",
    "stats_list = []\n",
    "full_db_groups = 0 \n",
    "daily_df = None # Store daily data for footnote calculation\n",
    "\n",
    "print(\"Processing data from ready directory...\")\n",
    "\n",
    "for config in stages_config:\n",
    "    file_path = f'../../data/ready/sg_1874_2025_{config[\"suffix\"]}.csv'\n",
    "    \n",
    "    try:\n",
    "        # Read the final CSV files (headers: date, group_id, area, ...)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            n_groups, n_records, a_max, mean_area = 0, 0, 0, 0\n",
    "        else:\n",
    "            # Ensure date column is in datetime format\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "            # Backup daily data for footnote\n",
    "            if config['suffix'] == 'daily':\n",
    "                daily_df = df.copy()\n",
    "\n",
    "            # ==========================================\n",
    "            # Core Logic: Calculate real Group count (handling ID reuse)\n",
    "            # ==========================================\n",
    "            # 1. Sort by ID and date\n",
    "            df = df.sort_values(['group_id', 'date'])\n",
    "            \n",
    "            # 2. Calculate time difference for the same ID\n",
    "            # NaT for the first record of an ID; otherwise the time delta\n",
    "            time_diff = df.groupby('group_id')['date'].diff()\n",
    "            \n",
    "            # 3. Identify new groups: First record (NaT) or gap > 180 days\n",
    "            # 180 days is far beyond sunspot physical lifetime, enough to distinguish ID reuse\n",
    "            is_new_start = (time_diff.isna()) | (time_diff > pd.Timedelta(days=180))\n",
    "            \n",
    "            n_groups = is_new_start.sum()\n",
    "            n_records = len(df)\n",
    "            a_max = df['area'].max()\n",
    "            mean_area = df['area'].mean()\n",
    "            # ==========================================\n",
    "        \n",
    "        # Record total groups for proportion calculation\n",
    "        if config['suffix'] == 'all':\n",
    "            full_db_groups = n_groups\n",
    "\n",
    "        stats_list.append({\n",
    "            'Category': config['name'],\n",
    "            'N Groups': n_groups,\n",
    "            'N Records': n_records,\n",
    "            'A_max (MH)': a_max,\n",
    "            '< A > (MH)': mean_area\n",
    "        })\n",
    "        print(f\"Processed: {config['name']} (Groups: {n_groups})\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {config['name']}: {e}\")\n",
    "\n",
    "# 2. Generate Main Table\n",
    "results_df = pd.DataFrame(stats_list)\n",
    "\n",
    "# Calculate Proportion (%)\n",
    "if full_db_groups > 0:\n",
    "    results_df['Prop. (%)'] = (results_df['N Groups'] / full_db_groups) * 100\n",
    "else:\n",
    "    results_df['Prop. (%)'] = 0.0\n",
    "\n",
    "# 3. Format Output (Matching LaTeX table style)\n",
    "def format_row(row):\n",
    "    return pd.Series({\n",
    "        'Category': row['Category'],\n",
    "        # N Groups: Integer with thousands separator\n",
    "        'N Groups': f\"{int(row['N Groups']):,}\", \n",
    "        # Prop: 2 decimal places\n",
    "        'Prop. (%)': f\"{row['Prop. (%)']:.2f}\",\n",
    "        # N Records: Integer\n",
    "        'N Records': f\"{int(row['N Records'])}\", \n",
    "        # A_max: Integer\n",
    "        'A_max (MH)': f\"{row['A_max (MH)']:.0f}\", \n",
    "        # <A>: 2 decimal places\n",
    "        '< A > (MH)': f\"{row['< A > (MH)']:.2f}\"\n",
    "    })\n",
    "\n",
    "formatted_df = results_df.apply(format_row, axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Table: Statistics of SG Lifecycles (1874–2025)\")\n",
    "print(\"=\"*80)\n",
    "print(formatted_df.to_string(index=False, justify='right'))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# 4. Footnote Calculation\n",
    "print(\"\\n[Table Notes Calculation]\")\n",
    "print(\"Note 1: MH: Millionths of a Solar Hemisphere.\")\n",
    "\n",
    "if daily_df is not None and not daily_df.empty:\n",
    "    daily_df['Year'] = daily_df['date'].dt.year\n",
    "    yearly_counts = daily_df.groupby('Year')['group_id'].nunique()\n",
    "    \n",
    "    cutoff_year = 1982\n",
    "    before_1982 = yearly_counts[yearly_counts.index < cutoff_year]\n",
    "    after_1982 = yearly_counts[yearly_counts.index >= cutoff_year]\n",
    "    \n",
    "    avg_before = before_1982.mean() if not before_1982.empty else 0\n",
    "    avg_after = after_1982.mean() if not after_1982.empty else 0\n",
    "    \n",
    "    print(f\"Note 2: The average annual count of Daily SGs was {avg_before:.2f} before {cutoff_year},\")\n",
    "    print(f\"        dropping to {avg_after:.2f} thereafter.\")\n",
    "else:\n",
    "    print(\"\\n[Note] Cannot calculate Note 2: Daily data not loaded or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa048ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for group_id duplicates within 180 days (potential errors)...\n",
      "\n",
      "[Onset] Total Records: 33279, Duplicates: 113 (0.3396%)\n",
      "Sample of 107 group_ids with near-duplicates:\n",
      "            date  group_id  area  dist_c     pa\n",
      "24293 1982-01-09      3548     0   0.536  137.4\n",
      "24294 1982-01-09      3548    10   0.413  132.2\n",
      "24303 1982-01-17      3554    10   0.212  213.8\n",
      "24304 1982-01-17      3554     0   0.372  194.6\n",
      "24487 1982-08-07      3847    10   0.471  125.2\n",
      "24488 1982-08-07      3847     0   0.384  122.6\n",
      "24538 1982-09-24      3920     0   0.293  159.4\n",
      "24539 1982-09-24      3920    10   0.349  223.3\n",
      "24628 1983-01-02      4044    30   0.209   19.3\n",
      "24629 1983-01-02      4044    10   0.278  342.1\n",
      "\n",
      "[Diss.] Total Records: 27870, Duplicates: 115 (0.4126%)\n",
      "Sample of 106 group_ids with near-duplicates:\n",
      "            date  group_id  area  dist_c     pa\n",
      "22825 1982-03-13      3635    10   0.721  274.1\n",
      "22826 1982-03-13      3635     0   0.738  265.7\n",
      "22847 1982-04-16      3690    10   0.692  249.5\n",
      "22849 1982-04-16      3690     0   0.704  249.8\n",
      "22875 1982-05-23      3745     0   0.476  314.3\n",
      "22876 1982-05-23      3745    10   0.228  342.5\n",
      "22905 1982-07-03      3792    10   0.449   74.1\n",
      "22906 1982-07-03      3792     0   0.330   78.1\n",
      "22926 1982-07-27      3822    20   0.431  182.2\n",
      "22927 1982-07-27      3822    10   0.446  137.7\n",
      "\n",
      "[Daily] Total Records: 8301, Duplicates: 1 (0.0120%)\n",
      "Sample of 1 group_ids with near-duplicates:\n",
      "           date  group_id  area  dist_c     pa\n",
      "7847 1983-11-15      4362    10   0.173  150.0\n",
      "7848 1983-11-15      4362     0   0.422  123.5\n",
      "\n",
      "Note: These near-duplicates are very rare and will be ignored in the statistical analysis.\n"
     ]
    }
   ],
   "source": [
    "# Check for group_id duplicates within 180 days in Onset, Diss, and Daily databases\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "files_to_check = [\n",
    "    ('Onset', '../../data/ready/sg_1874_2025_onset.csv'),\n",
    "    ('Diss.', '../../data/ready/sg_1874_2025_diss.csv'),\n",
    "    ('Daily', '../../data/ready/sg_1874_2025_daily.csv')\n",
    "]\n",
    "\n",
    "print(\"Checking for group_id duplicates within 180 days (potential errors)...\")\n",
    "\n",
    "for name, path in files_to_check:\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Sort by group_id and date to calculate time difference\n",
    "        df = df.sort_values(['group_id', 'date'])\n",
    "        \n",
    "        # Calculate time difference for the same group_id\n",
    "        time_diff = df.groupby('group_id')['date'].diff()\n",
    "        \n",
    "        # Identify duplicates within 180 days (ID reuse > 180 days is considered normal)\n",
    "        is_near_dup = (time_diff.notna()) & (time_diff <= pd.Timedelta(days=180))\n",
    "        \n",
    "        total_records = len(df)\n",
    "        dup_records = is_near_dup.sum()\n",
    "        dup_ratio = (dup_records / total_records) * 100 if total_records > 0 else 0\n",
    "        \n",
    "        # Get all records involved in these near-duplicates for display\n",
    "        dup_ids = df.loc[is_near_dup, 'group_id'].unique()\n",
    "        near_duplicates = df[df['group_id'].isin(dup_ids)].sort_values(['group_id', 'date'])\n",
    "        \n",
    "        print(f\"\\n[{name}] Total Records: {total_records}, Duplicates: {dup_records} ({dup_ratio:.4f}%)\")\n",
    "        \n",
    "        if not near_duplicates.empty:\n",
    "            print(f\"Sample of {len(dup_ids)} group_ids with near-duplicates:\")\n",
    "            # Print index and first 5 columns\n",
    "            print(near_duplicates.iloc[:10, :5]) # Show first 10 rows of such cases\n",
    "        else:\n",
    "            print(\"No group_id duplicates found within 180 days.\")\n",
    "    else:\n",
    "        print(f\"\\n[{name}] File not found: {path}\")\n",
    "\n",
    "print(\"\\nNote: These near-duplicates are very rare and will be ignored in the statistical analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969043e-e1e5-4300-8937-f819b2444a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
