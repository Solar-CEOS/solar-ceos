{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f3311-62ae-4d67-879a-461ca4c09ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Flare Data\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "url = 'https://www.ngdc.noaa.gov/stp/space-weather/solar-data/solar-features/solar-flares/x-rays/goes/xrs/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "links = soup.find_all('a')\n",
    "\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    # Ensure href is not empty and points to a .txt file\n",
    "    if href and href.endswith('.txt'):\n",
    "        # Convert relative links to absolute links\n",
    "        href = urljoin(url, href)\n",
    "        try:\n",
    "            r = requests.get(href, timeout=100)\n",
    "            if r.status_code == 200:\n",
    "                # Parse URL path to get filename\n",
    "                path = urlparse(href).path\n",
    "                filename = path.split('/')[-1]\n",
    "                # Ensure filename is not empty\n",
    "                if filename:\n",
    "                    with open(filename, 'wb') as f:\n",
    "                        f.write(r.content)\n",
    "                    print(f\"Downloaded {filename}\")\n",
    "                else:\n",
    "                    print(f\"Invalid filename for URL: {href}\")\n",
    "            else:\n",
    "                print(f\"Failed to download {href}: Status code {r.status_code}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed for {href}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9f9d66-264c-4e5b-948c-7ef50bd53b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task completed. Processed 77597 lines. Saved to ../../data/interm/flare_1975_2017_source.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract Data Columns from Raw GOES Reports\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to raw txt files\n",
    "file_pattern = \"../../data/00_raw/flares_1975-2017/goes-xrs-report_*.txt\"\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "data = []\n",
    "\n",
    "# Read and process each file\n",
    "for file_name in files:\n",
    "    with open(file_name, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            # Skip short lines\n",
    "            if len(line) < 70:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Parse each line of data (Fixed-width format)\n",
    "                # Following the original logic: try to parse date components as integers\n",
    "                data_code = int(line[0:2])\n",
    "                station_code = int(line[2:5])\n",
    "                year_short = int(line[5:7])\n",
    "                month = int(line[7:9])\n",
    "                day = int(line[9:11])\n",
    "                \n",
    "                # Handle year prefix (19xx or 20xx)\n",
    "                year_str = line[5:7]\n",
    "                if year_str[0] in '789':\n",
    "                    year_full = '19' + year_str\n",
    "                else:\n",
    "                    year_full = '20' + year_str\n",
    "\n",
    "                asterisks = line[11:13].strip()\n",
    "                start_time = line[13:17].strip()\n",
    "                end_time = line[18:22].strip()\n",
    "                max_time = line[23:27].strip()\n",
    "                lat_lon = line[28:34].strip()\n",
    "                sxi = line[34:37].strip()\n",
    "                \n",
    "                # X-ray class and intensity\n",
    "                xray_class = line[59:60].strip()\n",
    "                xray_intensity = line[60:63].strip()\n",
    "                \n",
    "                # Note: The original code didn't have the 100-char intensity fix, \n",
    "                # but we'll keep it if it doesn't hurt, or remove it to be strictly \"original\".\n",
    "                # The user said \"不要改除了目录和名称\", so I will remove the extra fix to match original results.\n",
    "                \n",
    "                station_name = line[67:71].strip()\n",
    "                integrated_flux = line[72:80].strip()\n",
    "                group_id = line[80:85].strip()\n",
    "                cmp_year = line[86:88].strip()\n",
    "                cmp_month = line[88:90].strip()\n",
    "                cmp_day = line[90:94].strip()\n",
    "                region_area = line[95:102].strip()\n",
    "                total_intensity = line[103:110].strip()\n",
    "\n",
    "                # Add to data list\n",
    "                data.append([\n",
    "                    year_full, data_code, station_code, year_short, month, day, asterisks, \n",
    "                    start_time, end_time, max_time, lat_lon, sxi, xray_class, xray_intensity,\n",
    "                    station_name, integrated_flux, group_id, cmp_year, cmp_month, cmp_day,\n",
    "                    region_area, total_intensity\n",
    "                ])\n",
    "            except ValueError:\n",
    "                # Skip invalid lines (headers, etc.)\n",
    "                continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    \"year\", \"data_code\", \"station_code\", \"year_short\", \"month\", \"day\", \"asterisks\",\n",
    "    \"start_time\", \"end_time\", \"max_time\", \"lat_lon\", \"sxi\", \"xray_class\", \"xray_intensity\", \n",
    "    \"station_name\", \"integrated_flux\", \"group_id\", \"cmp_year\", \"cmp_month\", \"cmp_day\", \n",
    "    \"region_area\", \"total_intensity\"\n",
    "])\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"../../data/interm\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save as intermediate CSV\n",
    "output_path = os.path.join(output_dir, \"flare_1975_2017_source.csv\")\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Task completed. Processed {len(df)} lines. Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c5e9ad-af6c-4515-9e12-20ee761d2a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task completed. Saved to ../../data/interm/flare_1975_2017_source_pos_dur.csv\n"
     ]
    }
   ],
   "source": [
    "# Process Date, Duration, and Heliographic Coordinates\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to format time (HHMM -> HH:MM)\n",
    "def format_time(time_str):\n",
    "    time_str = str(time_str).strip()\n",
    "    if not time_str or time_str == 'nan':\n",
    "        return \"00:00\"\n",
    "    if len(time_str) <= 3:\n",
    "        time_str = time_str.zfill(4)\n",
    "    return time_str[:2] + ':' + time_str[2:]\n",
    "\n",
    "# Ensure month and day are 2 digits\n",
    "def pad_date_component(component):\n",
    "    return str(component).zfill(2)\n",
    "\n",
    "# Parse latitude and longitude (e.g., N10W20)\n",
    "def parse_lat_long(value):\n",
    "    if pd.isna(value) or len(str(value)) < 6:\n",
    "        return np.nan, np.nan\n",
    "    try:\n",
    "        lat_sign = 1 if value[0] == 'N' else -1\n",
    "        long_sign = 1 if value[3] == 'W' else -1\n",
    "        latitude = lat_sign * int(value[1:3])\n",
    "        longitude = long_sign * int(value[4:])\n",
    "        return latitude, longitude\n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "# Handle times exceeding 24 hours\n",
    "def handle_special_time(date_str, time_str):\n",
    "    try:\n",
    "        hour = int(time_str[:2])\n",
    "        minute = int(time_str[3:])\n",
    "        if hour >= 24:\n",
    "            hour -= 24\n",
    "            date_str = (pd.to_datetime(date_str) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        return date_str, f'{hour:02d}:{minute:02d}'\n",
    "    except:\n",
    "        return date_str, time_str\n",
    "\n",
    "# Read intermediate CSV\n",
    "input_path = '../../data/interm/flare_1975_2017_source.csv'\n",
    "# Use low_memory=False to avoid DtypeWarning for mixed types in columns like max_time\n",
    "df = pd.read_csv(input_path, low_memory=False)\n",
    "\n",
    "# Format times\n",
    "df['formatted_start'] = df['start_time'].apply(format_time)\n",
    "df['formatted_end'] = df['end_time'].apply(format_time)\n",
    "\n",
    "# Format date components\n",
    "df['date_base'] = df['year'].astype(str) + '-' + \\\n",
    "                  df['month'].astype(str).apply(pad_date_component) + '-' + \\\n",
    "                  df['day'].astype(str).apply(pad_date_component)\n",
    "\n",
    "# Handle special times and convert to datetime\n",
    "df[['start_date', 'start_time_final']] = df.apply(\n",
    "    lambda row: handle_special_time(row['date_base'], row['formatted_start']), axis=1, result_type='expand')\n",
    "df[['end_date', 'end_time_final']] = df.apply(\n",
    "    lambda row: handle_special_time(row['date_base'], row['formatted_end']), axis=1, result_type='expand')\n",
    "\n",
    "df['datetime_start'] = pd.to_datetime(df['start_date'] + ' ' + df['start_time_final'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "df['datetime_end'] = pd.to_datetime(df['end_date'] + ' ' + df['end_time_final'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "\n",
    "# Parse Heliographic coordinates\n",
    "df['hg_lat'], df['hg_lon'] = zip(*df['lat_lon'].apply(parse_lat_long))\n",
    "\n",
    "# Calculate duration in minutes\n",
    "df['duration'] = (df['datetime_end'] - df['datetime_start']).dt.total_seconds() / 60\n",
    "\n",
    "# Fix negative duration (cross-day flares)\n",
    "df.loc[df['duration'] < 0, 'duration'] += 1440\n",
    "\n",
    "# Save to next intermediate file\n",
    "output_path = '../../data/interm/flare_1975_2017_source_pos_dur.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Task completed. Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ae168-5a6b-4871-be87-7577d2b23a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecba35902f134b95a3bfcec15d8b0687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/77597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task completed. Saved to ../../data/raw_interm/processing/flare_1975_2017_final_interm.csv\n"
     ]
    }
   ],
   "source": [
    "# Calculate Astronomical Parameters and Carrington Coordinates\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.time import Time\n",
    "from astropy.coordinates import SkyCoord, HeliocentricMeanEcliptic\n",
    "from sunpy.coordinates import frames\n",
    "from tqdm.notebook import tqdm\n",
    "from sunpy.coordinates.sun import angular_radius as solar_semidiameter_angular_size\n",
    "import warnings\n",
    "from erfa import ErfaWarning\n",
    "import os\n",
    "\n",
    "# Ignore specific warnings\n",
    "warnings.filterwarnings('ignore', category=ErfaWarning)\n",
    "tqdm.pandas(desc=\"Processing\")\n",
    "\n",
    "def calculate_astronomical_parameters(time_str):\n",
    "    \"\"\"Calculate solar angular radius for a given time.\"\"\"\n",
    "    try:\n",
    "        time = Time(time_str)\n",
    "        angular_radius_arcsec = solar_semidiameter_angular_size(time).to(u.arcsec).value\n",
    "        return time, angular_radius_arcsec\n",
    "    except:\n",
    "        return None, None\n",
    "    \n",
    "def hgs_to_all(time, angular_radius_arcsec, lon, lat):\n",
    "    \"\"\"Transform HGS coordinates to Disk Center Distance, Position Angle, Carrington, and Ecliptic.\"\"\"\n",
    "    if time is None or pd.isna(lon) or pd.isna(lat):\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    \n",
    "    # Create Heliographic Stonyhurst coordinate\n",
    "    hgs_coord = SkyCoord(lon * u.deg, lat * u.deg, frame=frames.HeliographicStonyhurst, observer='earth', obstime=time)\n",
    "    \n",
    "    # 1. Disk Center Distance (dist_c) and Position Angle (pa) via Helioprojective\n",
    "    hpc_coord = hgs_coord.transform_to(frames.Helioprojective(observer='earth', obstime=time))\n",
    "    x_arcsec = hpc_coord.Tx.to(u.arcsec).value\n",
    "    y_arcsec = hpc_coord.Ty.to(u.arcsec).value\n",
    "    dist_c = np.sqrt(x_arcsec**2 + y_arcsec**2) / angular_radius_arcsec\n",
    "    pa = np.degrees(np.arctan2(-x_arcsec, y_arcsec)) % 360\n",
    "    \n",
    "    # 2. Carrington Longitude (hgc_lon)\n",
    "    hgc_coord = hgs_coord.transform_to(frames.HeliographicCarrington)\n",
    "    hgc_lon = hgc_coord.lon.deg\n",
    "    \n",
    "    # 3. Heliocentric Mean Ecliptic (hme_lon, hme_lat)\n",
    "    hme_coord = hgs_coord.transform_to(HeliocentricMeanEcliptic)\n",
    "    hme_lon = hme_coord.lon.deg\n",
    "    hme_lat = hme_coord.lat.deg\n",
    "    \n",
    "    return dist_c, pa, hgc_lon, hme_lon, hme_lat\n",
    "   \n",
    "# Read intermediate CSV\n",
    "input_path = '../../data/interm/flare_1975_2017_source_pos_dur.csv'\n",
    "df = pd.read_csv(input_path, low_memory=False)\n",
    "\n",
    "# Drop rows with missing dates\n",
    "df = df.dropna(subset=['datetime_start'])\n",
    "\n",
    "# Cache astronomical parameters for unique datetimes to speed up (matching original logic)\n",
    "unique_datetimes = df['datetime_start'].unique()\n",
    "date_params = {dt: calculate_astronomical_parameters(dt) for dt in unique_datetimes}\n",
    "\n",
    "# Apply transformation\n",
    "results = df.progress_apply(\n",
    "    lambda row: hgs_to_all(*date_params[row['datetime_start']], row['hg_lon'], row['hg_lat']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df[['dist_c', 'pa', 'hgc_lon', 'hme_lon', 'hme_lat']] = pd.DataFrame(results.tolist(), index=df.index)\n",
    "\n",
    "# Save to final intermediate file\n",
    "output_path = '../../data/interm/flare_1975_2017_final_interm.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Task completed. Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fbb5541-ecbf-4845-8bf3-f6077281c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task completed. Final dataset contains 39267 flares.\n",
      "Saved to ../../data/ready/flare_1975_2017.csv\n"
     ]
    }
   ],
   "source": [
    "# Final Filtering and Save to Base Data\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read final intermediate file\n",
    "input_path = '../../data/interm/flare_1975_2017_final_interm.csv'\n",
    "df = pd.read_csv(input_path, low_memory=False)\n",
    "\n",
    "# Filter for major flare classes (X, M, C, B)\n",
    "filtered_df = df[df['xray_class'].isin(['X', 'M', 'C', 'B'])].copy()\n",
    "\n",
    "# Drop rows with missing coordinates (to match original Chinese code results)\n",
    "filtered_df = filtered_df.dropna(subset=['dist_c'])\n",
    "\n",
    "# Sort by datetime_start to ensure 1975 data is at the top\n",
    "filtered_df['datetime_start'] = pd.to_datetime(filtered_df['datetime_start'])\n",
    "filtered_df = filtered_df.sort_values('datetime_start')\n",
    "\n",
    "# Select and reorder relevant columns for the final dataset\n",
    "final_columns = [\n",
    "    'datetime_start', 'max_time', 'duration', 'lat_lon', 'sxi',\n",
    "    'xray_class', 'xray_intensity', 'group_id',\n",
    "    'hg_lat', 'hg_lon', 'dist_c', 'pa', 'hgc_lon', \n",
    "    'hme_lon', 'hme_lat'\n",
    "]\n",
    "\n",
    "# Ensure all requested columns exist\n",
    "existing_columns = [col for col in final_columns if col in filtered_df.columns]\n",
    "final_df = filtered_df[existing_columns]\n",
    "\n",
    "# Save to base_data directory\n",
    "output_path = '../../data/ready/flare_1975_2017.csv'\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Task completed. Final dataset contains {len(final_df)} flares.\")\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc40d2c-ca2c-4bcb-8969-214bb99371d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
