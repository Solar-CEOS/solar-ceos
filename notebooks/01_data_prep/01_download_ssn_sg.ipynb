{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f946036-b8d8-4795-8f33-edf27c5f57dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task completed.\n"
     ]
    }
   ],
   "source": [
    "# Format Sunspot Number data\n",
    "\n",
    "import pandas as pd\n",
    "# Load CSV file, using semicolon as delimiter\n",
    "file_path = '../../data/00_raw/SN_d_tot_V2.0.csv'\n",
    "data = pd.read_csv(file_path, delimiter=';', header=None)\n",
    "# Generate date string column, format YYYY-MM-DD\n",
    "data['date_str'] = data[0].astype(str) + '-' + data[1].astype(str).str.zfill(2) + '-' + data[2].astype(str).str.zfill(2)\n",
    "# Convert date string column to datetime objects\n",
    "data['date'] = pd.to_datetime(data['date_str'])\n",
    "# Set start and end dates\n",
    "start_date = '1849-01-01'\n",
    "end_date = '2025-07-31'\n",
    "# Convert string dates to datetime objects for comparison\n",
    "start_date = pd.to_datetime(start_date)\n",
    "end_date = pd.to_datetime(end_date)\n",
    "# Filter rows within the specified date range\n",
    "filtered_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "# Select date column and the 5th column (SSN)\n",
    "final_data = filtered_data[['date', 4]]\n",
    "# Rename columns\n",
    "final_data.columns = ['date', 'ssn']\n",
    "# Save filtered data to a new CSV file\n",
    "output_path = '../../data/ready/ssn_daily_1849_2025.csv'\n",
    "final_data.to_csv(output_path, index=False)\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6becd-c1a7-4875-a4db-f0c4c9637ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically download annual sunspot group position txt data\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "url = 'http://solarcyclescience.com/activeregions.html'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "links = soup.find_all('a')\n",
    "\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    # Ensure href is not empty and points to a .txt file\n",
    "    if href and href.endswith('.txt'):\n",
    "        # Convert relative links to absolute links\n",
    "        href = urljoin(url, href)\n",
    "        try:\n",
    "            r = requests.get(href, timeout=100)\n",
    "            if r.status_code == 200:\n",
    "                # Parse URL path to get filename\n",
    "                path = urlparse(href).path\n",
    "                filename = path.split('/')[-1]\n",
    "                # Ensure filename is not empty\n",
    "                if filename:\n",
    "                    with open(filename, 'wb') as f:\n",
    "                        f.write(r.content)\n",
    "                    print(f\"Downloaded {filename}\")\n",
    "                else:\n",
    "                    print(f\"Invalid filename for URL: {href}\")\n",
    "            else:\n",
    "                print(f\"Failed to download {href}: Status code {r.status_code}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed for {href}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb800a48-a0c0-4fa1-90ca-9bc5bbd2b731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task completed. Total lines written: 256992\n"
     ]
    }
   ],
   "source": [
    "# Merge multiple txt files into one by writing directly to the output file\n",
    "\n",
    "import os\n",
    "\n",
    "# Define start and end years\n",
    "start = 1874\n",
    "end = 2025\n",
    "output_file = '../../data/interm/1874-2025.txt'\n",
    "\n",
    "total_lines_written = 0\n",
    "\n",
    "# Open the output file once and write lines directly to avoid memory overhead\n",
    "with open(output_file, 'w') as outfile:\n",
    "    for i in range(start, end + 1):\n",
    "        file_path = f'../../data/00_raw/solar_cycle_data/g{i}.txt'\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as infile:\n",
    "                for line in infile:\n",
    "                    if line.strip():  # Check for non-empty lines\n",
    "                        outfile.write(line)\n",
    "                        total_lines_written += 1\n",
    "\n",
    "print(f\"Task completed. Total lines written: {total_lines_written}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45afd98-2d0a-4c05-9278-0875ab0a1ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task completed.\n"
     ]
    }
   ],
   "source": [
    "# Extract data based on format.txt\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Dynamically adjust column specifications\n",
    "def get_column_specs_for_year(year):\n",
    "    column_specs = [\n",
    "        (12, 22 if year <= 1981 else 20, 'group_id'),  # Sunspot group ID\n",
    "        (40, 44, 'area'),                              # Area\n",
    "        (44, 50, 'dist_c'),                            # Disk Center Distance\n",
    "        (50, 56, 'pa'),                                # Position Angle\n",
    "        (56, 62, 'hcc_lon'),                           # Carrington Longitude\n",
    "        (62, 68, 'hg_lat'),                            # Heliographic Latitude\n",
    "        (68, 74, 'hg_lon'),                            # Heliographic Longitude\n",
    "    ]\n",
    "    return column_specs\n",
    "\n",
    "# Parse a single line of data and handle date \"%Y-%m-%d %H:%M:%S\"\n",
    "def parse_line(line):\n",
    "    year = int(line[0:4])\n",
    "    month = str(int(line[4:6])).zfill(2) \n",
    "    day = str(int(line[6:8])).zfill(2) \n",
    "    # Parse time information\n",
    "    time_fraction = float(line[8:12])\n",
    "    total_seconds = time_fraction * 86400  # Convert thousandths of a day to seconds\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)    \n",
    "    # Combine date and time into a string\n",
    "    date_obj = f\"{year}-{month}-{day} {int(hours):02}:{int(minutes):02}:{int(seconds):02}\"\n",
    "    # Get column specs for the year\n",
    "    column_specs = get_column_specs_for_year(year)    \n",
    "    # Parse other fields\n",
    "    record = {\"date\": date_obj}\n",
    "    for start, end, name in column_specs:\n",
    "        record[name] = line[start:end].strip()\n",
    "    return record\n",
    "\n",
    "# Parse the entire file\n",
    "def parse_data_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return [parse_line(line) for line in file if line.strip()]\n",
    "\n",
    "# Combine all files and save\n",
    "def combine_and_save_data(txt_files, output_csv_path):\n",
    "    all_data = []\n",
    "    for filepath in txt_files:\n",
    "        all_data.extend(parse_data_file(filepath)) \n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Example usage\n",
    "directory_path = '../../data/00_raw/solar_cycle_data/g*.txt'\n",
    "output_csv_path = '../../data/interm/sg_gsn_ar_source.csv'\n",
    "txt_files = glob.glob(directory_path)\n",
    "combine_and_save_data(txt_files, output_csv_path)\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d32954-77cb-4ef4-90de-e457eefed7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully sorted by date.\n",
      "Process completed: Data filtered and ordered.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "input_path = '../../data/interm/sg_gsn_ar_source.csv'\n",
    "data = pd.read_csv(input_path)\n",
    "\n",
    "# Filter rows where group_id is 0 or invalid\n",
    "data['group_id'] = pd.to_numeric(data['group_id'], errors='coerce')\n",
    "filtered_data = data[data['group_id'] != 0].copy()\n",
    "\n",
    "# Sorting by date\n",
    "# Note: Ensure the column name matches your CSV (e.g., 'date', 'timestamp', or 'obs_date')\n",
    "date_col = 'date'\n",
    "if date_col in filtered_data.columns:\n",
    "    filtered_data[date_col] = pd.to_datetime(filtered_data[date_col])\n",
    "    filtered_data = filtered_data.sort_values(by=date_col).reset_index(drop=True)\n",
    "    print(f\"Dataset successfully sorted by {date_col}.\")\n",
    "else:\n",
    "    print(f\"Warning: Column '{date_col}' not found. Skipping sort.\")\n",
    "\n",
    "# Export to CSV\n",
    "output_path = '../../data/interm/sg_gsn_ar_source_filtered.csv'\n",
    "filtered_data.to_csv(output_path, encoding='utf-8-sig', index=False)\n",
    "\n",
    "print(\"Process completed: Data filtered and ordered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d840bf0-2901-4665-82aa-ff7731b77b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
