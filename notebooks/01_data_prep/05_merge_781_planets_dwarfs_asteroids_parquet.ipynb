{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3122f31",
   "metadata": {},
   "source": [
    "# Process 781 Celestial Objects (Planets + Dwarf Planets + Asteroids)\n",
    "\n",
    "This notebook is used to process data for planets, dwarf planets, and asteroids, outputting three files:\n",
    "1. Ecliptic Longitude, Ecliptic Latitude, Distance, and Range Rate\n",
    "2. Geometric Position Parameters (x, y, z)\n",
    "3. Velocity Parameters (vx, vy, vz)\n",
    "\n",
    "Actual number of objects: 19 Planets/Dwarf Planets + 762 Asteroids = 781 total.\n",
    "Given the large volume of data, Parquet format is used for storage to improve efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1257a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete. Saved to ../../data/final/major_planets_dwarfs_subset.csv\n",
      "Extracted columns: 240\n"
     ]
    }
   ],
   "source": [
    "# Extract data for major planets and dwarf planets and save as CSV\n",
    "\n",
    "# List of bodies\n",
    "planets = ['SSB', '199', '299', '399', '499', '599', '699', '799', '899']\n",
    "dwarfs = ['Ceres', '999', '136108', '136472', '136199','90482','120347','50000','225088','90377']\n",
    "combined_list = planets + dwarfs\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory for final processed data\n",
    "output_dir = \"../../data/ready\"\n",
    "\n",
    "# Files to read from (Planets, Dwarfs, Asteroids)\n",
    "files = [\n",
    "    f\"{output_dir}/781_planets_dwarfs_asteroids_lonlat.parquet\",\n",
    "    f\"{output_dir}/781_planets_dwarfs_asteroids_velocity.parquet\",\n",
    "    f\"{output_dir}/781_planets_dwarfs_asteroids_xyz.parquet\"\n",
    "]\n",
    "\n",
    "# Check if files exist before processing\n",
    "if all(os.path.exists(f) for f in files):\n",
    "    # Read all files and merge\n",
    "    dfs = [pd.read_parquet(file) for file in files]\n",
    "    df_combined = pd.concat(dfs, axis=1)\n",
    "\n",
    "    # Extract date column\n",
    "    date_col = df_combined['date'].iloc[:, 0] if 'date' in df_combined.columns and isinstance(df_combined['date'], pd.DataFrame) else df_combined['date']\n",
    "\n",
    "    # Extract columns for selected bodies\n",
    "    selected_cols = [col for col in df_combined.columns if col != 'date' and any(planet in col for planet in combined_list)]\n",
    "    df_selected = df_combined[selected_cols]\n",
    "\n",
    "    # Insert date column at the beginning\n",
    "    df_selected.insert(0, 'date', date_col.values)\n",
    "\n",
    "    # Save as CSV for quick reference\n",
    "    output_csv = f\"{output_dir}/major_planets_dwarfs_subset.csv\"\n",
    "    df_selected.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Data extraction complete. Saved to {output_csv}\")\n",
    "    print(f\"Extracted columns: {len(selected_cols)}\")\n",
    "else:\n",
    "    print(\"Note: Parquet files not found. Run the processing cells below first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6eabebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee8a7497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions defined successfully\n"
     ]
    }
   ],
   "source": [
    "# Custom date parsing function to handle \"A.D. YYYY-MMM-DD HH:MM:SS.SSSS\" format\n",
    "def parse_special_date(date_str):\n",
    "    # Remove \"A.D. \" prefix\n",
    "    if \"A.D. \" in date_str:\n",
    "        date_str = date_str.replace(\"A.D. \", \"\")\n",
    "    # Parse the remaining part\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, format=\"%Y-%b-%d %H:%M:%S.%f\")\n",
    "    except:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str)\n",
    "        except:\n",
    "            print(f\"Unable to parse date: {date_str}\")\n",
    "            return pd.NaT\n",
    "\n",
    "# Standardize date format function\n",
    "def standardize_date(df):\n",
    "    \"\"\"Standardize date format to YYYY-MM-DD\"\"\"\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "    return df\n",
    "\n",
    "print(\"Functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fd0946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 781 objects from ../../data/00_raw/781_planets_dwarfs_asteroids_params.csv using column 'body'\n",
      "lon_lat - planets_dwarfs: ../../data/00_raw/helio_ecl_sph_00h/planets_dwarfs_daily_1849 (Files: 19)\n",
      "lon_lat - asteroids: ../../data/00_raw/helio_ecl_sph_00h/asteroids_daily_1849 (Files: 762)\n",
      "xyz - planets_dwarfs: ../../data/00_raw/helio_cart_states_00h/planets_dwarfs_daily_1849 (Files: 19)\n",
      "xyz - asteroids: ../../data/00_raw/helio_cart_states_00h/asteroids_daily_1849 (Files: 762)\n",
      "\n",
      "Total files: 781 objects (each has lonlat and XYZ files)\n",
      "Will process 781 objects in the order specified by the CSV\n"
     ]
    }
   ],
   "source": [
    "# Define folder paths to traverse\n",
    "# TODO: Ensure these directories exist and contain the raw CSV files from Notebook 03\n",
    "base_folders = {\n",
    "    'lon_lat': {\n",
    "        'planets_dwarfs': '../../data/00_raw/helio_ecl_sph_00h/planets_dwarfs_daily_1849',\n",
    "        'asteroids': '../../data/00_raw/helio_ecl_sph_00h/asteroids_daily_1849'\n",
    "    },\n",
    "    'xyz': {\n",
    "        'planets_dwarfs': '../../data/00_raw/helio_cart_states_00h/planets_dwarfs_daily_1849',\n",
    "        'asteroids': '../../data/00_raw/helio_cart_states_00h/asteroids_daily_1849'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get star processing order from CSV\n",
    "def get_star_order():\n",
    "    \"\"\"Get the processing order of objects from a parameter CSV file\"\"\"\n",
    "    # TODO: Update this path to the actual location of your 781 objects parameter file\n",
    "    csv_file = '../../data/00_raw/781_planets_dwarfs_asteroids_params.csv' \n",
    "    if os.path.exists(csv_file):\n",
    "        df_params = pd.read_csv(csv_file)\n",
    "        # Check for common column names (English or Chinese)\n",
    "        star_col = None\n",
    "        for col in ['body', 'body_id', 'star_id', 'object', 'ID']:\n",
    "            if col in df_params.columns:\n",
    "                star_col = col\n",
    "                break\n",
    "        \n",
    "        if star_col:\n",
    "            star_list = df_params[star_col].astype(str).tolist()\n",
    "            print(f\"Read {len(star_list)} objects from {csv_file} using column '{star_col}'\")\n",
    "            return star_list\n",
    "        else:\n",
    "            print(f\"Warning: Could not find a valid ID column in {csv_file}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Warning: Parameter file {csv_file} not found. Will use folder scanning.\")\n",
    "        return None\n",
    "\n",
    "# Get star processing order\n",
    "star_order = get_star_order()\n",
    "\n",
    "# Check if folders exist and count files\n",
    "total_files = 0\n",
    "for data_type, folders in base_folders.items():\n",
    "    for category, folder_path in folders.items():\n",
    "        if os.path.exists(folder_path):\n",
    "            file_count = len(glob.glob(os.path.join(folder_path, '*.csv')))\n",
    "            total_files += file_count\n",
    "            print(f\"{data_type} - {category}: {folder_path} (Files: {file_count})\")\n",
    "        else:\n",
    "            print(f\"Warning: {folder_path} does not exist\")\n",
    "\n",
    "if total_files > 0:\n",
    "    print(f\"\\nTotal files: {total_files//2} objects (each has lonlat and XYZ files)\")\n",
    "if star_order:\n",
    "    print(f\"Will process {len(star_order)} objects in the order specified by the CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fb6abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lon_lat_data():\n",
    "    \"\"\"Process ecliptic longitude/latitude data, outputting lon, lat, range, and range_rate\"\"\"\n",
    "    print(\"Starting processing of lon/lat data...\")\n",
    "\n",
    "    df_list = []\n",
    "    processed_count = 0\n",
    "\n",
    "    if star_order:\n",
    "        # Process objects in the order specified by the CSV\n",
    "        for star_id in tqdm(star_order, desc=\"Processing objects (CSV order)\"):\n",
    "            try:\n",
    "                lonlat_file_found = False\n",
    "                xyz_file_found = False\n",
    "                lonlat_file_path = None\n",
    "                xyz_file_path = None\n",
    "\n",
    "                # Search for lonlat file\n",
    "                for category, folder_path in base_folders['lon_lat'].items():\n",
    "                    if not os.path.exists(folder_path):\n",
    "                        continue\n",
    "\n",
    "                    # Try different naming conventions\n",
    "                    possible_files = [\n",
    "                        os.path.join(folder_path, f\"{star_id}_lonlat.csv\"),\n",
    "                        os.path.join(folder_path, f\"{star_id}_lon_lat.csv\"),\n",
    "                        os.path.join(folder_path, f\"{star_id}.csv\")\n",
    "                    ]\n",
    "                    \n",
    "                    glob_pattern = os.path.join(folder_path, f\"{star_id}_*.csv\")\n",
    "                    possible_files.extend(glob.glob(glob_pattern))\n",
    "\n",
    "                    for file_path in possible_files:\n",
    "                        if os.path.exists(file_path):\n",
    "                            lonlat_file_path = file_path\n",
    "                            lonlat_file_found = True\n",
    "                            break\n",
    "                    if lonlat_file_found: break\n",
    "\n",
    "                # Search for XYZ file (needed for range/range_rate)\n",
    "                for category, folder_path in base_folders['xyz'].items():\n",
    "                    if not os.path.exists(folder_path):\n",
    "                        continue\n",
    "\n",
    "                    possible_files = [\n",
    "                        os.path.join(folder_path, f\"{star_id}_xyz.csv\"),\n",
    "                        os.path.join(folder_path, f\"{star_id}.csv\")\n",
    "                    ]\n",
    "                    \n",
    "                    glob_pattern = os.path.join(folder_path, f\"{star_id}_*.csv\")\n",
    "                    possible_files.extend(glob.glob(glob_pattern))\n",
    "\n",
    "                    for file_path in possible_files:\n",
    "                        if os.path.exists(file_path):\n",
    "                            xyz_file_path = file_path\n",
    "                            xyz_file_found = True\n",
    "                            break\n",
    "                    if xyz_file_found: break\n",
    "\n",
    "                if not lonlat_file_found or not xyz_file_found:\n",
    "                    continue\n",
    "\n",
    "                # Read lonlat data\n",
    "                df_lonlat = pd.read_csv(lonlat_file_path)\n",
    "                if 'date' in df_lonlat.columns:\n",
    "                    df_lonlat['date'] = df_lonlat['date'].apply(parse_special_date)\n",
    "\n",
    "                lonlat_cols = ['date']\n",
    "                if 'lon' in df_lonlat.columns: lonlat_cols.append('lon')\n",
    "                if 'lat' in df_lonlat.columns: lonlat_cols.append('lat')\n",
    "                \n",
    "                for col in df_lonlat.columns:\n",
    "                    if col != 'date' and (col.endswith('_lon') or col.endswith('_lat')):\n",
    "                        lonlat_cols.append(col)\n",
    "\n",
    "                df_lonlat = df_lonlat[lonlat_cols]\n",
    "\n",
    "                # Rename columns with star_id prefix\n",
    "                column_mapping = {}\n",
    "                for col in df_lonlat.columns:\n",
    "                    if col != 'date':\n",
    "                        if col == 'lon' or col.endswith('_lon'):\n",
    "                            column_mapping[col] = f\"{star_id}_lon\"\n",
    "                        elif col == 'lat' or col.endswith('_lat'):\n",
    "                            column_mapping[col] = f\"{star_id}_lat\"\n",
    "                        else:\n",
    "                            column_mapping[col] = f\"{star_id}_{col}\"\n",
    "                df_lonlat.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "                # Read range data from XYZ file\n",
    "                df_xyz = pd.read_csv(xyz_file_path)\n",
    "                if 'date' in df_xyz.columns:\n",
    "                    df_xyz['date'] = df_xyz['date'].apply(parse_special_date)\n",
    "\n",
    "                distance_data = df_xyz[['date']].copy()\n",
    "                if 'range' in df_xyz.columns:\n",
    "                    distance_data[f\"{star_id}_range\"] = df_xyz['range']\n",
    "                if 'range_rate' in df_xyz.columns:\n",
    "                    distance_data[f\"{star_id}_range_rate\"] = df_xyz['range_rate']\n",
    "\n",
    "                # Merge lonlat and distance for this star\n",
    "                df_temp = pd.merge(df_lonlat, distance_data, on='date', how='left')\n",
    "                \n",
    "                # Set date as index for efficient concatenation later\n",
    "                df_temp.set_index('date', inplace=True)\n",
    "                df_list.append(df_temp)\n",
    "\n",
    "                processed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing object {star_id}: {e}\")\n",
    "                continue\n",
    "    else:\n",
    "        print(\"Object order not provided. Skipping folder scanning implementation for brevity.\")\n",
    "\n",
    "    if df_list:\n",
    "        print(f\"Concatenating {len(df_list)} objects...\")\n",
    "        # Use pd.concat with axis=1 for much faster merging than pd.merge in a loop\n",
    "        merged_df = pd.concat(df_list, axis=1).reset_index()\n",
    "        \n",
    "        merged_df = standardize_date(merged_df)\n",
    "        cols = ['date'] + [col for col in merged_df.columns if col != 'date']\n",
    "        merged_df = merged_df[cols]\n",
    "\n",
    "        output_file = '../../data/ready/781_planets_dwarfs_asteroids_lonlat.parquet'\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        merged_df.to_parquet(output_file, index=False, compression='snappy')\n",
    "\n",
    "        print(f\"Lon/Lat data processing complete!\")\n",
    "        print(f\"Processed {processed_count} objects\")\n",
    "        print(f\"Saved to: {output_file}\")\n",
    "        return merged_df\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea393077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xyz_position_data():\n",
    "    \"\"\"Process XYZ position data\"\"\"\n",
    "    print(\"Starting processing of XYZ position data...\")\n",
    "    \n",
    "    merged_df = None\n",
    "    processed_count = 0\n",
    "    \n",
    "    if star_order:\n",
    "        for star_id in tqdm(star_order, desc=\"Processing XYZ positions\"):\n",
    "            try:\n",
    "                file_found = False\n",
    "                file_path = None\n",
    "                \n",
    "                for category, folder_path in base_folders['xyz'].items():\n",
    "                    if not os.path.exists(folder_path): continue\n",
    "                    \n",
    "                    possible_files = [\n",
    "                        os.path.join(folder_path, f\"{star_id}_xyz.csv\"),\n",
    "                        os.path.join(folder_path, f\"{star_id}.csv\")\n",
    "                    ]\n",
    "                    glob_pattern = os.path.join(folder_path, f\"{star_id}_*.csv\")\n",
    "                    possible_files.extend(glob.glob(glob_pattern))\n",
    "                    \n",
    "                    for possible_file in possible_files:\n",
    "                        if os.path.exists(possible_file):\n",
    "                            file_path = possible_file\n",
    "                            file_found = True\n",
    "                            break\n",
    "                    if file_found: break\n",
    "                \n",
    "                if not file_found: continue\n",
    "                \n",
    "                df_temp = pd.read_csv(file_path)\n",
    "                if 'date' in df_temp.columns:\n",
    "                    df_temp['date'] = df_temp['date'].apply(parse_special_date)\n",
    "                \n",
    "                position_cols = ['date']\n",
    "                for c in ['x', 'y', 'z']:\n",
    "                    if c in df_temp.columns: position_cols.append(c)\n",
    "                \n",
    "                df_temp = df_temp[position_cols]\n",
    "                column_mapping = {c: f\"{star_id}_{c}\" for c in ['x', 'y', 'z'] if c in df_temp.columns}\n",
    "                df_temp.rename(columns=column_mapping, inplace=True)\n",
    "                \n",
    "                if merged_df is None:\n",
    "                    merged_df = df_temp\n",
    "                else:\n",
    "                    merged_df = pd.merge(merged_df, df_temp, on='date', how='outer')\n",
    "                \n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing XYZ for {star_id}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if merged_df is not None:\n",
    "        merged_df = standardize_date(merged_df)\n",
    "        cols = ['date'] + [col for col in merged_df.columns if col != 'date']\n",
    "        merged_df = merged_df[cols]\n",
    "        \n",
    "        output_file = '../../data/ready/781_planets_dwarfs_asteroids_xyz.parquet'\n",
    "        merged_df.to_parquet(output_file, index=False, compression='snappy')\n",
    "        print(f\"XYZ position data processing complete! Saved to: {output_file}\")\n",
    "        return merged_df\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24627761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_velocity_data():\n",
    "    \"\"\"Process velocity data (vx, vy, vz)\"\"\"\n",
    "    print(\"Starting processing of velocity data...\")\n",
    "    \n",
    "    merged_df = None\n",
    "    processed_count = 0\n",
    "    \n",
    "    if star_order:\n",
    "        for star_id in tqdm(star_order, desc=\"Processing velocity data\"):\n",
    "            try:\n",
    "                file_found = False\n",
    "                file_path = None\n",
    "                \n",
    "                for category, folder_path in base_folders['xyz'].items():\n",
    "                    if not os.path.exists(folder_path): continue\n",
    "                    \n",
    "                    possible_files = [\n",
    "                        os.path.join(folder_path, f\"{star_id}_xyz.csv\"),\n",
    "                        os.path.join(folder_path, f\"{star_id}.csv\")\n",
    "                    ]\n",
    "                    glob_pattern = os.path.join(folder_path, f\"{star_id}_*.csv\")\n",
    "                    possible_files.extend(glob.glob(glob_pattern))\n",
    "                    \n",
    "                    for possible_file in possible_files:\n",
    "                        if os.path.exists(possible_file):\n",
    "                            file_path = possible_file\n",
    "                            file_found = True\n",
    "                            break\n",
    "                    if file_found: break\n",
    "                \n",
    "                if not file_found: continue\n",
    "                \n",
    "                df_temp = pd.read_csv(file_path)\n",
    "                if 'date' in df_temp.columns:\n",
    "                    df_temp['date'] = df_temp['date'].apply(parse_special_date)\n",
    "                \n",
    "                velocity_cols = ['date']\n",
    "                for c in ['vx', 'vy', 'vz']:\n",
    "                    if c in df_temp.columns: velocity_cols.append(c)\n",
    "                \n",
    "                df_temp = df_temp[velocity_cols]\n",
    "                column_mapping = {c: f\"{star_id}_{c}\" for c in ['vx', 'vy', 'vz'] if c in df_temp.columns}\n",
    "                df_temp.rename(columns=column_mapping, inplace=True)\n",
    "                \n",
    "                if merged_df is None:\n",
    "                    merged_df = df_temp\n",
    "                else:\n",
    "                    merged_df = pd.merge(merged_df, df_temp, on='date', how='outer')\n",
    "                \n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing velocity for {star_id}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if merged_df is not None:\n",
    "        merged_df = standardize_date(merged_df)\n",
    "        cols = ['date'] + [col for col in merged_df.columns if col != 'date']\n",
    "        merged_df = merged_df[cols]\n",
    "        \n",
    "        output_file = '../../data/ready/781_planets_dwarfs_asteroids_velocity.parquet'\n",
    "        merged_df.to_parquet(output_file, index=False, compression='snappy')\n",
    "        print(f\"Velocity data processing complete! Saved to: {output_file}\")\n",
    "        return merged_df\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c79525f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing of 781 objects (Planets, Dwarfs, Asteroids) data...\n",
      "============================================================\n",
      "1. Processing Lon/Lat and Distance data\n",
      "Starting processing of lon/lat data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing objects (CSV order): 100%|█████████| 781/781 [55:31<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating 781 objects...\n",
      "Lon/Lat data processing complete!\n",
      "Processed 781 objects\n",
      "Saved to: ../../data/ready/781_planets_dwarfs_asteroids_lonlat.parquet\n",
      "\n",
      "2. Processing XYZ position data\n",
      "Starting processing of XYZ position data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing XYZ positions: 100%|███████████████| 781/781 [29:29<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XYZ position data processing complete! Saved to: ../../data/ready/781_planets_dwarfs_asteroids_xyz.parquet\n",
      "\n",
      "3. Processing velocity data\n",
      "Starting processing of velocity data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing velocity data: 100%|███████████████| 781/781 [29:33<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Velocity data processing complete! Saved to: ../../data/ready/781_planets_dwarfs_asteroids_velocity.parquet\n",
      "\n",
      "All data processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Execute all data processing functions\n",
    "print(\"Starting processing of 781 objects (Planets, Dwarfs, Asteroids) data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Process Lon/Lat and Distance data\n",
    "print(\"1. Processing Lon/Lat and Distance data\")\n",
    "lon_lat_df = process_lon_lat_data()\n",
    "print()\n",
    "\n",
    "# 2. Process XYZ position data  \n",
    "print(\"2. Processing XYZ position data\")\n",
    "position_df = process_xyz_position_data()\n",
    "print()\n",
    "\n",
    "# 3. Process velocity data\n",
    "print(\"3. Processing velocity data\")\n",
    "velocity_df = process_velocity_data()\n",
    "print()\n",
    "\n",
    "print(\"All data processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e9c6af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Data Processing Summary\n",
      "==================================================\n",
      "✓ ../../data/ready/781_planets_dwarfs_asteroids_lonlat.parquet - Size: 2228.45 MB\n",
      "  Dimensions: 73780 rows x 3125 columns\n",
      "  Date range: 1849-01-01 to 2051-01-01\n",
      "  Number of objects: 781\n",
      "  Column examples: ['date', 'SSB_lon', 'SSB_lat', 'SSB_range', 'SSB_range_rate']...\n",
      "\n",
      "✓ ../../data/ready/781_planets_dwarfs_asteroids_xyz.parquet - Size: 1671.37 MB\n",
      "  Dimensions: 73780 rows x 2344 columns\n",
      "  Date range: 1849-01-01 to 2051-01-01\n",
      "  Number of objects: 781\n",
      "  Column examples: ['date', 'SSB_x', 'SSB_y', 'SSB_z', '199_x']...\n",
      "\n",
      "✓ ../../data/ready/781_planets_dwarfs_asteroids_velocity.parquet - Size: 1671.38 MB\n",
      "  Dimensions: 73780 rows x 2344 columns\n",
      "  Date range: 1849-01-01 to 2051-01-01\n",
      "  Number of objects: 781\n",
      "  Column examples: ['date', 'SSB_vx', 'SSB_vy', 'SSB_vz', '199_vx']...\n",
      "\n",
      "Processing complete! All data saved in Parquet format for efficient access.\n"
     ]
    }
   ],
   "source": [
    "# Data Summary and Validation\n",
    "print(\"=\" * 50)\n",
    "print(\"Data Processing Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "output_files = [\n",
    "    '../../data/ready/781_planets_dwarfs_asteroids_lonlat.parquet',\n",
    "    '../../data/ready/781_planets_dwarfs_asteroids_xyz.parquet', \n",
    "    '../../data/ready/781_planets_dwarfs_asteroids_velocity.parquet'\n",
    "]\n",
    "\n",
    "for file in output_files:\n",
    "    if os.path.exists(file):\n",
    "        file_size = os.path.getsize(file) / (1024 * 1024)  # MB\n",
    "        print(f\"✓ {file} - Size: {file_size:.2f} MB\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_parquet(file)\n",
    "            print(f\"  Dimensions: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "            print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "            \n",
    "            # Estimate number of objects\n",
    "            non_date_cols = [col for col in df.columns if col != 'date']\n",
    "            if 'lonlat' in file:\n",
    "                star_count = len([col for col in non_date_cols if col.endswith('_lon')])\n",
    "            elif 'xyz' in file:\n",
    "                star_count = len([col for col in non_date_cols if col.endswith('_x')])\n",
    "            elif 'velocity' in file:\n",
    "                star_count = len([col for col in non_date_cols if col.endswith('_vx')])\n",
    "            else:\n",
    "                star_count = len(non_date_cols) // 3\n",
    "            \n",
    "            print(f\"  Number of objects: {star_count}\")\n",
    "            print(f\"  Column examples: {list(df.columns[:5])}...\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading file: {e}\")\n",
    "    else:\n",
    "        print(f\"✗ {file} - File not found\")\n",
    "\n",
    "print(\"Processing complete! All data saved in Parquet format for efficient access.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0191282-5716-498c-9ca8-d93ece03f684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
